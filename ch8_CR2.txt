[student@workstation ~]$ lab start compreview-apps 
SUCCESS Waiting for cluster
SUCCESS Copy exercise files
SUCCESS Creating do280-support in IdM
SUCCESS Apply manifest file support-group.yaml
SUCCESS Generate beeper-api certificate
[student@workstation ~]$ cd DO280/labs/compreview-apps/
[student@workstation compreview-apps]$ ls
beeper-api  limitrange.yaml  project-cleaner  quota.yaml  subscription.yaml  support-group.yaml
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
Login successful.

You have access to 72 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc new-project workshop-support
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc describe ns workshop-support
Name:         workshop-support
Labels:       kubernetes.io/metadata.name=workshop-support
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=workshop-support
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: admin
              openshift.io/sa.scc.mcs: s0:c32,c29
              openshift.io/sa.scc.supplemental-groups: 1001050000/10000
              openshift.io/sa.scc.uid-range: 1001050000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  memory    -    750Mi  250Mi            500Mi          -
 Container  cpu       -    750m   100m             500m           -
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc label ns workshop-support category=support
namespace/workshop-support labeled
[student@workstation compreview-apps]$ oc describe ns workshop-support
Name:         workshop-support
Labels:       category=support
              kubernetes.io/metadata.name=workshop-support
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=workshop-support
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: admin
              openshift.io/sa.scc.mcs: s0:c32,c29
              openshift.io/sa.scc.supplemental-groups: 1001050000/10000
              openshift.io/sa.scc.uid-range: 1001050000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  cpu       -    750m   100m             500m           -
 Container  memory    -    750Mi  250Mi            500Mi          -
[student@workstation compreview-apps]$ oc adm policy add-role-to-group admin workshop-support -n workshop-support
clusterrole.rbac.authorization.k8s.io/admin added: "workshop-support"
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc get group
NAME                USERS
Default SMB Group   
admins              Administrator
developer           
do280-attendees     do280-attendee
editors             
ocpadmins           Administrator
ocpdevs             . developer
project-team        developer, tester
workshop-support    do280-support
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc create quota workshop-support --hard=limits.cpu=4,limits.memory=4Gi,requests.cpu=3500m,requests.memory=3Gi
resourcequota/workshop-support created
[student@workstation compreview-apps]$ oc describe resourcequotas workshop
Name:            workshop
Namespace:       workshop-support
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     2
limits.memory    0     1Gi
requests.cpu     0     1500m
requests.memory  0     750Mi
[student@workstation compreview-apps]$ oc describe resourcequotas workshop-support 
Name:            workshop-support
Namespace:       workshop-support
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     4
limits.memory    0     4Gi
requests.cpu     0     3500m
requests.memory  0     3Gi
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ ls
beeper-api  limitrange.yaml  project-cleaner  quota.yaml  subscription.yaml  support-group.yaml
[student@workstation compreview-apps]$ cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
 name: workshop-support
 namespace: workshop-support
spec:
 limits:
   - CHANGE_ME
[student@workstation compreview-apps]$ cat quota.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
 name: workshop-quota-template
 namespace: workshop-template
spec:
  CHANGE_ME
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ vim limitrange.yaml 
[student@workstation compreview-apps]$ cat limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
 name: workshop-support
 namespace: workshop-support
spec:
 limits:
   - default:
       cpu: 300m
       memory: 400Mi
     defaultRequest:
       cpu: 100m
       memory: 250Mi
     type: Container
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc apply -f limitrange.yaml
limitrange/workshop-support created
[student@workstation compreview-apps]$ oc get limitranges 
NAME               CREATED AT
workshop           2023-03-08T17:32:38Z
workshop-support   2023-03-08T17:36:46Z
[student@workstation compreview-apps]$ oc describe limitranges workshop-support 
Name:       workshop-support
Namespace:  workshop-support
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    100m             300m           -
Container   memory    -    -    250Mi            400Mi          -
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc create sa project-cleaner-sa
serviceaccount/project-cleaner-sa created
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "project-cleaner-sa"
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ ls
beeper-api  limitrange.yaml  project-cleaner  quota.yaml  subscription.yaml  support-group.yaml
[student@workstation compreview-apps]$ tree
.
├── beeper-api
│   ├── beeper-api-ingresspolicy.yaml
│   ├── beeper-db.yaml
│   ├── certs
│   │   ├── beeper-api.csr
│   │   ├── beeper-api.key
│   │   ├── beeper-api.pem
│   │   ├── ca.key
│   │   ├── ca.pem
│   │   ├── ca.srl
│   │   └── san.ext
│   ├── db-networkpolicy.yaml
│   ├── deployment.yaml
│   └── service.yaml
├── limitrange.yaml
├── project-cleaner
│   ├── cluster-role.yaml
│   ├── cron-job.yaml
│   └── example-pod.yaml
├── quota.yaml
├── subscription.yaml
└── support-group.yaml

3 directories, 19 files
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ cd project-cleaner/
[student@workstation project-cleaner]$ ls
cluster-role.yaml  cron-job.yaml  example-pod.yaml
[student@workstation project-cleaner]$ cat cluster-role.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: project-cleaner
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - delete

[student@workstation project-cleaner]$ oc apply -f cluster-role.yaml
clusterrole.rbac.authorization.k8s.io/project-cleaner created
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc adm policy add-role-to-user project-cleaner -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/project-cleaner added: "project-cleaner-sa"
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cat cron-job.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: CHANGE_ME
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          CHANGE_ME
[student@workstation project-cleaner]$ cat example-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  restartPolicy: Never
  containers:
    - name: project-cleaner
      image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
      imagePullPolicy: Always
      env:
      - name: "PROJECT_TAG"
        value: "workshop"
      - name: "EXPIRATION_SECONDS"
        value: "10"
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
  serviceAccountName: CHANGE_ME
  securityContext:
    runAsUser: 1001

[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cat cron-job.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: CHANGE_ME
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          CHANGE_ME
[student@workstation project-cleaner]$ vim cron-job.yaml
[student@workstation project-cleaner]$ cat cron-job.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: project-cleaner
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
              imagePullPolicy: Always
              env:
              - name: "PROJECT_TAG"
                value: "worskhop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:
                limits:
                  cpu: 100m
                  memory: 200Mi
           serviceAccountName: project-cleaner-sa
           securityContext:
             runAsUser: 1001
[student@workstation project-cleaner]$ cat ~/DO280/solutions/compreview-apps/
beeper-api/        limitrange.yaml    project-cleaner/   quota.yaml         subscription.yaml  
[student@workstation project-cleaner]$ cat ~/DO280/solutions/compreview-apps/
beeper-api/        limitrange.yaml    project-cleaner/   quota.yaml         subscription.yaml  
[student@workstation project-cleaner]$ cat ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: project-cleaner
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
              imagePullPolicy: Always
              env:
              - name: "PROJECT_TAG"
                value: "workshop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:
                limits:
                  cpu: 200m
                  memory: 200Mi
          serviceAccountName: project-cleaner-sa
          securityContext:
            runAsUser: 1001
[student@workstation project-cleaner]$ diff ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml cron-job.yaml 
20c20
<                 value: "workshop"
---
>                 value: "worskhop"
25c25
<                   cpu: 200m
---
>                   cpu: 100m
27,29c27,29
<           serviceAccountName: project-cleaner-sa
<           securityContext:
<             runAsUser: 1001
---
>            serviceAccountName: project-cleaner-sa
>            securityContext:
>              runAsUser: 1001
[student@workstation project-cleaner]$ vim ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml
[student@workstation project-cleaner]$ vim cron-job.yaml 
[student@workstation project-cleaner]$ diff ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml cron-job.yaml 
20c20
<                 value: "workshop"
---
>                 value: "worskhop"
25c25
<                   cpu: 200m
---
>                   cpu: 100m
[student@workstation project-cleaner]$ vim cron-job.yaml 
[student@workstation project-cleaner]$ diff ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml cron-job.yaml 
25c25
<                   cpu: 200m
---
>                   cpu: 100m
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml 
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc new-project clean-test
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ oc project workshop-support 
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc label ns clean-test workshop=
error: 'workshop' already has a value (clean-test), and --overwrite is false
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc describe ns clean-test
Name:         clean-test
Labels:       kubernetes.io/metadata.name=clean-test
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=clean-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: do280-support
              openshift.io/sa.scc.mcs: s0:c33,c7
              openshift.io/sa.scc.supplemental-groups: 1001070000/10000
              openshift.io/sa.scc.uid-range: 1001070000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  cpu       -    750m   100m             500m           -
 Container  memory    -    750Mi  250Mi            500Mi          -
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971625   0/1           69s        69s

NAME                                 READY   STATUS    RESTARTS   AGE
pod/project-cleaner-27971625-8gw55   0/1     Error     0          56s
pod/project-cleaner-27971625-8pqgp   0/1     Error     0          32s
pod/project-cleaner-27971625-mvhfj   0/1     Error     0          69s
pod/project-cleaner-27971625-nm79x   1/1     Running   0          8s
pod/project-cleaner-27971625-qmbzt   0/1     Error     0          20s
pod/project-cleaner-27971625-v2q9s   0/1     Error     0          43s
[student@workstation project-cleaner]$ oc label ns clean-test workshop=
error: 'workshop' already has a value (clean-test), and --overwrite is false
[student@workstation project-cleaner]$ oc label ns clean-test workshop= --overwrite=false
error: 'workshop' already has a value (clean-test), and --overwrite is false
[student@workstation project-cleaner]$ oc label ns clean-test workshop= --overwrite=true
namespace/clean-test labeled
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971625   0/1           2m9s       2m9s
job.batch/project-cleaner-27971626   0/1           45s        45s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971625-8gw55   0/1     Error    0          116s
pod/project-cleaner-27971625-8pqgp   0/1     Error    0          92s
pod/project-cleaner-27971625-b874b   0/1     Error    0          56s
pod/project-cleaner-27971625-mvhfj   0/1     Error    0          2m9s
pod/project-cleaner-27971625-nm79x   0/1     Error    0          68s
pod/project-cleaner-27971625-qmbzt   0/1     Error    0          80s
pod/project-cleaner-27971625-v2q9s   0/1     Error    0          103s
pod/project-cleaner-27971626-4jk5s   0/1     Error    0          11s
pod/project-cleaner-27971626-hxbzh   0/1     Error    0          45s
pod/project-cleaner-27971626-ss7vw   0/1     Error    0          33s
pod/project-cleaner-27971626-tszmz   0/1     Error    0          22s
[student@workstation project-cleaner]$ oc label ns clean-test workshop=clean-test --overwrite=true
namespace/clean-test labeled
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971625   0/1           2m28s      2m28s
job.batch/project-cleaner-27971626   0/1           64s        64s

NAME                                 READY   STATUS    RESTARTS   AGE
pod/project-cleaner-27971625-8gw55   0/1     Error     0          2m15s
pod/project-cleaner-27971625-8pqgp   0/1     Error     0          111s
pod/project-cleaner-27971625-b874b   0/1     Error     0          75s
pod/project-cleaner-27971625-mvhfj   0/1     Error     0          2m28s
pod/project-cleaner-27971625-nm79x   0/1     Error     0          87s
pod/project-cleaner-27971625-qmbzt   0/1     Error     0          99s
pod/project-cleaner-27971625-v2q9s   0/1     Error     0          2m2s
pod/project-cleaner-27971626-4jk5s   0/1     Error     0          30s
pod/project-cleaner-27971626-b2rh5   1/1     Running   0          7s
pod/project-cleaner-27971626-hxbzh   0/1     Error     0          64s
pod/project-cleaner-27971626-jnqvj   0/1     Error     0          18s
pod/project-cleaner-27971626-ss7vw   0/1     Error     0          52s
pod/project-cleaner-27971626-tszmz   0/1     Error     0          41s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971629   0/1           2m         2m
job.batch/project-cleaner-27971630   0/1           32s        32s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971629-cs9x8   0/1     Error    0          69s
pod/project-cleaner-27971629-hd72r   0/1     Error    0          81s
pod/project-cleaner-27971629-hkljh   0/1     Error    0          45s
pod/project-cleaner-27971629-j87wg   0/1     Error    0          98s
pod/project-cleaner-27971629-sbdkj   0/1     Error    0          57s
pod/project-cleaner-27971629-tnjg9   0/1     Error    0          2m
pod/project-cleaner-27971629-vwncx   0/1     Error    0          109s
pod/project-cleaner-27971630-47rjk   0/1     Error    0          20s
pod/project-cleaner-27971630-5mcqj   0/1     Error    0          9s
pod/project-cleaner-27971630-vhvj2   0/1     Error    0          32s
[student@workstation project-cleaner]$ oc label ns clean-test workshop= --overwrite=true
namespace/clean-test labeled
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971629   0/1           2m23s      2m23s
job.batch/project-cleaner-27971630   0/1           55s        55s

NAME                                 READY   STATUS    RESTARTS   AGE
pod/project-cleaner-27971629-cs9x8   0/1     Error     0          92s
pod/project-cleaner-27971629-hd72r   0/1     Error     0          104s
pod/project-cleaner-27971629-hkljh   0/1     Error     0          68s
pod/project-cleaner-27971629-j87wg   0/1     Error     0          2m1s
pod/project-cleaner-27971629-sbdkj   0/1     Error     0          80s
pod/project-cleaner-27971629-tnjg9   0/1     Error     0          2m23s
pod/project-cleaner-27971629-vwncx   0/1     Error     0          2m12s
pod/project-cleaner-27971630-47rjk   0/1     Error     0          43s
pod/project-cleaner-27971630-5mcqj   0/1     Error     0          32s
pod/project-cleaner-27971630-ggr7w   1/1     Running   0          9s
pod/project-cleaner-27971630-v72n2   0/1     Error     0          21s
pod/project-cleaner-27971630-vhvj2   0/1     Error     0          55s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971629   0/1           2m40s      2m40s
job.batch/project-cleaner-27971630   0/1           72s        72s

NAME                                 READY   STATUS              RESTARTS   AGE
pod/project-cleaner-27971629-cs9x8   0/1     Error               0          109s
pod/project-cleaner-27971629-hd72r   0/1     Error               0          2m1s
pod/project-cleaner-27971629-hkljh   0/1     Error               0          85s
pod/project-cleaner-27971629-j87wg   0/1     Error               0          2m18s
pod/project-cleaner-27971629-sbdkj   0/1     Error               0          97s
pod/project-cleaner-27971629-tnjg9   0/1     Error               0          2m40s
pod/project-cleaner-27971629-vwncx   0/1     Error               0          2m29s
pod/project-cleaner-27971630-47rjk   0/1     Error               0          60s
pod/project-cleaner-27971630-5mcqj   0/1     Error               0          49s
pod/project-cleaner-27971630-f6qkk   0/1     Error               0          13s
pod/project-cleaner-27971630-gcnt5   0/1     ContainerCreating   0          2s
pod/project-cleaner-27971630-ggr7w   0/1     Error               0          26s
pod/project-cleaner-27971630-v72n2   0/1     Error               0          38s
pod/project-cleaner-27971630-vhvj2   0/1     Error               0          72s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971629   0/1           2m45s      2m45s
job.batch/project-cleaner-27971630   0/1           77s        77s

NAME                                 READY   STATUS    RESTARTS   AGE
pod/project-cleaner-27971629-cs9x8   0/1     Error     0          114s
pod/project-cleaner-27971629-hd72r   0/1     Error     0          2m6s
pod/project-cleaner-27971629-hkljh   0/1     Error     0          90s
pod/project-cleaner-27971629-j87wg   0/1     Error     0          2m23s
pod/project-cleaner-27971629-sbdkj   0/1     Error     0          102s
pod/project-cleaner-27971629-tnjg9   0/1     Error     0          2m45s
pod/project-cleaner-27971629-vwncx   0/1     Error     0          2m34s
pod/project-cleaner-27971630-47rjk   0/1     Error     0          65s
pod/project-cleaner-27971630-5mcqj   0/1     Error     0          54s
pod/project-cleaner-27971630-f6qkk   0/1     Error     0          18s
pod/project-cleaner-27971630-gcnt5   1/1     Running   0          7s
pod/project-cleaner-27971630-ggr7w   0/1     Error     0          31s
pod/project-cleaner-27971630-v72n2   0/1     Error     0          43s
pod/project-cleaner-27971630-vhvj2   0/1     Error     0          77s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971630   0/1           90s        90s
job.batch/project-cleaner-27971631   0/1           8s         8s

NAME                                 READY   STATUS              RESTARTS   AGE
pod/project-cleaner-27971630-47rjk   0/1     Error               0          78s
pod/project-cleaner-27971630-5mcqj   0/1     Error               0          67s
pod/project-cleaner-27971630-f6qkk   0/1     Error               0          31s
pod/project-cleaner-27971630-gcnt5   0/1     Error               0          20s
pod/project-cleaner-27971630-ggr7w   0/1     Error               0          44s
pod/project-cleaner-27971630-v72n2   0/1     Error               0          56s
pod/project-cleaner-27971630-vhvj2   0/1     Error               0          90s
pod/project-cleaner-27971631-c4snm   0/1     ContainerCreating   0          8s
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971630-vhvj2
Listing namespaces with label workshop:
Traceback (most recent call last):
  File "/opt/app-root/src/main.py", line 55, in <module>
    prune_namespaces(ns_label=PROJECT_TAG, min_age_seconds=int(EXPIRATION_SECONDS))
  File "/opt/app-root/src/main.py", line 16, in prune_namespaces
    for ns_name in _find_namespaces(min_age_seconds, ns_label, v1):
  File "/opt/app-root/src/main.py", line 28, in _find_namespaces
    for ns in v1.list_namespace(label_selector=ns_label).items:
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14721, in list_namespace
    return self.list_namespace_with_http_info(**kwargs)  # noqa: E501
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14828, in list_namespace_with_http_info
    return self.api_client.call_api(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 348, in call_api
    return self.__call_api(resource_path, method,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 180, in __call_api
    response_data = self.request(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 373, in request
    return self.rest_client.GET(url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 241, in GET
    return self.request("GET", url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 235, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (403)
Reason: Forbidden
HTTP response headers: HTTPHeaderDict({'Audit-Id': 'a393c16f-0d75-4114-862b-6da2697bd034', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': 'b88be2b9-eb69-4c83-a4e0-a96b73c4ddae', 'X-Kubernetes-Pf-Prioritylevel-Uid': '2a0917b5-060d-4e79-8cc5-80285a96c0c4', 'Date': 'Wed, 08 Mar 2023 17:50:42 GMT', 'Content-Length': '312'})
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"namespaces is forbidden: User \"system:serviceaccount:workshop-support:project-cleaner-sa\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope","reason":"Forbidden","details":{"kind":"namespaces"},"code":403}


[student@workstation project-cleaner]$ vim cron-job.yaml 
[student@workstation project-cleaner]$ vim cron-job.yaml 
[student@workstation project-cleaner]$ oc delete -f cron-job.yaml
cronjob.batch "project-cleaner" deleted
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 READY   STATUS        RESTARTS   AGE
pod/project-cleaner-27971633-mdjc9   1/1     Terminating   0          10s
[student@workstation project-cleaner]$ oc get jobs,pods
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ oc get jobs,pods

No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ vim cron-job.yaml 
[student@workstation project-cleaner]$ diff cron-job.yaml ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml 
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ oc project
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc get projects| grep clean 
clean-test                                                                        Active
[student@workstation project-cleaner]$ oc delete project clean-test 
project.project.openshift.io "clean-test" deleted
[student@workstation project-cleaner]$ oc get projects| grep clean 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc new-project clean-test
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ oc describe ns clean-test
Name:         clean-test
Labels:       kubernetes.io/metadata.name=clean-test
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=clean-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: admin
              openshift.io/sa.scc.mcs: s0:c27,c9
              openshift.io/sa.scc.supplemental-groups: 1000720000/10000
              openshift.io/sa.scc.uid-range: 1000720000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  cpu       -    750m   100m             500m           -
 Container  memory    -    750Mi  250Mi            500Mi          -
[student@workstation project-cleaner]$ oc project workshop-support 
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc whoami 
admin
[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc label ns clean-test workshop=
error: 'workshop' already has a value (clean-test), and --overwrite is false
[student@workstation project-cleaner]$ oc label ns clean-test workshop= --overwrite=true
namespace/clean-test labeled
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971639   0/1           53s        53s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971639-gqrlc   0/1     Error    0          24s
pod/project-cleaner-27971639-l52z2   0/1     Error    0          39s
pod/project-cleaner-27971639-p6hgc   0/1     Error    0          53s
pod/project-cleaner-27971639-spjk9   0/1     Error    0          31s
pod/project-cleaner-27971639-t5r27   0/1     Error    0          10s
pod/project-cleaner-27971639-wdb8q   0/1     Error    0          17s
pod/project-cleaner-27971639-zh4pt   0/1     Error    0          46s
[student@workstation project-cleaner]$ oc label ns clean-test workshop=clean-test --overwrite=true
namespace/clean-test labeled
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971639   0/1           70s        70s
job.batch/project-cleaner-27971640   0/1           10s        10s

NAME                                 READY   STATUS              RESTARTS   AGE
pod/project-cleaner-27971639-gqrlc   0/1     Error               0          41s
pod/project-cleaner-27971639-l52z2   0/1     Error               0          56s
pod/project-cleaner-27971639-p6hgc   0/1     Error               0          70s
pod/project-cleaner-27971639-spjk9   0/1     Error               0          48s
pod/project-cleaner-27971639-t5r27   0/1     Error               0          27s
pod/project-cleaner-27971639-wdb8q   0/1     Error               0          34s
pod/project-cleaner-27971639-zh4pt   0/1     Error               0          63s
pod/project-cleaner-27971640-95r8d   0/1     ContainerCreating   0          1s
pod/project-cleaner-27971640-wmz72   0/1     Error               0          10s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971639   0/1           73s        73s
job.batch/project-cleaner-27971640   0/1           13s        13s

NAME                                 READY   STATUS    RESTARTS   AGE
pod/project-cleaner-27971639-gqrlc   0/1     Error     0          44s
pod/project-cleaner-27971639-l52z2   0/1     Error     0          59s
pod/project-cleaner-27971639-p6hgc   0/1     Error     0          73s
pod/project-cleaner-27971639-spjk9   0/1     Error     0          51s
pod/project-cleaner-27971639-t5r27   0/1     Error     0          30s
pod/project-cleaner-27971639-wdb8q   0/1     Error     0          37s
pod/project-cleaner-27971639-zh4pt   0/1     Error     0          66s
pod/project-cleaner-27971640-95r8d   1/1     Running   0          4s
pod/project-cleaner-27971640-wmz72   0/1     Error     0          13s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971639   0/1           78s        78s
job.batch/project-cleaner-27971640   0/1           18s        18s

NAME                                 READY   STATUS              RESTARTS   AGE
pod/project-cleaner-27971639-gqrlc   0/1     Error               0          49s
pod/project-cleaner-27971639-l52z2   0/1     Error               0          64s
pod/project-cleaner-27971639-p6hgc   0/1     Error               0          78s
pod/project-cleaner-27971639-spjk9   0/1     Error               0          56s
pod/project-cleaner-27971639-t5r27   0/1     Error               0          35s
pod/project-cleaner-27971639-wdb8q   0/1     Error               0          42s
pod/project-cleaner-27971639-zh4pt   0/1     Error               0          71s
pod/project-cleaner-27971640-95r8d   0/1     Error               0          9s
pod/project-cleaner-27971640-fx47g   0/1     ContainerCreating   0          2s
pod/project-cleaner-27971640-wmz72   0/1     Error               0          18s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971639   0/1           107s       107s
job.batch/project-cleaner-27971640   0/1           47s        47s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971639-gqrlc   0/1     Error    0          78s
pod/project-cleaner-27971639-l52z2   0/1     Error    0          93s
pod/project-cleaner-27971639-p6hgc   0/1     Error    0          107s
pod/project-cleaner-27971639-spjk9   0/1     Error    0          85s
pod/project-cleaner-27971639-t5r27   0/1     Error    0          64s
pod/project-cleaner-27971639-wdb8q   0/1     Error    0          71s
pod/project-cleaner-27971639-zh4pt   0/1     Error    0          100s
pod/project-cleaner-27971640-95r8d   0/1     Error    0          38s
pod/project-cleaner-27971640-c4x5l   0/1     Error    0          17s
pod/project-cleaner-27971640-fx47g   0/1     Error    0          31s
pod/project-cleaner-27971640-ghwwx   0/1     Error    0          24s
pod/project-cleaner-27971640-jdgck   0/1     Error    0          9s
pod/project-cleaner-27971640-wmz72   0/1     Error    0          47s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971639   0/1           109s       109s
job.batch/project-cleaner-27971640   0/1           49s        49s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971639-gqrlc   0/1     Error    0          80s
pod/project-cleaner-27971639-l52z2   0/1     Error    0          95s
pod/project-cleaner-27971639-p6hgc   0/1     Error    0          109s
pod/project-cleaner-27971639-spjk9   0/1     Error    0          87s
pod/project-cleaner-27971639-t5r27   0/1     Error    0          66s
pod/project-cleaner-27971639-wdb8q   0/1     Error    0          73s
pod/project-cleaner-27971639-zh4pt   0/1     Error    0          102s
pod/project-cleaner-27971640-95r8d   0/1     Error    0          40s
pod/project-cleaner-27971640-c4x5l   0/1     Error    0          19s
pod/project-cleaner-27971640-fx47g   0/1     Error    0          33s
pod/project-cleaner-27971640-ghwwx   0/1     Error    0          26s
pod/project-cleaner-27971640-jdgck   0/1     Error    0          11s
pod/project-cleaner-27971640-wmz72   0/1     Error    0          49s
[student@workstation project-cleaner]$ oc delete -f cron-job.yaml 
cronjob.batch "project-cleaner" deleted
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get jobs,pods
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ vim cron-job.yaml 
[student@workstation project-cleaner]$ oc project
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc get projects | grep clean
clean-test                                                                        Active
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc delete project clean-test 
project.project.openshift.io "clean-test" deleted
[student@workstation project-cleaner]$ oc get projects | grep clean
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cat cron-job.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: project-cleaner
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
              imagePullPolicy: Always
              env:
              - name: "PROJECT_TAG"
                value: "workshop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:
                limits:
                  cpu: 100m
                  memory: 200Mi
          serviceAccountName: project-cleaner-sa
          securityContext:
            runAsUser: 1001
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc new-project clean-test
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc project workshop-support 
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc delete project clean-test 
project.project.openshift.io "clean-test" deleted
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc delete -f cron-job.yaml 
cronjob.batch "project-cleaner" deleted
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc whoami 
admin
[student@workstation project-cleaner]$ oc project
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml 
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ oc new-project clean-test
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc project workshop-support 
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc describe ns clean-test 
Name:         clean-test
Labels:       kubernetes.io/metadata.name=clean-test
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=clean-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: do280-support
              openshift.io/sa.scc.mcs: s0:c28,c7
              openshift.io/sa.scc.supplemental-groups: 1000770000/10000
              openshift.io/sa.scc.uid-range: 1000770000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  cpu       -    750m   100m             500m           -
 Container  memory    -    750Mi  250Mi            500Mi          -
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971647   0/1           80s        80s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971647-2xm68   0/1     Error    0          46s
pod/project-cleaner-27971647-4rw74   0/1     Error    0          23s
pod/project-cleaner-27971647-8kzlp   0/1     Error    0          57s
pod/project-cleaner-27971647-hl8xn   0/1     Error    0          80s
pod/project-cleaner-27971647-m867s   0/1     Error    0          69s
pod/project-cleaner-27971647-pjn49   0/1     Error    0          35s
pod/project-cleaner-27971647-sw27m   0/1     Error    0          11s
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971647-4rw74
Listing namespaces with label workshop:
Traceback (most recent call last):
  File "/opt/app-root/src/main.py", line 55, in <module>
    prune_namespaces(ns_label=PROJECT_TAG, min_age_seconds=int(EXPIRATION_SECONDS))
  File "/opt/app-root/src/main.py", line 16, in prune_namespaces
    for ns_name in _find_namespaces(min_age_seconds, ns_label, v1):
  File "/opt/app-root/src/main.py", line 28, in _find_namespaces
    for ns in v1.list_namespace(label_selector=ns_label).items:
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14721, in list_namespace
    return self.list_namespace_with_http_info(**kwargs)  # noqa: E501
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14828, in list_namespace_with_http_info
    return self.api_client.call_api(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 348, in call_api
    return self.__call_api(resource_path, method,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 180, in __call_api
    response_data = self.request(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 373, in request
    return self.rest_client.GET(url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 241, in GET
    return self.request("GET", url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 235, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (403)
Reason: Forbidden
HTTP response headers: HTTPHeaderDict({'Audit-Id': '5241528e-7251-4e05-be07-1b520814cb13', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': 'b88be2b9-eb69-4c83-a4e0-a96b73c4ddae', 'X-Kubernetes-Pf-Prioritylevel-Uid': '2a0917b5-060d-4e79-8cc5-80285a96c0c4', 'Date': 'Wed, 08 Mar 2023 18:08:05 GMT', 'Content-Length': '312'})
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"namespaces is forbidden: User \"system:serviceaccount:workshop-support:project-cleaner-sa\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope","reason":"Forbidden","details":{"kind":"namespaces"},"code":403}


[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971647   0/1           2m38s      2m38s
job.batch/project-cleaner-27971648   0/1           78s        78s

NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner-27971647-2xm68   0/1     Error    0          2m4s
pod/project-cleaner-27971647-4rw74   0/1     Error    0          101s
pod/project-cleaner-27971647-8kzlp   0/1     Error    0          2m15s
pod/project-cleaner-27971647-hl8xn   0/1     Error    0          2m38s
pod/project-cleaner-27971647-m867s   0/1     Error    0          2m27s
pod/project-cleaner-27971647-pjn49   0/1     Error    0          113s
pod/project-cleaner-27971647-sw27m   0/1     Error    0          89s
pod/project-cleaner-27971648-7ktsd   0/1     Error    0          22s
pod/project-cleaner-27971648-8g2s4   0/1     Error    0          44s
pod/project-cleaner-27971648-8p6xf   0/1     Error    0          33s
pod/project-cleaner-27971648-d6cmt   0/1     Error    0          66s
pod/project-cleaner-27971648-kwq65   0/1     Error    0          11s
pod/project-cleaner-27971648-ncmk7   0/1     Error    0          55s
pod/project-cleaner-27971648-twgdd   0/1     Error    0          78s
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc describe ns clean-test 
Name:         clean-test
Labels:       kubernetes.io/metadata.name=clean-test
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=clean-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: do280-support
              openshift.io/sa.scc.mcs: s0:c28,c7
              openshift.io/sa.scc.supplemental-groups: 1000770000/10000
              openshift.io/sa.scc.uid-range: 1000770000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  cpu       -    750m   100m             500m           -
 Container  memory    -    750Mi  250Mi            500Mi          -
[student@workstation project-cleaner]$ # oc label ns clean-test workshop= --overwrite=true
[student@workstation project-cleaner]$ ls
cluster-role.yaml  cron-job.yaml  example-pod.yaml
[student@workstation project-cleaner]$ cat example-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  restartPolicy: Never
  containers:
    - name: project-cleaner
      image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
      imagePullPolicy: Always
      env:
      - name: "PROJECT_TAG"
        value: "workshop"
      - name: "EXPIRATION_SECONDS"
        value: "10"
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
  serviceAccountName: CHANGE_ME
  securityContext:
    runAsUser: 1001

[student@workstation project-cleaner]$ vim example-pod.yaml
[student@workstation project-cleaner]$ oc whoami 
admin
[student@workstation project-cleaner]$ cat example-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  restartPolicy: Never
  containers:
    - name: project-cleaner
      image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
      imagePullPolicy: Always
      env:
      - name: "PROJECT_TAG"
        value: "workshop"
      - name: "EXPIRATION_SECONDS"
        value: "10"
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
  serviceAccountName: project-cleaner-sa
  securityContext:
    runAsUser: 1001

[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc create -f example-pod.yaml
pod/project-cleaner created
[student@workstation project-cleaner]$ oc get pods 
NAME                             READY   STATUS              RESTARTS   AGE
project-cleaner                  0/1     ContainerCreating   0          3s
project-cleaner-27971653-7tr75   0/1     Error               0          38s
project-cleaner-27971653-fbvbg   0/1     Error               0          61s
project-cleaner-27971653-jb6l6   0/1     Error               0          72s
project-cleaner-27971653-mth92   0/1     Error               0          27s
project-cleaner-27971653-n552x   0/1     Error               0          49s
project-cleaner-27971653-qtnng   0/1     Error               0          83s
project-cleaner-27971653-qwsbm   0/1     Error               0          95s
project-cleaner-27971654-czpjh   0/1     Error               0          16s
project-cleaner-27971654-xdrhk   0/1     ContainerCreating   0          5s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971653   0/1           105s       105s
job.batch/project-cleaner-27971654   0/1           26s        26s

NAME                                 READY   STATUS              RESTARTS   AGE
pod/project-cleaner                  0/1     Error               0          13s
pod/project-cleaner-27971653-7tr75   0/1     Error               0          48s
pod/project-cleaner-27971653-fbvbg   0/1     Error               0          71s
pod/project-cleaner-27971653-jb6l6   0/1     Error               0          82s
pod/project-cleaner-27971653-mth92   0/1     Error               0          37s
pod/project-cleaner-27971653-n552x   0/1     Error               0          59s
pod/project-cleaner-27971653-qtnng   0/1     Error               0          93s
pod/project-cleaner-27971653-qwsbm   0/1     Error               0          105s
pod/project-cleaner-27971654-9f22p   0/1     ContainerCreating   0          1s
pod/project-cleaner-27971654-czpjh   0/1     Error               0          26s
pod/project-cleaner-27971654-xdrhk   0/1     Error               0          15s
[student@workstation project-cleaner]$ watch oc get jobs,pods
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971654-9f22p
Listing namespaces with label workshop:
Traceback (most recent call last):
  File "/opt/app-root/src/main.py", line 55, in <module>
    prune_namespaces(ns_label=PROJECT_TAG, min_age_seconds=int(EXPIRATION_SECONDS))
  File "/opt/app-root/src/main.py", line 16, in prune_namespaces
    for ns_name in _find_namespaces(min_age_seconds, ns_label, v1):
  File "/opt/app-root/src/main.py", line 28, in _find_namespaces
    for ns in v1.list_namespace(label_selector=ns_label).items:
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14721, in list_namespace
    return self.list_namespace_with_http_info(**kwargs)  # noqa: E501
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14828, in list_namespace_with_http_info
    return self.api_client.call_api(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 348, in call_api
    return self.__call_api(resource_path, method,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 180, in __call_api
    response_data = self.request(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 373, in request
    return self.rest_client.GET(url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 241, in GET
    return self.request("GET", url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 235, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (403)
Reason: Forbidden
HTTP response headers: HTTPHeaderDict({'Audit-Id': '1de86ad9-30ad-451d-b6d5-db3c7c179967', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': 'b88be2b9-eb69-4c83-a4e0-a96b73c4ddae', 'X-Kubernetes-Pf-Prioritylevel-Uid': '2a0917b5-060d-4e79-8cc5-80285a96c0c4', 'Date': 'Wed, 08 Mar 2023 18:15:29 GMT', 'Content-Length': '312'})
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"namespaces is forbidden: User \"system:serviceaccount:workshop-support:project-cleaner-sa\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope","reason":"Forbidden","details":{"kind":"namespaces"},"code":403}


[student@workstation project-cleaner]$ oc whoami 
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get users.user.openshift.io ~)
[student@workstation project-cleaner]$ oc project 
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc whoami 
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get users.user.openshift.io ~)
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc whoami 
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get users.user.openshift.io ~)
[student@workstation project-cleaner]$ oc whoami 
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get users.user.openshift.io ~)
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc whoami 
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get users.user.openshift.io ~)
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971654-9f22p
Error from server (NotFound): pods "project-cleaner-27971654-9f22p" not found
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971654-9f22p


Error from server (NotFound): pods "project-cleaner-27971654-9f22p" not found
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc whoami 
admin
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get scc anyuid -o yaml 
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups:
- system:cluster-admins
kind: SecurityContextConstraints
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    kubernetes.io/description: anyuid provides all features of the restricted SCC
      but allows users to run with any UID and any GID.
    release.openshift.io/create-only: "true"
  creationTimestamp: "2023-02-24T17:05:56Z"
  generation: 1
  name: anyuid
  resourceVersion: "391"
  uid: fdbd714f-5e96-431d-8f23-a22be87375f9
priority: 10
readOnlyRootFilesystem: false
requiredDropCapabilities:
- MKNOD
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- ephemeral
- persistentVolumeClaim
- projected
- secret
[student@workstation project-cleaner]$ oc project
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc get sa 
NAME                 SECRETS   AGE
builder              1         50m
default              1         50m
deployer             1         50m
project-cleaner-sa   1         46m
[student@workstation project-cleaner]$ oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "project-cleaner-sa"
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get scc anyuid -o yaml 
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups:
- system:cluster-admins
kind: SecurityContextConstraints
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    kubernetes.io/description: anyuid provides all features of the restricted SCC
      but allows users to run with any UID and any GID.
    release.openshift.io/create-only: "true"
  creationTimestamp: "2023-02-24T17:05:56Z"
  generation: 1
  name: anyuid
  resourceVersion: "391"
  uid: fdbd714f-5e96-431d-8f23-a22be87375f9
priority: 10
readOnlyRootFilesystem: false
requiredDropCapabilities:
- MKNOD
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- ephemeral
- persistentVolumeClaim
- projected
- secret
[student@workstation project-cleaner]$ cat cluster-role.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: project-cleaner
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - delete

[student@workstation project-cleaner]$ oc apply -f cluster-role.yaml
clusterrole.rbac.authorization.k8s.io/project-cleaner unchanged
[student@workstation project-cleaner]$ oc adm policy add-role-to-user project-cleaner -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/project-cleaner added: "project-cleaner-sa"
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc get pods,jobs
NAME                                 READY   STATUS   RESTARTS   AGE
pod/project-cleaner                  0/1     Error    0          11m
pod/project-cleaner-27971664-59rnx   0/1     Error    0          116s
pod/project-cleaner-27971664-62fvh   0/1     Error    0          58s
pod/project-cleaner-27971664-dlmjf   0/1     Error    0          2m7s
pod/project-cleaner-27971664-kq97q   0/1     Error    0          69s
pod/project-cleaner-27971664-ph2z6   0/1     Error    0          92s
pod/project-cleaner-27971664-r2jbv   0/1     Error    0          81s
pod/project-cleaner-27971664-z8pml   0/1     Error    0          104s
pod/project-cleaner-27971665-7rgqw   0/1     Error    0          23s
pod/project-cleaner-27971665-br47m   0/1     Error    0          47s
pod/project-cleaner-27971665-m6v5l   0/1     Error    0          11s
pod/project-cleaner-27971665-x8p48   0/1     Error    0          34s

NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971664   0/1           2m7s       2m7s
job.batch/project-cleaner-27971665   0/1           47s        47s
[student@workstation project-cleaner]$ oc get clusterrole
clusterrolebindings.authorization.openshift.io  clusterroles.authorization.openshift.io         
clusterrolebindings.rbac.authorization.k8s.io   clusterroles.rbac.authorization.k8s.io          
[student@workstation project-cleaner]$ oc get clusterrole
clusterrolebindings.authorization.openshift.io  clusterroles.authorization.openshift.io         
clusterrolebindings.rbac.authorization.k8s.io   clusterroles.rbac.authorization.k8s.io          
[student@workstation project-cleaner]$ oc get clusterrole
clusterrolebindings.authorization.openshift.io  clusterroles.authorization.openshift.io         
clusterrolebindings.rbac.authorization.k8s.io   clusterroles.rbac.authorization.k8s.io          
[student@workstation project-cleaner]$ oc get clusterrole
clusterrolebindings.authorization.openshift.io  clusterroles.authorization.openshift.io         
clusterrolebindings.rbac.authorization.k8s.io   clusterroles.rbac.authorization.k8s.io          
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc delete -f cron-job.yaml 
cronjob.batch "project-cleaner" deleted
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ o cget clusterrole | grep project 
bash: o: command not found...
[student@workstation project-cleaner]$ oc get clusterrole | grep project 
net-attach-def-project                                                      2023-02-24T17:14:27Z
project-cleaner                                                             2023-03-08T17:38:07Z
project-helm-chartrepository-editor                                         2023-02-24T17:33:49Z
[student@workstation project-cleaner]$ oc describe clusterrole project-cleaner
Name:         project-cleaner
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  namespaces  []                 []              [get list delete]
[student@workstation project-cleaner]$ oc get clusterrolebinding | grep project 
[student@workstation project-cleaner]$ oc get clusterrolebinding 
NAME                                                                        ROLE                                                                                    AGE
admin                                                                       ClusterRole/admin                                                                       153m
alertmanager-main                                                           ClusterRole/alertmanager-main                                                           12d
basic-users                                                                 ClusterRole/basic-user                                                                  12d
cloud-controller-manager                                                    ClusterRole/cloud-controller-manager                                                    12d
cloud-credential-operator-rolebinding                                       ClusterRole/cloud-credential-operator-role                                              12d
cloud-node-manager                                                          ClusterRole/cloud-node-manager                                                          12d
cluster-admin                                                               ClusterRole/cluster-admin                                                               12d
cluster-admin-0                                                             ClusterRole/cluster-admin                                                               11d
cluster-admin-1                                                             ClusterRole/cluster-admin                                                               2d2h
cluster-admin-2                                                             ClusterRole/cluster-admin                                                               152m
cluster-admins                                                              ClusterRole/cluster-admin                                                               12d
cluster-autoscaler                                                          ClusterRole/cluster-autoscaler                                                          12d
cluster-autoscaler-operator                                                 ClusterRole/cluster-autoscaler-operator                                                 12d
cluster-baremetal-operator                                                  ClusterRole/cluster-baremetal-operator                                                  12d
cluster-monitoring-operator                                                 ClusterRole/cluster-monitoring-operator                                                 12d
cluster-node-tuning-operator                                                ClusterRole/cluster-node-tuning-operator                                                12d
cluster-node-tuning:tuned                                                   ClusterRole/cluster-node-tuning:tuned                                                   12d
cluster-readers                                                             ClusterRole/cluster-reader                                                              12d
cluster-samples-operator                                                    ClusterRole/cluster-samples-operator                                                    12d
cluster-samples-operator-imageconfig-reader                                 ClusterRole/cluster-samples-operator-imageconfig-reader                                 12d
cluster-samples-operator-proxy-reader                                       ClusterRole/cluster-samples-operator-proxy-reader                                       12d
cluster-status-binding                                                      ClusterRole/cluster-status                                                              12d
cluster-storage-operator-role                                               ClusterRole/cluster-admin                                                               12d
cluster-version-operator                                                    ClusterRole/cluster-admin                                                               12d
console                                                                     ClusterRole/console                                                                     12d
console-auth-delegator                                                      ClusterRole/system:auth-delegator                                                       12d
console-extensions-reader                                                   ClusterRole/console-extensions-reader                                                   12d
console-operator                                                            ClusterRole/console-operator                                                            12d
console-operator-auth-delegator                                             ClusterRole/system:auth-delegator                                                       12d
control-plane-machine-set-operator                                          ClusterRole/control-plane-machine-set-operator                                          12d
csi-snapshot-controller-operator-clusterrole                                ClusterRole/csi-snapshot-controller-operator-clusterrole                                12d
csi-snapshot-controller-runner-operator                                     ClusterRole/openshift-csi-snapshot-controller-runner                                    12d
default-account-cluster-image-registry-operator                             ClusterRole/cluster-image-registry-operator                                             12d
default-account-cluster-network-operator                                    ClusterRole/cluster-admin                                                               12d
default-account-openshift-machine-config-operator                           ClusterRole/cluster-admin                                                               12d
dns-monitoring                                                              ClusterRole/dns-monitoring                                                              12d
helm-chartrepos-view                                                        ClusterRole/helm-chartrepos-viewer                                                      12d
insights-operator                                                           ClusterRole/insights-operator                                                           12d
insights-operator-auth                                                      ClusterRole/system:auth-delegator                                                       12d
insights-operator-gather                                                    ClusterRole/insights-operator-gather                                                    12d
insights-operator-gather-reader                                             ClusterRole/cluster-reader                                                              12d
kube-apiserver                                                              ClusterRole/kube-apiserver                                                              12d
kube-state-metrics                                                          ClusterRole/kube-state-metrics                                                          12d
lvms-operator-service-system:auth-delegator                                 ClusterRole/system:auth-delegator                                                       11d
lvms-operator.v4.12.0-58b6bdc8dc                                            ClusterRole/lvms-operator.v4.12.0-58b6bdc8dc                                            11d
machine-api-controllers                                                     ClusterRole/machine-api-controllers                                                     12d
machine-api-operator                                                        ClusterRole/machine-api-operator                                                        12d
machine-api-operator-ext-remediation                                        ClusterRole/machine-api-operator-ext-remediation                                        12d
machine-config-controller                                                   ClusterRole/machine-config-controller                                                   12d
machine-config-daemon                                                       ClusterRole/machine-config-daemon                                                       12d
machine-config-server                                                       ClusterRole/machine-config-server                                                       12d
manage-groups                                                               ClusterRole/manage-groups                                                               152m
marketplace-operator                                                        ClusterRole/marketplace-operator                                                        12d
metallb-operator-controller-manager-service-system:auth-delegator           ClusterRole/system:auth-delegator                                                       11d
metallb-operator-webhook-server-service-system:auth-delegator               ClusterRole/system:auth-delegator                                                       11d
metallb-operator.4.12.0-202302061702-7d4674d6c5                             ClusterRole/metallb-operator.4.12.0-202302061702-7d4674d6c5                             11d
metallb-operator.4.12.0-202302061702-c574f659c                              ClusterRole/metallb-operator.4.12.0-202302061702-c574f659c                              11d
metallb-operator.4.12.0-202302061702-controller-6f976d6dbc                  ClusterRole/metallb-operator.4.12.0-202302061702-controller-6f976d6dbc                  11d
metallb-operator.4.12.0-202302061702-dd6c5c46f                              ClusterRole/metallb-operator.4.12.0-202302061702-dd6c5c46f                              11d
metallb-operator.4.12.0-202302061702-default-579877c844                     ClusterRole/metallb-operator.4.12.0-202302061702-default-579877c844                     11d
metallb-operator.4.12.0-202302061702-speaker-76545f9755                     ClusterRole/metallb-operator.4.12.0-202302061702-speaker-76545f9755                     11d
metrics-daemon-sa-rolebinding                                               ClusterRole/metrics-daemon-role                                                         12d
multus                                                                      ClusterRole/multus                                                                      12d
multus-admission-controller-webhook                                         ClusterRole/multus-admission-controller-webhook                                         12d
multus-whereabouts                                                          ClusterRole/whereabouts-cni                                                             12d
network-diagnostics                                                         ClusterRole/network-diagnostics                                                         12d
node-exporter                                                               ClusterRole/node-exporter                                                               12d
olm-operator-binding-openshift-operator-lifecycle-manager                   ClusterRole/system:controller:operator-lifecycle-manager                                12d
openshift-csi-snapshot-controller-role                                      ClusterRole/openshift-csi-snapshot-controller-runner                                    12d
openshift-dns                                                               ClusterRole/openshift-dns                                                               12d
openshift-dns-operator                                                      ClusterRole/openshift-dns-operator                                                      12d
openshift-image-registry-pruner                                             ClusterRole/system:image-pruner                                                         12d
openshift-ingress-operator                                                  ClusterRole/openshift-ingress-operator                                                  12d
openshift-ingress-router                                                    ClusterRole/openshift-ingress-router                                                    12d
openshift-ovn-kubernetes-controller                                         ClusterRole/openshift-ovn-kubernetes-controller                                         12d
openshift-ovn-kubernetes-node                                               ClusterRole/openshift-ovn-kubernetes-node                                               12d
openshift-state-metrics                                                     ClusterRole/openshift-state-metrics                                                     12d
openstack-cloud-controller-manager                                          ClusterRole/openstack-cloud-controller-manager                                          12d
packageserver-service-system:auth-delegator                                 ClusterRole/system:auth-delegator                                                       12d
prometheus-adapter                                                          ClusterRole/prometheus-adapter                                                          12d
prometheus-adapter-view                                                     ClusterRole/cluster-monitoring-view                                                     12d
prometheus-k8s                                                              ClusterRole/prometheus-k8s                                                              12d
prometheus-k8s-scheduler-resources                                          ClusterRole/prometheus-k8s-scheduler-resources                                          12d
prometheus-operator                                                         ClusterRole/prometheus-operator                                                         12d
registry-monitoring                                                         ClusterRole/registry-monitoring                                                         12d
registry-registry-role                                                      ClusterRole/system:registry                                                             12d
resource-metrics:system:auth-delegator                                      ClusterRole/system:auth-delegator                                                       12d
router-monitoring                                                           ClusterRole/router-monitoring                                                           12d
run-nfs-client-provisioner                                                  ClusterRole/nfs-client-provisioner-runner                                               12d
self-access-reviewers                                                       ClusterRole/self-access-reviewer                                                        12d
self-provisioners                                                           ClusterRole/self-provisioner                                                            2d1h
storage-version-migration-migrator                                          ClusterRole/cluster-admin                                                               12d
system-bootstrap-node-bootstrapper                                          ClusterRole/system:node-bootstrapper                                                    12d
system-bootstrap-node-renewal                                               ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient        12d
system:basic-user                                                           ClusterRole/system:basic-user                                                           12d
system:build-strategy-docker-binding                                        ClusterRole/system:build-strategy-docker                                                12d
system:build-strategy-jenkinspipeline-binding                               ClusterRole/system:build-strategy-jenkinspipeline                                       12d
system:build-strategy-source-binding                                        ClusterRole/system:build-strategy-source                                                12d
system:controller:attachdetach-controller                                   ClusterRole/system:controller:attachdetach-controller                                   12d
system:controller:certificate-controller                                    ClusterRole/system:controller:certificate-controller                                    12d
system:controller:clusterrole-aggregation-controller                        ClusterRole/system:controller:clusterrole-aggregation-controller                        12d
system:controller:cronjob-controller                                        ClusterRole/system:controller:cronjob-controller                                        12d
system:controller:daemon-set-controller                                     ClusterRole/system:controller:daemon-set-controller                                     12d
system:controller:deployment-controller                                     ClusterRole/system:controller:deployment-controller                                     12d
system:controller:disruption-controller                                     ClusterRole/system:controller:disruption-controller                                     12d
system:controller:endpoint-controller                                       ClusterRole/system:controller:endpoint-controller                                       12d
system:controller:endpointslice-controller                                  ClusterRole/system:controller:endpointslice-controller                                  12d
system:controller:endpointslicemirroring-controller                         ClusterRole/system:controller:endpointslicemirroring-controller                         12d
system:controller:ephemeral-volume-controller                               ClusterRole/system:controller:ephemeral-volume-controller                               12d
system:controller:expand-controller                                         ClusterRole/system:controller:expand-controller                                         12d
system:controller:generic-garbage-collector                                 ClusterRole/system:controller:generic-garbage-collector                                 12d
system:controller:horizontal-pod-autoscaler                                 ClusterRole/system:controller:horizontal-pod-autoscaler                                 12d
system:controller:job-controller                                            ClusterRole/system:controller:job-controller                                            12d
system:controller:namespace-controller                                      ClusterRole/system:controller:namespace-controller                                      12d
system:controller:node-controller                                           ClusterRole/system:controller:node-controller                                           12d
system:controller:persistent-volume-binder                                  ClusterRole/system:controller:persistent-volume-binder                                  12d
system:controller:pod-garbage-collector                                     ClusterRole/system:controller:pod-garbage-collector                                     12d
system:controller:pv-protection-controller                                  ClusterRole/system:controller:pv-protection-controller                                  12d
system:controller:pvc-protection-controller                                 ClusterRole/system:controller:pvc-protection-controller                                 12d
system:controller:replicaset-controller                                     ClusterRole/system:controller:replicaset-controller                                     12d
system:controller:replication-controller                                    ClusterRole/system:controller:replication-controller                                    12d
system:controller:resourcequota-controller                                  ClusterRole/system:controller:resourcequota-controller                                  12d
system:controller:root-ca-cert-publisher                                    ClusterRole/system:controller:root-ca-cert-publisher                                    12d
system:controller:route-controller                                          ClusterRole/system:controller:route-controller                                          12d
system:controller:service-account-controller                                ClusterRole/system:controller:service-account-controller                                12d
system:controller:service-ca-cert-publisher                                 ClusterRole/system:controller:service-ca-cert-publisher                                 12d
system:controller:service-controller                                        ClusterRole/system:controller:service-controller                                        12d
system:controller:statefulset-controller                                    ClusterRole/system:controller:statefulset-controller                                    12d
system:controller:ttl-after-finished-controller                             ClusterRole/system:controller:ttl-after-finished-controller                             12d
system:controller:ttl-controller                                            ClusterRole/system:controller:ttl-controller                                            12d
system:deployer                                                             ClusterRole/system:deployer                                                             12d
system:discovery                                                            ClusterRole/system:discovery                                                            12d
system:image-builder                                                        ClusterRole/system:image-builder                                                        12d
system:image-puller                                                         ClusterRole/system:image-puller                                                         12d
system:kube-controller-manager                                              ClusterRole/system:kube-controller-manager                                              12d
system:kube-dns                                                             ClusterRole/system:kube-dns                                                             12d
system:kube-scheduler                                                       ClusterRole/system:kube-scheduler                                                       12d
system:masters                                                              ClusterRole/system:master                                                               12d
system:monitoring                                                           ClusterRole/system:monitoring                                                           12d
system:node                                                                 ClusterRole/system:node                                                                 12d
system:node-admin                                                           ClusterRole/system:node-admin                                                           12d
system:node-admins                                                          ClusterRole/system:node-admin                                                           12d
system:node-bootstrapper                                                    ClusterRole/system:node-bootstrapper                                                    12d
system:node-proxier                                                         ClusterRole/system:node-proxier                                                         12d
system:node-proxiers                                                        ClusterRole/system:node-proxier                                                         12d
system:oauth-token-deleters                                                 ClusterRole/system:oauth-token-deleter                                                  12d
system:openshift:controller:build-config-change-controller                  ClusterRole/system:openshift:controller:build-config-change-controller                  12d
system:openshift:controller:build-controller                                ClusterRole/system:openshift:controller:build-controller                                12d
system:openshift:controller:cluster-csr-approver-controller                 ClusterRole/system:openshift:controller:cluster-csr-approver-controller                 12d
system:openshift:controller:cluster-quota-reconciliation-controller         ClusterRole/system:openshift:controller:cluster-quota-reconciliation-controller         12d
system:openshift:controller:default-rolebindings-controller                 ClusterRole/system:openshift:controller:default-rolebindings-controller                 12d
system:openshift:controller:deployer-controller                             ClusterRole/system:openshift:controller:deployer-controller                             12d
system:openshift:controller:deploymentconfig-controller                     ClusterRole/system:openshift:controller:deploymentconfig-controller                     12d
system:openshift:controller:horizontal-pod-autoscaler                       ClusterRole/system:openshift:controller:horizontal-pod-autoscaler                       12d
system:openshift:controller:image-import-controller                         ClusterRole/system:openshift:controller:image-import-controller                         12d
system:openshift:controller:image-trigger-controller                        ClusterRole/system:openshift:controller:image-trigger-controller                        12d
system:openshift:controller:kube-apiserver-check-endpoints-auth-delegator   ClusterRole/system:auth-delegator                                                       12d
system:openshift:controller:kube-apiserver-check-endpoints-crd-reader       ClusterRole/system:openshift:controller:check-endpoints-crd-reader                      12d
system:openshift:controller:kube-apiserver-check-endpoints-node-reader      ClusterRole/system:openshift:controller:check-endpoints-node-reader                     12d
system:openshift:controller:machine-approver                                ClusterRole/system:openshift:controller:machine-approver                                12d
system:openshift:controller:namespace-security-allocation-controller        ClusterRole/system:openshift:controller:namespace-security-allocation-controller        12d
system:openshift:controller:origin-namespace-controller                     ClusterRole/system:openshift:controller:origin-namespace-controller                     12d
system:openshift:controller:podsecurity-admission-label-syncer-controller   ClusterRole/system:openshift:controller:podsecurity-admission-label-syncer-controller   12d
system:openshift:controller:pv-recycler-controller                          ClusterRole/system:openshift:controller:pv-recycler-controller                          12d
system:openshift:controller:resourcequota-controller                        ClusterRole/system:openshift:controller:resourcequota-controller                        12d
system:openshift:controller:service-ca                                      ClusterRole/system:openshift:controller:service-ca                                      12d
system:openshift:controller:service-ingress-ip-controller                   ClusterRole/system:openshift:controller:service-ingress-ip-controller                   12d
system:openshift:controller:service-serving-cert-controller                 ClusterRole/system:openshift:controller:service-serving-cert-controller                 12d
system:openshift:controller:serviceaccount-controller                       ClusterRole/system:openshift:controller:serviceaccount-controller                       12d
system:openshift:controller:serviceaccount-pull-secrets-controller          ClusterRole/system:openshift:controller:serviceaccount-pull-secrets-controller          12d
system:openshift:controller:template-instance-controller                    ClusterRole/system:openshift:controller:template-instance-controller                    12d
system:openshift:controller:template-instance-controller:admin              ClusterRole/admin                                                                       12d
system:openshift:controller:template-instance-finalizer-controller          ClusterRole/system:openshift:controller:template-instance-finalizer-controller          12d
system:openshift:controller:template-instance-finalizer-controller:admin    ClusterRole/admin                                                                       12d
system:openshift:controller:template-service-broker                         ClusterRole/system:openshift:controller:template-service-broker                         12d
system:openshift:controller:unidling-controller                             ClusterRole/system:openshift:controller:unidling-controller                             12d
system:openshift:discovery                                                  ClusterRole/system:openshift:discovery                                                  12d
system:openshift:kube-controller-manager:gce-cloud-provider                 ClusterRole/system:openshift:kube-controller-manager:gce-cloud-provider                 12d
system:openshift:oauth-apiserver                                            ClusterRole/cluster-admin                                                               12d
system:openshift:openshift-apiserver                                        ClusterRole/cluster-admin                                                               12d
system:openshift:openshift-authentication                                   ClusterRole/cluster-admin                                                               12d
system:openshift:openshift-controller-manager                               ClusterRole/system:openshift:openshift-controller-manager                               12d
system:openshift:openshift-controller-manager:image-trigger-controller      ClusterRole/system:openshift:openshift-controller-manager:image-trigger-controller      12d
system:openshift:openshift-controller-manager:ingress-to-route-controller   ClusterRole/system:openshift:openshift-controller-manager:ingress-to-route-controller   12d
system:openshift:openshift-controller-manager:update-buildconfig-status     ClusterRole/system:openshift:openshift-controller-manager:update-buildconfig-status     12d
system:openshift:openshift-route-controller-manager                         ClusterRole/system:openshift:openshift-route-controller-manager                         12d
system:openshift:operator:authentication                                    ClusterRole/cluster-admin                                                               12d
system:openshift:operator:cloud-controller-manager                          ClusterRole/system:openshift:operator:cloud-controller-manager                          12d
system:openshift:operator:cluster-kube-scheduler-operator                   ClusterRole/cluster-admin                                                               12d
system:openshift:operator:etcd-operator                                     ClusterRole/cluster-admin                                                               12d
system:openshift:operator:kube-apiserver-operator                           ClusterRole/cluster-admin                                                               12d
system:openshift:operator:kube-apiserver-recovery                           ClusterRole/cluster-admin                                                               12d
system:openshift:operator:kube-controller-manager-operator                  ClusterRole/cluster-admin                                                               12d
system:openshift:operator:kube-controller-manager-recovery                  ClusterRole/cluster-admin                                                               12d
system:openshift:operator:kube-scheduler-recovery                           ClusterRole/cluster-admin                                                               12d
system:openshift:operator:kube-scheduler:public-2                           ClusterRole/system:kube-scheduler                                                       12d
system:openshift:operator:kube-storage-version-migrator-operator            ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-apiserver-operator                      ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-config-operator                         ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-controller-manager-operator             ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-etcd-installer                          ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-kube-apiserver-installer                ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-kube-controller-manager-installer       ClusterRole/cluster-admin                                                               12d
system:openshift:operator:openshift-kube-scheduler-installer                ClusterRole/cluster-admin                                                               12d
system:openshift:operator:service-ca-operator                               ClusterRole/cluster-admin                                                               12d
system:openshift:public-info-viewer                                         ClusterRole/system:openshift:public-info-viewer                                         12d
system:openshift:scc:anyuid                                                 ClusterRole/system:openshift:scc:anyuid                                                 5h46m
system:openshift:scc:hostmount-anyuid                                       ClusterRole/system:openshift:scc:hostmount-anyuid                                       12d
system:openshift:scc:restricted-v2                                          ClusterRole/system:openshift:scc:restricted-v2                                          12d
system:openshift:tokenreview-openshift-controller-manager                   ClusterRole/system:openshift:tokenreview-openshift-controller-manager                   12d
system:openshift:tokenreview-openshift-route-controller-manager             ClusterRole/system:openshift:tokenreview-openshift-route-controller-manager             12d
system:openshift:useroauthaccesstoken-manager                               ClusterRole/system:openshift:useroauthaccesstoken-manager                               12d
system:public-info-viewer                                                   ClusterRole/system:public-info-viewer                                                   12d
system:scope-impersonation                                                  ClusterRole/system:scope-impersonation                                                  12d
system:sdn-readers                                                          ClusterRole/system:sdn-reader                                                           12d
system:service-account-issuer-discovery                                     ClusterRole/system:service-account-issuer-discovery                                     12d
system:volume-scheduler                                                     ClusterRole/system:volume-scheduler                                                     12d
system:webhooks                                                             ClusterRole/system:webhook                                                              12d
thanos-querier                                                              ClusterRole/thanos-querier                                                              12d
topolvm-controller                                                          ClusterRole/topolvm-controller                                                          11d
topolvm-csi-provisioner                                                     ClusterRole/topolvm-csi-provisioner                                                     11d
topolvm-csi-resizer                                                         ClusterRole/topolvm-csi-resizer                                                         11d
topolvm-csi-snapshotter                                                     ClusterRole/topolvm-csi-snapshotter                                                     11d
topolvm-node                                                                ClusterRole/topolvm-node                                                                11d
vg-manager-clusterrolebinding                                               ClusterRole/vg-manager-clusterrole                                                      11d
[student@workstation project-cleaner]$ oc get clusterrolebinding  |grep cleaner
[student@workstation project-cleaner]$ oc get rolebinding -o wide -n project-cleaner
No resources found in project-cleaner namespace.
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get rolebinding -n project-cleaner
No resources found in project-cleaner namespace.
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc whomai
error: unknown command "whomai" for "oc"

Did you mean this?
	whoami
[student@workstation project-cleaner]$ oc whoami 
admin
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc project
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc project project-cleaner
error: A project named "project-cleaner" does not exist on "https://api.ocp4.example.com:6443".
Your projects are:
* clean-test
* default
* kube-node-lease
* kube-public
* kube-system
* metallb-system
* NFS Provisioner (nfs-client-provisioner)
* openshift
* openshift-apiserver
* openshift-apiserver-operator
* openshift-authentication
* openshift-authentication-operator
* openshift-cloud-controller-manager
* openshift-cloud-controller-manager-operator
* openshift-cloud-credential-operator
* openshift-cloud-network-config-controller
* openshift-cluster-csi-drivers
* openshift-cluster-machine-approver
* openshift-cluster-node-tuning-operator
* openshift-cluster-samples-operator
* openshift-cluster-storage-operator
* openshift-cluster-version
* openshift-compliance
* openshift-config
* openshift-config-managed
* openshift-config-operator
* openshift-console
* openshift-console-operator
* openshift-console-user-settings
* openshift-controller-manager
* openshift-controller-manager-operator
* openshift-dns
* openshift-dns-operator
* openshift-etcd
* openshift-etcd-operator
* openshift-host-network
* openshift-image-registry
* openshift-infra
* openshift-ingress
* openshift-ingress-canary
* openshift-ingress-operator
* openshift-insights
* openshift-kni-infra
* openshift-kube-apiserver
* openshift-kube-apiserver-operator
* openshift-kube-controller-manager
* openshift-kube-controller-manager-operator
* openshift-kube-scheduler
* openshift-kube-scheduler-operator
* openshift-kube-storage-version-migrator
* openshift-kube-storage-version-migrator-operator
* openshift-machine-api
* openshift-machine-config-operator
* openshift-marketplace
* openshift-monitoring
* openshift-multus
* openshift-network-diagnostics
* openshift-network-operator
* openshift-node
* openshift-nutanix-infra
* openshift-oauth-apiserver
* openshift-openstack-infra
* openshift-operator-lifecycle-manager
* openshift-operators
* openshift-ovirt-infra
* openshift-ovn-kubernetes
* openshift-route-controller-manager
* openshift-service-ca
* openshift-service-ca-operator
* openshift-storage
* openshift-user-workload-monitoring
* openshift-vsphere-infra
* Console chapter applications (webconsole-apps)
* workshop-support
[student@workstation project-cleaner]$ oc get rolebinding 
NAME                    ROLE                               AGE
admin                   ClusterRole/admin                  57m
admin-0                 ClusterRole/admin                  56m
project-cleaner         ClusterRole/project-cleaner        51m
system:deployers        ClusterRole/system:deployer        57m
system:image-builders   ClusterRole/system:image-builder   57m
system:image-pullers    ClusterRole/system:image-puller    57m
[student@workstation project-cleaner]$ oc get rolebinding  -o wide 
NAME                    ROLE                               AGE   USERS   GROUPS                                    SERVICEACCOUNTS
admin                   ClusterRole/admin                  57m   admin                                             
admin-0                 ClusterRole/admin                  56m           workshop-support                          
project-cleaner         ClusterRole/project-cleaner        51m                                                     workshop-support/project-cleaner-sa
system:deployers        ClusterRole/system:deployer        57m                                                     workshop-support/deployer
system:image-builders   ClusterRole/system:image-builder   57m                                                     workshop-support/builder
system:image-pullers    ClusterRole/system:image-puller    57m           system:serviceaccounts:workshop-support   
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get cj
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ cat cron-job.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: project-cleaner
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
              imagePullPolicy: Always
              env:
              - name: "PROJECT_TAG"
                value: "workshop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:
                limits:
                  cpu: 100m
                  memory: 200Mi
          serviceAccountName: project-cleaner-sa
          securityContext:
            runAsUser: 1001
[student@workstation project-cleaner]$ # oc apply -f cron-job.yaml
[student@workstation project-cleaner]$ oc get projects
NAME                                               DISPLAY NAME                   STATUS
clean-test                                                                        Active
default                                                                           Active
kube-node-lease                                                                   Active
kube-public                                                                       Active
kube-system                                                                       Active
metallb-system                                                                    Active
nfs-client-provisioner                             NFS Provisioner                Active
openshift                                                                         Active
openshift-apiserver                                                               Active
openshift-apiserver-operator                                                      Active
openshift-authentication                                                          Active
openshift-authentication-operator                                                 Active
openshift-cloud-controller-manager                                                Active
openshift-cloud-controller-manager-operator                                       Active
openshift-cloud-credential-operator                                               Active
openshift-cloud-network-config-controller                                         Active
openshift-cluster-csi-drivers                                                     Active
openshift-cluster-machine-approver                                                Active
openshift-cluster-node-tuning-operator                                            Active
openshift-cluster-samples-operator                                                Active
openshift-cluster-storage-operator                                                Active
openshift-cluster-version                                                         Active
openshift-compliance                                                              Terminating
openshift-config                                                                  Active
openshift-config-managed                                                          Active
openshift-config-operator                                                         Active
openshift-console                                                                 Active
openshift-console-operator                                                        Active
openshift-console-user-settings                                                   Active
openshift-controller-manager                                                      Active
openshift-controller-manager-operator                                             Active
openshift-dns                                                                     Active
openshift-dns-operator                                                            Active
openshift-etcd                                                                    Active
openshift-etcd-operator                                                           Active
openshift-host-network                                                            Active
openshift-image-registry                                                          Active
openshift-infra                                                                   Active
openshift-ingress                                                                 Active
openshift-ingress-canary                                                          Active
openshift-ingress-operator                                                        Active
openshift-insights                                                                Active
openshift-kni-infra                                                               Active
openshift-kube-apiserver                                                          Active
openshift-kube-apiserver-operator                                                 Active
openshift-kube-controller-manager                                                 Active
openshift-kube-controller-manager-operator                                        Active
openshift-kube-scheduler                                                          Active
openshift-kube-scheduler-operator                                                 Active
openshift-kube-storage-version-migrator                                           Active
openshift-kube-storage-version-migrator-operator                                  Active
openshift-machine-api                                                             Active
openshift-machine-config-operator                                                 Active
openshift-marketplace                                                             Active
openshift-monitoring                                                              Active
openshift-multus                                                                  Active
openshift-network-diagnostics                                                     Active
openshift-network-operator                                                        Active
openshift-node                                                                    Active
openshift-nutanix-infra                                                           Active
openshift-oauth-apiserver                                                         Active
openshift-openstack-infra                                                         Active
openshift-operator-lifecycle-manager                                              Active
openshift-operators                                                               Active
openshift-ovirt-infra                                                             Active
openshift-ovn-kubernetes                                                          Active
openshift-route-controller-manager                                                Active
openshift-service-ca                                                              Active
openshift-service-ca-operator                                                     Active
openshift-storage                                                                 Active
openshift-user-workload-monitoring                                                Active
openshift-vsphere-infra                                                           Active
webconsole-apps                                    Console chapter applications   Active
workshop-support                                                                  Active
[student@workstation project-cleaner]$ oc delete project clean-test 
project.project.openshift.io "clean-test" deleted
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc whoami 
do280-support
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get projects | grep clean 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml 
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc new-project clean-test 
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ oc project workshop-support 
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc describe ns clean-test 
Name:         clean-test
Labels:       kubernetes.io/metadata.name=clean-test
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=clean-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: do280-support
              openshift.io/sa.scc.mcs: s0:c28,c12
              openshift.io/sa.scc.supplemental-groups: 1000780000/10000
              openshift.io/sa.scc.uid-range: 1000780000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  memory    -    750Mi  250Mi            500Mi          -
 Container  cpu       -    750m   100m             500m           -
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971672   0/1           36s        36s

NAME                                 READY   STATUS              RESTARTS   AGE
pod/project-cleaner                  0/1     Error               0          17m
pod/project-cleaner-27971672-9dvkw   0/1     ContainerCreating   0          1s
pod/project-cleaner-27971672-9qgn6   0/1     Error               0          24s
pod/project-cleaner-27971672-bh492   0/1     Error               0          12s
pod/project-cleaner-27971672-lrjrn   0/1     Error               0          36s
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971672-9dvkw
Listing namespaces with label workshop:
Traceback (most recent call last):
  File "/opt/app-root/src/main.py", line 55, in <module>
    prune_namespaces(ns_label=PROJECT_TAG, min_age_seconds=int(EXPIRATION_SECONDS))
  File "/opt/app-root/src/main.py", line 16, in prune_namespaces
    for ns_name in _find_namespaces(min_age_seconds, ns_label, v1):
  File "/opt/app-root/src/main.py", line 28, in _find_namespaces
    for ns in v1.list_namespace(label_selector=ns_label).items:
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14721, in list_namespace
    return self.list_namespace_with_http_info(**kwargs)  # noqa: E501
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14828, in list_namespace_with_http_info
    return self.api_client.call_api(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 348, in call_api
    return self.__call_api(resource_path, method,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 180, in __call_api
    response_data = self.request(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 373, in request
    return self.rest_client.GET(url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 241, in GET
    return self.request("GET", url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 235, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (403)
Reason: Forbidden
HTTP response headers: HTTPHeaderDict({'Audit-Id': '3a04d579-ada9-4df7-9fa9-8537df295b83', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': 'b88be2b9-eb69-4c83-a4e0-a96b73c4ddae', 'X-Kubernetes-Pf-Prioritylevel-Uid': '2a0917b5-060d-4e79-8cc5-80285a96c0c4', 'Date': 'Wed, 08 Mar 2023 18:32:44 GMT', 'Content-Length': '312'})
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"namespaces is forbidden: User \"system:serviceaccount:workshop-support:project-cleaner-sa\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope","reason":"Forbidden","details":{"kind":"namespaces"},"code":403}


[student@workstation project-cleaner]$ oc logs pod/project-cleaner
Listing namespaces with label workshop:
Traceback (most recent call last):
  File "/opt/app-root/src/main.py", line 55, in <module>
    prune_namespaces(ns_label=PROJECT_TAG, min_age_seconds=int(EXPIRATION_SECONDS))
  File "/opt/app-root/src/main.py", line 16, in prune_namespaces
    for ns_name in _find_namespaces(min_age_seconds, ns_label, v1):
  File "/opt/app-root/src/main.py", line 28, in _find_namespaces
    for ns in v1.list_namespace(label_selector=ns_label).items:
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14721, in list_namespace
    return self.list_namespace_with_http_info(**kwargs)  # noqa: E501
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api/core_v1_api.py", line 14828, in list_namespace_with_http_info
    return self.api_client.call_api(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 348, in call_api
    return self.__call_api(resource_path, method,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 180, in __call_api
    response_data = self.request(
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/api_client.py", line 373, in request
    return self.rest_client.GET(url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 241, in GET
    return self.request("GET", url,
  File "/opt/app-root/lib64/python3.9/site-packages/kubernetes/client/rest.py", line 235, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (403)
Reason: Forbidden
HTTP response headers: HTTPHeaderDict({'Audit-Id': '6edd643c-9a7b-4c21-bb95-3929defab7f4', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': 'b88be2b9-eb69-4c83-a4e0-a96b73c4ddae', 'X-Kubernetes-Pf-Prioritylevel-Uid': '2a0917b5-060d-4e79-8cc5-80285a96c0c4', 'Date': 'Wed, 08 Mar 2023 18:15:18 GMT', 'Content-Length': '312'})
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"namespaces is forbidden: User \"system:serviceaccount:workshop-support:project-cleaner-sa\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope","reason":"Forbidden","details":{"kind":"namespaces"},"code":403}


[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc adm policy add-clusterrole-to-user project-cleaner -z project-cleaner-sa
error: unknown shorthand flag: 'z' in -z
See 'oc adm policy --help' for usage.
[student@workstation project-cleaner]$ oc adm policy add-cluster-role-to-user project-cleaner -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/project-cleaner added: "project-cleaner-sa"
[student@workstation project-cleaner]$ o clogin -u do280-support -p redhat 
bash: o: command not found...
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971676   0/1           3m4s       3m4s
job.batch/project-cleaner-27971677   1/1           83s        100s

NAME                                 READY   STATUS      RESTARTS   AGE
pod/project-cleaner                  0/1     Error       0          23m
pod/project-cleaner-27971676-6jkvm   0/1     Error       0          2m17s
pod/project-cleaner-27971676-78dts   0/1     Error       0          2m28s
pod/project-cleaner-27971676-gw62x   0/1     Error       0          3m4s
pod/project-cleaner-27971676-hgg9w   0/1     Error       0          112s
pod/project-cleaner-27971676-jv22l   0/1     Error       0          2m4s
pod/project-cleaner-27971676-mh85p   0/1     Error       0          2m40s
pod/project-cleaner-27971676-pxt5k   0/1     Error       0          2m53s
pod/project-cleaner-27971677-bg9sz   0/1     Completed   0          30s
pod/project-cleaner-27971677-cgmnj   0/1     Error       0          75s
pod/project-cleaner-27971677-j6zp4   0/1     Error       0          100s
pod/project-cleaner-27971677-lxpc8   0/1     Error       0          64s
pod/project-cleaner-27971677-m4xsx   0/1     Error       0          41s
pod/project-cleaner-27971677-srx4f   0/1     Error       0          88s
pod/project-cleaner-27971677-vp4qf   0/1     Error       0          53s
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971677-bg9sz
Listing namespaces with label workshop:
 - namespace: clean-test, created 411.254234 seconds ago (2023-03-08 18:31:54+00:00)
 - namespace: workshop-support, created 3967.254309 seconds ago (2023-03-08 17:32:38+00:00)
Deleting namespaces: clean-test, workshop-support
Namespace 'clean-test' deleted
Namespace 'workshop-support' deleted
[student@workstation project-cleaner]$ oc get project clean-test 
Error from server (NotFound): namespaces "clean-test" not found
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ ls
cluster-role.yaml  cron-job.yaml  example-pod.yaml
[student@workstation project-cleaner]$ pwd
/home/student/DO280/labs/compreview-apps/project-cleaner
[student@workstation project-cleaner]$  cd ..
[student@workstation compreview-apps]$ ls
beeper-api  limitrange.yaml  project-cleaner  quota.yaml  subscription.yaml  support-group.yaml
[student@workstation compreview-apps]$ cd beeper-api/
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ cat beeper-db.yaml 
apiVersion: v1
items:
- apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      template.openshift.io/expose-database_name: '{.data[''db-name'']}'
      template.openshift.io/expose-password: '{.data[''db-password'']}'
      template.openshift.io/expose-username: '{.data[''db-user'']}'
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  stringData:
    db-name: beeper
    db-password: beeper123
    db-user: beeper-api
    db-host: beeper-db
    db-port: "5432"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      template.openshift.io/expose-uri: postgres://{.spec.clusterIP}:{.spec.ports[?(.name=="postgresql")].port}
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  spec:
    ports:
    - name: postgresql
      nodePort: 0
      port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: beeper-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      template.alpha.openshift.io/wait-for-ready: "true"
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: postgresql
        app: beeper-db
    template:
      metadata:
        labels:
          name: postgresql
          app: beeper-db
      spec:
        containers:
        - capabilities: {}
          env:
          - name: POSTGRESQL_USER
            valueFrom:
              secretKeyRef:
                key: db-user
                name: beeper-db
          - name: POSTGRESQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: db-password
                name: beeper-db
          - name: POSTGRESQL_DATABASE
            valueFrom:
              secretKeyRef:
                key: db-name
                name: beeper-db
          image: registry.ocp4.example.com:8443/rhel8/postgresql-13:1-7
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/libexec/check-container
              - --live
            initialDelaySeconds: 120
            timeoutSeconds: 10
          name: postgresql
          ports:
          - containerPort: 5432
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/libexec/check-container
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              memory: 512Mi
          securityContext:
            capabilities: {}
            privileged: false
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/lib/pgsql/data
            name: postgresql-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        volumes:
        - name: postgresql-data
          persistentVolumeClaim:
            claimName: beeper-db
  status: {}
kind: List
metadata: {}
[student@workstation beeper-api]$ oc apply -f beeper-db.yaml
Error from server (NotFound): error when creating "beeper-db.yaml": namespaces "workshop-support" not found
Error from server (NotFound): error when creating "beeper-db.yaml": namespaces "workshop-support" not found
Error from server (NotFound): error when creating "beeper-db.yaml": namespaces "workshop-support" not found
Error from server (NotFound): error when creating "beeper-db.yaml": namespaces "workshop-support" not found
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ oc project 
error: you do not have rights to view project "workshop-support" specified in your config or the project doesn't exist
[student@workstation beeper-api]$ oc whoami 
do280-support
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get project | grep workshop
[student@workstation beeper-api]$ oc whoami 
do280-support
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 72 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation beeper-api]$ oc get project | grep workshop
[student@workstation beeper-api]$ oc get project | grep workshop-support 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc new-project workshop-support
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation beeper-api]$ oc label ns workshop-support category=support
namespace/workshop-support labeled
[student@workstation beeper-api]$ oc adm policy add-role-to-group admin workshop-support -n workshop-support
clusterrole.rbac.authorization.k8s.io/admin added: "workshop-support"
[student@workstation beeper-api]$ oc create quota workshop-support --hard=limits.cpu=4,limits.memory=4Gi,requests.cpu=3500m,requests.memory=3Gi
resourcequota/workshop-support created
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ cat ../limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
 name: workshop-support
 namespace: workshop-support
spec:
 limits:
   - default:
       cpu: 300m
       memory: 400Mi
     defaultRequest:
       cpu: 100m
       memory: 250Mi
     type: Container
[student@workstation beeper-api]$ oc apply -f ../limitrange.yaml
limitrange/workshop-support created
[student@workstation beeper-api]$ oc create sa project-cleaner-sa
serviceaccount/project-cleaner-sa created
[student@workstation beeper-api]$ oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "project-cleaner-sa"
[student@workstation beeper-api]$ cd ../project-cleaner/
[student@workstation project-cleaner]$ ls
cluster-role.yaml  cron-job.yaml  example-pod.yaml
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cat cluster-role.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: project-cleaner
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - delete

[student@workstation project-cleaner]$ oc apply -f cluster-role.yaml
clusterrole.rbac.authorization.k8s.io/project-cleaner unchanged
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc adm policy add-cluster-role-to-user project-cleaner -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/project-cleaner added: "project-cleaner-sa"
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ o cget jobs,pod
bash: o: command not found...
[student@workstation project-cleaner]$ oc get jobs,pod
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get cj 
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml 
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ oc new-project clean-test
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "clean-test".
[student@workstation project-cleaner]$ oc get jobs,pods
No resources found in clean-test namespace.
[student@workstation project-cleaner]$ oc get jobs,pods -n workshop-support
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cd ..
[student@workstation compreview-apps]$ cd beeper-api/
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ oc project
error: you do not have rights to view project "clean-test" specified in your config or the project doesn't exist
[student@workstation beeper-api]$ oc whoami
admin
[student@workstation beeper-api]$ oc get projects | grep workshop
[student@workstation beeper-api]$ cd ..
[student@workstation compreview-apps]$ history 
  175  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  176  oc adm policy add-role-to-user edit system:serviceaccount:configmap-reloader:configmap-reloader --rolebinding-name=appsec-api-binding -n appsec-api 
  177  oc login -u developer -p developer https://api.ocp4.example.com:6443
  178  ls
  179  ls config-app/
  180  cat config-app/configmap.yaml 
  181  cat config-app/deployment.yaml 
  182  ls
  183  ls config-app/
  184  cat config-app/route.yaml 
  185  cat config-app/service.yaml 
  186  oc apply -f ./config-app
  187  oc get configmap config-app --output="jsonpath={.data.config\.yaml}"
  188  oc get route
  189  curl -s https://config-app-appsec-api.apps.ocp4.example.com/config | jq 
  190  cat config-app/deployment.yaml 
  191  vim config-app/deployment.yaml
  192  head config-app/deployment.yaml
  193  oc apply -f config-app/deployment.yaml
  194  oc get deployment config-app -o yaml 
  195  cat config-app/configmap.yaml 
  196  vim config-app/configmap.yaml
  197  cat config-app/configmap.yaml
  198  oc apply -f config-app/configmap.yaml
  199  watch "curl -s https://config-app-appsec-api.apps.ocp4.example.com/config | jq "
  200  cd
  201  lab finish appsec-api
  202  .
  203  ip addr | grep 172.25.250.9
  204  sudo tcpdump -i eth0 -A -n port 80 | grep "angular"
  205  cd DO280/labs/network-ingress/
  206  ls
  207  oc create route edge todo-https --service todo-http --hostname todo-https.apps.ocp4.example.com
  208  oc get route 
  209  firefox https://todo-https.apps.ocp4.example.com & 
  210  curl -I -v https://todo-https.apps.ocp4.example.com 
  211  oc get svc todo-http -o jsonpath="{.spec.clusterIP}{'\n'}"
  212  oc debug -t deployment/todo-http --image registry.ocp4.example.com:8443/ubi8/ubi:8.4 
  213  oc delete route todo-https
  214  cd certs
  215  ls
  216  ls -l 
  217  openssl genrsa -out training.key 4096
  218  openssl req -new -key training.key -out training.csr -subj "/C=US/ST=North Carolina/L=Raleigh/O=Red Hat/CN=todo-https.apps.ocp4.example.com"
  219  openssl x509 -req -in training.csr -passin file:passphrase.txt -CA training-CA.pem -CAkey training-CA.key -CAcreateserial -out training.crt -days 1825 -sha256 -extfile training.ext 
  220  ls -ltr 
  221  cd ..
  222  oc create secret tls todo-certs --cert certs/training.crt --key certs/training.key 
  223  cat todo-app-v2.yaml 
  224  oc create -f todo-app-v2.yaml
  225  oc get pods 
  226  oc describe pod/todo-https-79f4d84899-vb2fk | grep -A2 Mounts
  227  oc create route passthrough todo-https --service todo-https --port 8443 --hostname todo-https.apps.ocp4.example.com
  228  oc get route 
  229  curl -vv -I --cacert certs/training-CA.pem https://todo-https.apps.ocp4.example.com
  230  oc get svc todo-https -o jsonpath="{.spec.clusterIP}{'\n'}"
  231  oc debug -t deployment/todo-https --image registry.ocp4.example.com:8443/ubi8/ubi:8.4 
  232  cd
  233  oc delete project network-ingress
  234  sudo yum install gnome-tweaks 
  235  gnome-tweaks
  236  lab start console-admin 
  237  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  238  oc whoami --show-console
  239  firefox $(oc whoami --show-consol) & 
  240  firefox $(oc whoami --show-console) & 
  241  htpasswd -n -b tester redhat
  242  lab finish console-admin 
  243  lab start console-workloads
  244  o clogin -u admin -p redhatocp https://api.ocp4.example.com:6443
  245  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  246  oc whoami --show-console 
  247  firefox $(oc whoami --show-console) & 
  248  lab finish  console-workloads
  249  lab start console-metrics
  250  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  251  oc whoami  --show-console
  252  firefox $(oc whoami --show-console) & 
  253  ls 
  254  cat DO280/labs/console-metrics/load.sh 
  255  DO280/labs/console-metrics/load.sh
  256  oc adm top pod -n console-apps
  257  lab finish console-metrics 
  258  lab start console-review
  259  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  260  htpasswd -n -b dba redhat
  261  htpasswd -n -b tester redhat
  262  firefox $(oc whoami --show-console) & 
  263  oc login -u dba -p redhat 
  264  oc new-app --name database --image registry.ocp4.example.com:8443/rhel8/mysql-80:1-211.1664898586 -e MYSQL_USER=famous -e MYSQL_PASSWORD=famous -e MYSQL_DATABASE=famous
  265  firefox $(oc whoami --show-console) & 
  266  lan grade console-review
  267  lab grade console-review
  268  oc whoami 
  269  oc login -u admin -p redhatocp 
  270  oc get rolebinding -o wide 
  271  oc edit rolebinding/edit-app-team
  272  oc delete rolebinding/edit-app-team
  273  oc delete rolebinding/tester-view
  274  oc policy add-role-to-user view tester -n console-review 
  275  oc policy add-role-to-group edit app-team -n console-review
  276  oc logout 
  277  lab grade console-review
  278  firefox $(oc whoami --show-console) & 
  279  oc login -u admin -p redhatocp 
  280  oc get rolebinding -o wide 
  281  firefox $(oc whoami --show-console) & 
  282  oc get rolebinding -o wide 
  283  oc delete rolebinding/edit
  284  oc delete rolebinding/view
  285  lab grade console-review
  286  lab finish console-review 
  287  lab start auth-providers
  288  cd DO280/labs/auth-providers/
  289  ls
  290  htpasswd -c -B -b htpasswd new_admin redhat
  291  cat htpasswd 
  292  htpasswd -b htpasswd new_developer developer
  293  cat htpasswd 
  294  oc login -u admin -p redhatocp 
  295  oc whoami 
  296  oc create secret generic localusers --from-file=htpasswd=htpasswd -n openshift-config
  297  oc adm policy add-cluster-role-to-user cluster-admin new_admin 
  298  # oc get oauth cluster -o yaml > oauth.yaml 
  299  cat oauth.yaml 
  300  oc get oauth cluster -o yaml > oauth.yaml 
  301  cat oauth.yaml 
  302  vim oauth.yaml 
  303  cat oauth.yaml
  304  oc replace -f oauth.yaml
  305  vim oauth.yaml 
  306  oc replace -f oauth.yaml
  307  oc get pods -n openshift-authentication
  308  watch oc get pods -n openshift-authentication
  309  oc get pods -n openshift-authentication
  310  oc login -u new_admin -p redhat
  311  oc get nodes
  312  oc login -u new_developer -p developer 
  313  oc get nodes
  314  oc login -u new_admin -p redhat 
  315  oc get users
  316  oc get identity
  317  cd
  318  oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-providers/ --confirm
  319  cat /home/student/DO280/labs/auth-providers/htpasswd
  320  htpasswd -b /home/student/DO280/labs/auth-providers/htpasswd managed redhat 
  321  htpasswd -b /home/student/DO280/labs/auth-providers/htpasswd manager redhat 
  322  htpasswd -D /home/student/DO280/labs/auth-providers/htpasswd managed
  323  cat /home/student/DO280/labs/auth-providers/htpasswd
  324  oc set data secret/localusers --from-file=htpasswd=/home/student/DO280/labs/auth-providers/htpasswd -n openshift-config
  325  oc get pods -n openshift-authentication
  326  watch oc get pods -n openshift-authentication
  327  oc get pods -n openshift-authentication -w
  328  oc login -u manager -p redhat 
  329  oc get pods -n openshift-authentication
  330  oc new-app --name database --image registry.ocp4.example.com:8443/rhel8/mysql-80:1-211.1664898586 -e MYSQL_USER=famous -e MYSQL_PASSWORD=famous -e MYSQL_DATABASE=famous
  331  oc login -u new_admin -p redhat 
  332  oc get pods -n openshift-authentication
  333  oc get pods -n openshift-authentication -w
  334  watch oc get pods -n openshift-authentication 
  335  oc get pods -n openshift-authentication 
  336  oc login -u manager -p redhat 
  337  oc new-project auth-providers
  338  oc login -u new_developer -p developer 
  339  oc delete project auth-providers
  340  oc login -u new_admin -p redhat 
  341  oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-providers/ --confirm
  342  cat /home/student/DO280/labs/auth-providers/htpasswd
  343  MANAGER_PASSWD="$(openssl rand -hex 15)"
  344  echo ${MANAGER_PASSWD}
  345  htpasswd -b /home/student/DO280/labs/auth-providers/htpasswd manager ${MANAGER_PASSWD}
  346  oc set data secret/localusers --from-file=htpasswd=/home/student/DO280/labs/auth-providers/htpasswd -n openshift-config
  347  oc get pods -n openshift-authentication
  348  watch oc get pods -n openshift-authentication
  349  oc login -u manager -p ${MANAGER_PASSWD}
  350  watch oc get pods -n openshift-authentication
  351  oc get pods -n openshift-authentication
  352  oc login -u manager -p ${MANAGER_PASSWD}
  353  oc login -u new_admin -p redhat 
  354  oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-providers/ --confirm
  355  cat /home/student/DO280/labs/auth-providers/htpasswd
  356  htpasswd -D /home/student/DO280/labs/auth-providers/htpasswd manager
  357  cat /home/student/DO280/labs/auth-providers/htpasswd
  358  oc set data secret/localusers --from-file=htpasswd=/home/student/DO280/labs/auth-providers/htpasswd -n openshift-config
  359  oc get pods -n openshift-authentication
  360  watch oc get pods -n openshift-authentication
  361  oc login -u manager -p ${MANAGER_PASSWD}
  362  watch oc get pods -n openshift-authentication
  363  oc get pods -n openshift-authentication
  364  oc login -u new_admin -p redhat 
  365  oc get identity
  366  oc delete identity "myusers:manager"
  367  oc get user
  368  oc delete user manager
  369  oc get user 
  370  oc get identity
  371  oc extract secret/localuser -n openshift-config --to - 
  372  oc extract secret/localusers -n openshift-config --to - 
  373  oc login -u admin -p redhatocp 
  374  oc delete project auth-providers
  375  oc edit oauth 
  376  oc delete secret/localusers -n openshift-config
  377  oc get user 
  378  oc delete user --all
  379  oc get identity 
  380  oc delete identity --all
  381  lab finish  auth-providers
  382  ls -a
  383  ls -a .auth/ocp4-kubeconfig 
  384  lab start auth-rbac
  385  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  386  oc get clusterrolebinding -o wide | grep -E 'NAME|self-provisioner'
  387  oc describe  clusterrolebindings self-provisioners
  388  oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
  389  oc describe  clusterrolebindings self-provisioners
  390  oc get clusterrolebinding -o wide | grep -E 'NAME|self-provisioner'
  391  oc login -u leader -p redhat 
  392  oc new-project test 
  393  oc login -u admin -p redhatocp 
  394  oc new-project auth-rbac
  395  oc policy add-role-to-user admin leader
  396  oc adm groups new dev-group
  397  oc adm groups dev-group
  398  oc adm groups dev-group -h
  399  oc get groups dev-group
  400  oc adm groups add-users dev-group developer
  401  oc get groups dev-group
  402  oc adm groups new qa-group
  403  oc get groups
  404  oc adm groups add-users qa-group qa-engineer
  405  oc get groups
  406  oc policy add-role-to-group edit dev-group
  407  oc policy add-role-to-group view qa-group
  408  oc get rolebindings -o wide | grep -v "^system:'
  409  oc get rolebindings -o wide | grep -v "^system:"
  410  oc login -u developer -p developer 
  411  oc new-app --name httpd httpd:2.4 
  412  oc policy add-role-to-user edit qa-engineer
  413  oc login -u qa-engineer -p redhat 
  414  oc scale deployment httpd --replicas 3 
  415  oc login -u admin -p redhatocp 
  416  oc adm policy add-cluster-role-to-group --rolebinding-name self-provisioners self-provisioner system:authenticated:oauth
  417  lab finish  auth-rbac 
  418  lab start auth-review 
  419  cat DO280/labs/auth-review/tmp_users 
  420  htpasswd -D DO280/labs/auth-review/tmp_users analyst
  421  cat DO280/labs/auth-review/tmp_users 
  422  for NAME in tester leader new_admin new_developer; do htpasswd -b /home/student/DO280/labs/auth-review/tmp_users ${NAME} 'L@bR3v!ew'; done
  423  cat DO280/labs/auth-review/tmp_users
  424  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  425  oc create secret generic auth-review --from-file=htpasswd=/home/student/DO280/labs/auth-review/tmp_users -n openshift-config
  426  oc get oauth cluster -o yaml > /home/student/DO280/labs/auth-review/oauth.yaml 
  427  vim /home/student/DO280/labs/auth-review/oauth.yaml
  428  cat /home/student/DO280/labs/auth-review/oauth.yaml
  429  oc replace -f /home/student/DO280/labs/auth-review/oauth.yaml
  430  watch oc get pods -n openshift-authentication 
  431  oc get pods -n openshift-authentication
  432  oc adm policy add-cluster-role-to-user cluster-admin new_admin
  433  oc login -u new_admin -p 'L@bR3v!ew'
  434  ic get nodes
  435  oc get nodes
  436  oc login -u develper -p 'L@bR3v!ew'
  437  oc login -u developer -p 'L@bR3v!ew'
  438  oc login -u new_developer -p 'L@bR3v!ew'
  439  oc get nodes
  440  oc login -u new_admin -p 'L@bR3v!ew'
  441  oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
  442  oc adm groups new managers
  443  oc adm groups add-users managers leader
  444  oc adm policy add-cluster-role-to-group self-provisioner managers
  445  oc login -u leader -p 'L@bR3v!ew'
  446  oc new-project auth-review
  447  oc login -u new_admin -p 'L@bR3v!ew'
  448  oc adm groups new developers
  449  oc get group
  450  oc adm groups add-users developers new_developer
  451  oc get groups developers
  452  oc policy add-role-to-group edit developers
  453  oc adm groups new qa
  454  oc adm groups add-users qa tester
  455  oc policy add-role-to-group view qa
  456  lab grade auth-review 
  457  lab finish auth-review
  458  vim token
  459  cat token 
  460  cd do280_pilot/
  461  ls
  462  vim ch3_ge1.txt
  463  vim ch3_ge2.txt
  464  vim ch3_ge3.txt
  465  vim ch3_lab.txt
  466  git add .
  467  git commit -m "Ch3 contents"
  468  git push 
  469  ls
  470  vim ch4_GE1.txt
  471  vim ch4_GE2.txt
  472  vim ch4_GE3.txt
  473  vim ch4_LAB.txt
  474  git add .
  475  git commit -m "CH4 contents"
  476  git push 
  477  ls
  478  vim ch5_GE1.txt
  479  vim ch5_GE2.txt
  480  vim ch5_LAB.txt
  481  git add .
  482  git commit -m "Chapter 5"
  483  git push 
  484  ls
  485  vim ch6_GE1.txt
  486  vim ch6_GE2.txt
  487  lab start network-policy
  488  oc login -u developer -p developer https://api.ocp4.example.com:6443 
  489  oc new-project network-policy 
  490  oc new-app --name hello --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 
  491  oc new-app --name test --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0
  492  oc get pods 
  493  oc get all
  494  oc get svc 
  495  oc expose svc/hello 
  496  oc get route 
  497  ls DO280/labs/network-policy/
  498  cat DO280/labs/network-policy/display-project-info.sh 
  499  DO280/labs/network-policy/display-project-info.sh
  500  oc rsh test-7986c46b6f-xcxmr curl 10.8.0.171:8080 | grep Hello 
  501  oc rsh test-7986c46b6f-xcxmr curl 172.30.243.136:8080| grep Hello 
  502  curl -s hello-network-policy.apps.ocp4.example.com | grep Hello 
  503  oc new-project different-namespace
  504  oc new-app --name sample-app --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 
  505  DO280/labs/network-policy/display-project-info.sh
  506  oc rsh test-7986c46b6f-xcxmr curl 10.8.0.171:8080 | grep Hello 
  507  oc rsh sample-app-6bb5944b6f-7zmqz curl 10.8.0.171:8080 | grep Hello 
  508  oc rsh sample-app-6bb5944b6f-7zmqz curl 10.8.0.172:8080 | grep Hello 
  509  oc project network-policy 
  510  cd DO280/labs/network-policy/
  511  ls
  512  cat deny-all.yaml 
  513  vim deny-all.yaml
  514  cat deny-all.yaml
  515  oc create -f deny-all.yaml
  516  oc rsh test-7986c46b6f-xcxmr curl 10.8.0.171:8080 | grep Hello
  517  oc project different-namespace 
  518  oc rsh sample-app-6bb5944b6f-7zmqz curl 10.8.0.172:8080 | grep Hello
  519  ls
  520  cat allow-specific.yaml 
  521  vim allow-specific.yaml
  522  cat allow-specific.yaml
  523  oc create -f allow-specific.yaml -n network-policy
  524  oc get networkpolicies -n network-policy
  525  oc login -u admin -p redhatocp 
  526  # oclabel namespace different-namespace network=different-
  527  oc delete -f allow-specific.yaml -n network-policy
  528  oc get networkpolicies -n network-policy
  529  vim allow-specific.yaml 
  530  cat allow-specific.yaml
  531  cat allow-specific.yaml | grep network 
  532  oc create -f allow-specific.yaml -n network-policy 
  533  oc get networkpolicies -n network-policy
  534  oc login -u admin -p redhatocp 
  535  oc label namespace different-namespace network=different-namespace
  536  oc describe namespace different-namespace 
  537  oc login -u developer -p developer 
  538  oc project different-namespace 
  539  oc rsh sample-app-6bb5944b6f-7zmqz curl curl 10.8.0.171:8080 | grep Hello
  540  oc rsh sample-app-6bb5944b6f-7zmqz curl curl 10.8.0.171:8181 | grep Hello
  541  oc rsh sample-app-6bb5944b6f-7zmqz curl curl 10.8.0.172:8080 | grep Hello
  542  curl -s hello-network-policy.apps.ocp4.example.com
  543  cat allow-from-openshift-ingress.yaml 
  544  vim allow-from-openshift-ingress.yaml
  545  cat allow-from-openshift-ingress.yaml
  546  oc create -n network-policy -f allow-from-openshift-ingress.yaml
  547  oc get networkpolicies -n network-policy
  548  oc login -u admin -p redhatocp 
  549  oc label namespace default network.openshift.io/policy-group=ingress
  550  curl -s hello-network-policy.apps.ocp4.example.com
  551  cd
  552  # lab finish network-policy 
  553  # On step 11.1 curl -s hello-network-policy.apps.ocp4.example.com, should not have allowed access and permits only once we add the default namesapce with the ingress matchlabels. 
  554  lab finish network-policy
  555  lab start network-ingress 4
  556  lab start network-ingress 
  557  oc login -u developer -p developer https://api.ocp4.example.com:6443
  558  oc new-project network-ingress
  559  ls DO280/labs/network-ingress/
  560  ls DO280/labs/network-ingress/certs/
  561  cat DO280/labs/network-ingress/todo-app-v1.yaml 
  562  oc create -f DO280/labs/network-ingress/todo-app-v1.yaml
  563  oc status 
  564  oc get pods 
  565  oc get all
  566  oc get route 
  567  oc expose svc todo-http --hostname todo-http.apps.ocp4.example.com
  568  oc get route 
  569  firefox http://todo-http.apps.ocp4.example.com & 
  570  lab finish network-ingress
  571  lab start network-svccerts 
  572  oc login -u admin -p redhatocp https://api.ocp4.exampel.com:6443 
  573  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  574  oc project network-svccerts
  575  oc annotate service server service.beta.openshift.io/serving-cert-secret-name=server-secret 
  576  oc describe servuce server
  577  oc describe service server
  578  oc describe secret server-secret
  579  cat DO280/labs/network-svccerts/server-secret.yaml 
  580  vim DO280/labs/network-svccerts/server-secret.yaml
  581  cat DO280/labs/network-svccerts/server-secret.yaml
  582  oc patch deployment server --patch-file DO280/labs/network-svccerts/server-secret.yaml
  583  oc exec no-ca-bundle -- openssl s_client -connect server.network-svccerts.svc:443
  584  oc create configmap ca-bundle
  585  oc annotate configmap ca-bundle service.beta.openshift.io/inject-cabundle=true 
  586  oc get configmap ca-bundle -o yaml
  587  ls
  588  cat DO280/labs/network-svccerts/client.yaml 
  589  vim DO280/labs/network-svccerts/client.yaml
  590  cat DO280/labs/network-svccerts/client.yaml
  591  oc apply -f DO280/labs/network-svccerts/client.yaml
  592  oc exec client -- curl -s https://server.network-svccerts.svc:443
  593  oc exec client -- openssl s_client -connect server.network-svccerts.svc:443
  594  lab finish network-svccerts 
  595  lab start network-review 
  596  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  597  oc project network-review 
  598  cd DO280/labs/network-review/
  599  ls
  600  cat stock-service.yaml 
  601  vim stock-service.yaml
  602  cat stock-service.yaml
  603  oc apply -f stock-service.yaml
  604  oc get secret stock-service-cert --output="jsonpath={.data.tls].crt}" | base64 -d | openssl x509 -in - -text 
  605  oc get secret stock-service-cert --output="jsonpath={.data.tls\.crt}" | base64 -d | openssl x509 -in - -text 
  606  ls
  607  cat stock-deployment.yaml 
  608  vim stock-deployment.yaml
  609  cat stock-deployment.yaml
  610  oc apply -f stock-deployment.yaml
  611  cat stock-service.yaml 
  612  vim stock-service.yaml
  613  cat stock-service.yaml
  614  oc apply -f stock-service.yaml
  615  ls
  616  cat service-ca-configmap.yaml 
  617  vim service-ca-configmap.yaml
  618  cat service-ca-configmap.yaml
  619  vim service-ca-configmap.yaml
  620  cat service-ca-configmap.yaml
  621  oc create -f service-ca-configmap.yaml
  622  oc describe configmap service-ca 
  623  ls
  624  cat product-deployment.yaml 
  625  vim product-deployment.yaml
  626  cat product-deployment.yaml
  627  oc apply -f product-deployment.yaml
  628  oc exec deployment/product -- curl -s https://stock.network-review.svc/product/1
  629  oc create secret tls passthrough-cert --cert certs/product.pem --key certs/product.key
  630  cat product-deployment.yaml 
  631  vim product-deployment.yaml
  632  cat product-deployment.yaml
  633  vim product-deployment.yaml
  634  cat product-deployment.yaml
  635  oc apply -f product-deployment.yaml
  636  oc create route passthrough product-https --service product --port 8080 --hostname product.apps.ocp4.example.com
  637  oc get route 
  638  curl --cacert certs/ca.pem https://product.apps.ocp4.example.com/products
  639  cat stock-ingresspolicy.yaml 
  640  vim stock-ingresspolicy.yaml
  641  cat stock-ingresspolicy.yaml
  642  oc create -f stock-ingresspolicy.yaml
  643  cat product-ingresspolicy.yaml 
  644  vim product-ingresspolicy.yaml
  645  cat product-ingresspolicy.yaml
  646  oc create -f product-ingresspolicy.yaml
  647  cd
  648  lab grade network-review 
  649  lab finish network-review 
  650  oc rsh sample-app-6bb5944b6f-zbwcg curl 10.8.1.8:8080 | grep Hello
  651  oc rsh sample-app-6bb5944b6f-zbwcg curl 10.8.1.9:8080 | grep Hello
  652  oc project network-policy
  653  cd DO280/labs/network-
  654  cd DO280/labs/network-policy/
  655  cat deny-all.yaml 
  656  vim deny-all.yaml 
  657  cat deny-all.yaml
  658  cat ~/DO280/solutions/network-policy/deny-all.yaml 
  659  oc create -f deny-all.yaml
  660  oc rsh test-7986c46b6f-qsfd2 curl 10.8.1.8:8080 | grep Hello
  661  oc project different-namespace 
  662  oc rsh sample-app-6bb5944b6f-zbwcg curl 10.8.1.9:8080 | grep Hello
  663  cat allow-specific.yaml 
  664  vim allow-specific.yaml 
  665  cat allow-specific.yaml
  666  cat ~/DO280/solutions/network-policy/allow-specific.yaml 
  667  oc create -n network-policy -f allow-specific.yaml 
  668  oc get networkpolicies -n network-policy 
  669  oc login -u admin -p redhatocp 
  670  oc label namespace different-namespace network=different-namespace
  671  oc describe namespace different-namespace
  672  oc login -u developer -p developer
  673  oc project different-namespace 
  674  oc rsh sample-app-6bb5944b6f-zbwcg curl 10.8.1.9:8080 | grep Hello
  675  oc rsh sample-app-6bb5944b6f-zbwcg curl 10.8.1.8:8080 | grep Hello
  676  oc rsh sample-app-6bb5944b6f-zbwcg curl 10.8.1.8:8181 | grep Hello
  677  curl -s hello-network-policy.apps.ocp4.example.com
  678  cat ~/DO280/labs/network-policy/allow-from-openshift-ingress.yaml 
  679  cat ~/DO280/solutions/network-policy/allow-from-openshift-ingress.yaml 
  680  vim ~/DO280/labs/network-policy/allow-from-openshift-ingress.yaml
  681  cat ~/DO280/labs/network-policy/allow-from-openshift-ingress.yaml
  682  oc create -n network-policy -f ~/DO280/labs/network-policy/allow-from-openshift-ingress.yaml
  683  oc get networkpolicies -n network-policy 
  684  o clogin -u admin -p redhatocp 
  685  oc login -u admin -p redhatocp
  686  '
  687  oc label namespace default network.openshift.io/policy-group=ingress
  688  curl -s hello-network-policy.apps.ocp4.example.com
  689  cd
  690  lab finish network-policy
  691  lab start selfservice-quotas 
  692  oc login -u developer -p developer https://api.ocp4.example.com:6443
  693  oc new-project selfservice-quotas
  694  oc create deployment test --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx
  695  oc set resources deployment test --requests=cpu=1
  696  oc get pod,deployment 
  697  oc scale deployment test --replicas=8
  698  oc get pod,deployment 
  699  oc get event --sort-by .metadata.creationTimestamp 
  700  oc login -u admin -p redhatocp 
  701  oc adm top node 
  702  oc describe node/master01
  703  oc new-project test
  704  oc create deployment test --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx
  705  oc set resources deployment test --requests=cpu=1
  706  oc get deployment,pod
  707  oc delete namespace test 
  708  oc project selfservice-quotas 
  709  oc get quota 
  710  oc get resourcequotas 
  711  oc scale deployment test --replicas=1
  712  oc create quota two-cpus --hard=requests.cpu=2
  713  oc get quota 
  714  oc get quota two-cpus -o yaml 
  715  oc scale deployment test --replicas=8 
  716  oc get pods 
  717  oc get pods,deployment 
  718  oc create deployment test2 --image registry.lab.example.com:8443/redhattraining/hello-world-nginx
  719  oc get pods,deployment 
  720  oc get quota two-cpus -o yaml 
  721  oc get event --sort-by .metadata.creationTimestamp
  722  oc new-project test 
  723  oc create deployment test --image registry.lab.example.com:8443/redhattraining/hello-world-nginx
  724  oc set resources deployment test --requests=cpu=1
  725  oc get pods,deployment
  726  oc delete pod/test-7c98d9d599-gds9q
  727  oc get pods,deployment
  728  oc project 
  729  oc get pods,deployment
  730  oc get pod,deployment
  731  oc delete project test 
  732  oc project
  733  oc get projects | grep test 
  734  oc new-project test
  735  oc create deployment test --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx 
  736  oc get all
  737  oc set resources deployment test --requests=cpu=1
  738  oc get pod,deployment
  739  lab finish selfservice-quotas 
  740  lab start selfservice-review 
  741  oc login -u admin -p redhatocp 
  742  oc create namespace template-test
  743  oc creatq quota memory --hard=requests.memory=2Gi,limits.memory=4Gi -n template-test 
  744  oc create quota memory --hard=requests.memory=2Gi,limits.memory=4Gi -n template-test 
  745  cat DO280/labs/selfservice-review/limitrange.yaml 
  746  vim DO280/labs/selfservice-review/limitrange.yaml
  747  cat DO280/labs/selfservice-review/limitrange.yaml
  748  oc create -f DO280/labs/selfservice-review/limitrange.yaml
  749  oc create deployment -n template-test test-limits --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 
  750  oc get pod -n template-test 
  751  oc get pod -n template-test -o jsonpath='{.items[0].spec.containers[0].resources}'
  752  oc scale deployment -n template-test test-limits --replicas=10
  753  oc get deployment -n template-test 
  754  oc describe resourcequotas -n template-test memory
  755  oc adm create-bootstrap-project-template -o yaml > template.yaml
  756  oc get limitrange,quota -n template-test -o yaml >> template.yaml
  757  cat template.yaml 
  758  vim template.yaml 
  759  cat template.yaml 
  760  oc create -f template.yaml -n openshift-config
  761  oc get template -n openshift-config
  762  oc edit projects.config.openshift.io cluster
  763  watch oc get pod -n openshift-apiserver
  764  oc get pod -n openshift-apiserver
  765  oc new-project template-validate
  766  oc describe quota
  767  oc describe limitranges 
  768  lab grade selfservice-review 
  769  lab finish selfservice-review
  770  lab start operators-web
  771  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  772  oc whoami --show-console 
  773  firefox $(oc whoami --show-console) &
  774  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  775  firefox $(oc whoami --show-console) &
  776  lab finish operators-web
  777  vim .venv/labs/lib/python3.6/site-packages/do280/operators-review.py 
  778  lab start network-policy 
  779  oc login -u developer -p developer https://api.ocp4.example.com:6443
  780  oc new-project network-policy
  781  oc new-app --name hello --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 
  782  oc new-app --name test --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 
  783  oc expose service hello 
  784  oc get all
  785  cat DO280/labs/network-policy/display-project-info.sh 
  786  DO280/labs/network-policy/display-project-info.sh
  787  oc rsh test-7986c46b6f-qsfd2 curl 10.8.1.8:8080 | grep Hello 
  788  oc rsh test-7986c46b6f-qsfd2 curl 172.30.78.242:8080 | grep Hello 
  789  curl -s hello-network-policy.apps.ocp4.example.com | grep Hello 
  790  oc new-project different-namespace
  791  oc new-app --name sample-app --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 
  792  lab start appsec-scc
  793  oc login -u developer -p developer https://api.ocp4.example.com:6443
  794  oc whoami 
  795  oc new-project appsec-scc
  796  oc new-app --name gitlab --image registry.ocp4.example.com:8443/redhattraining/gitlab-ce:8.4.3-ce.0
  797  oc status 
  798  oc get pods
  799  watch oc get p
  800  oc get pods -w
  801  oc get pods 
  802  oc logs pod/gitlab-6fd4f89dbc-7htz9 
  803  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  804  oc get pod/gitlab-6fd4f89dbc-7htz9 -o yaml | oc adm policy scc-subject-review -f - 
  805  oc get scc | grep restrict
  806  oc get scc -o name 
  807  oc create sa gitlab-sa
  808  oc adm policy add-scc-to-user anyuid -z gitlab-sa
  809  oc login -u developer -p developer 
  810  oc set serviceaccount deployment/gitlab gitlab-sa
  811  oc get pods 
  812  oc get all
  813  oc expose service/gitlab --port=80 --hostname gitlab.apps.ocp4.example.com
  814  oc get route 
  815  curl -sL http://gitlab.apps.ocp4.example.com/ | grep '<title>'
  816  firefox http://gitlab.apps.ocp4.example.com & 
  817  oc delete project appsec-scc
  818  lab finish appsec-scc
  819  lab start selfservice-ranges 
  820  oc login -u admin -p redhatocp https://api.ocp4.example.com:8443
  821  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  822  oc whoami --show-console 
  823  firefox $(oc whoami --show-console) &
  824  lab finish selfservice-ranges 
  825  lab start selfservice-projtemplate
  826  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  827  oc describe group provisioners
  828  oc edit clusterrolebinding self-provisioners
  829  oc login -u developer -p developer 
  830  oc logout 
  831  oc login -u developer -p developer 
  832  oc new-project test 
  833  oc login -u provisioner1 -p redhat 
  834  oc new-project test 
  835  oc create configmap test 
  836  oc login -u provisioner2 -p redhat 
  837  oc project test 
  838  oc login -u admin -p redhatocp 
  839  oc delete project test 
  840  oc create namespace template-test
  841  cat DO280/labs/selfservice-projtemplate/limitrange.yaml 
  842  vim DO280/labs/selfservice-projtemplate/limitrange.yaml
  843  cat DO280/labs/selfservice-projtemplate/limitrange.yaml
  844  ls DO280/solutions/selfservice-projtemplate/
  845  oc create -f DO280/labs/selfservice-projtemplate/limitrange.yaml
  846  cat DO280/labs/selfservice-projtemplate/deployment.yaml 
  847  oc create -f DO280/labs/selfservice-projtemplate/deployment.yaml -n template-test
  848  oc get pod -n template-test
  849  oc get event -n template-test --sort-by .metadata.creationTimestamp 
  850  oc adm create-bootstrap-project-template -o yaml > template.yaml
  851  oc get limitrange -n template-test -o yaml >> template.yaml 
  852  cat template.yaml 
  853  vim template.yaml 
  854  cat template.yaml 
  855  oc create -f template.yaml -n openshift-conifg
  856  oc create -f template.yaml -n openshift-config
  857  oc get template -n openshift-config
  858  oc edit projects.config.openshift.io cluster
  859  watch oc get pod -n openshift-apiserver
  860  oc get pod -n openshift-apiserver
  861  oc login -u provisioner1 -p redhat 
  862  oc new-project test 
  863  oc login -u provisioner2 -p redhat 
  864  oc create configmap test 
  865  oc create -f ~/DO280/labs/selfservice-projtemplate/deployment.yaml 
  866  oc get pod
  867  oc get event --sort-by .metadata.creationTimestamp
  868  lab finish selfservice-projtemplate 
  869  lab start operators-cli 
  870  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  871  oc get packagemanifests
  872  oc describe packagemanifests file-integrity-operator
  873  cat DO280/labs/operators-cli/sub.yaml 
  874  vim DO280/labs/operators-cli/sub.yaml
  875  cat DO280/labs/operators-cli/sub.yaml
  876  cat DO280/solutions/operators-cli/sub.yaml 
  877  diff DO280/solutions/operators-cli/sub.yaml DO280/labs/operators-cli/sub.yaml 
  878  oc apply -f DO280/labs/operators-cli/sub.yaml
  879  oc describe operator file-integrity-operator 
  880  oc describe installplan install-7qk7l -n openshift-file-integrity
  881  oc patch installplans ved:  false
  882  # oc describe installplan install-7qk7l -n openshift-file-integrity
  883  oc patch installplan install-7qk7l --type merge -p '{"spec":{"approved":true}}' -n openshift-file-integrity
  884  oc describe operator file-integrity-operator 
  885  cat DO280/labs/operators-cli/worker-fileintegrity.yaml 
  886  oc apply -f DO280/labs/operators-cli/worker-fileintegrity.yaml
  887  oc describe fileintegrity worker-fileintegrity -n openshift-file-integrity
  888  oc get fileintegritynodestatuses -n openshift-file-integrity 
  889  watch oc get fileintegritynodestatuses -n openshift-file-integrity 
  890  oc get fileintegritynodestatuses -n openshift-file-integrity 
  891  oc delete subscriptions file-integrity-operator -n openshift-file-integrity
  892  oc get subscription -n openshift-file-integrity
  893  oc get csv -n openshift-file-integrity
  894  oc delete csv file-integrity-operator.v1.0.0 -n openshift-file-integrity
  895  oc get csv -n openshift-file-integrity
  896  oc get namespace | grep openshift-file-integrity
  897  oc delete namespace openshift-file-integrity
  898  lab finish operators-cli
  899  lab start operators-review 
  900  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  901  oc get packagemanifest
  902  oc get packagemanifest compliance-operator -o yaml 
  903  oc create namespace openshift-compliance
  904  cat DO280/labs/operators-review/operator-group.yaml 
  905  vim DO280/labs/operators-review/operator-group.yaml
  906  cat DO280/labs/operators-review/operator-group.yaml
  907  oc create -f DO280/labs/operators-review/operator-group.yaml
  908  cat DO280/labs/operators-review/subscription.yaml 
  909  vim DO280/labs/operators-review/subscription.yaml
  910  cat DO280/labs/operators-review/subscription.yaml
  911  oc create -f DO280/labs/operators-review/subscription.yaml
  912  oc project openshift-compliance
  913  oc whoami 
  914  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  915  oc login -u admin -p redhatocp 
  916  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  917  oc get projects --kubeconfig=/home/student/.auth/ocp4-kubeconfig | grep compliance
  918  oc project openshift-compliance --kubeconfig=/home/student/.auth/ocp4-kubeconfig 
  919  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  920  oc get co --kubeconfig=/home/student/.auth/ocp4-kubeconfig
  921  oc get co --kubeconfig=/home/student/.auth/ocp4-kubeconfig | grep auth
  922  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  923  oc project openshift-compliance 
  924  oc get csv 
  925  oc get csv compliance-operator.v0.1.61 -o jsonpath='{.spec.install.spec.deployments}'
  926  oc get all
  927  oc get csv compliance-operator.v0.1.61 -o jsonpath={.metadata.annotations.alm-examples}
  928  oc get csv compliance-operator.v0.1.61 -o jsonpath={.spec.install.spec.deployments}
  929  oc get csv compliance-operator.v0.1.61 -o jsonpath={.metadata.annotations.alm-examples}
  930  cat DO280/labs/operators-review/scan-setting-binding.yaml 
  931  oc create -f DO280/labs/operators-review/scan-setting-binding.yaml
  932  oc get compliancesuite,pod
  933  watch oc get compliancesuite,pod
  934  oc get compliancesuite,pod
  935  lab grade operators-review 
  936  oc get compliancesuite,pod
  937  oc get csv 
  938  oc get compliancesuite,pod
  939  lab grade operators-review 
  940  ls -al .venv/labs/requirements.txt/requirements.txt 
  941  cat .venv/labs/requirements.txt/requirements.txt
  942  ls .venv/
  943  ls .venv/labs/
  944  ls .venv/labs/share/
  945  ls .venv/labs/share/ansible-runner/a
  946  ls .venv/labs/share/ansible-runner/
  947  ls .venv/labs/share/ansible-runner/utils/
  948  ls .venv/labs/include/python3.6m/
  949  ls .venv/labs/lib/python3.6/site-packages/do280/
  950  ls .venv/labs/lib/python3.6/site-packages/do280/operators-review.py 
  951  vim .venv/labs/lib/python3.6/site-packages/do280/operators-review.py
  952  lab grade operators-review 
  953  lab finish operators-review 
  954  oc whoami
  955  oc get co | grep compliance
  956  oc get all -n openshift-compliance
  957  lab finish operators-review 
  958  lab start operators-review
  959  lab start compreview-apps 
  960  cd DO280/labs/compreview-apps/
  961  ls
  962  oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
  963  oc new-project workshop-support
  964  oc describe ns workshop-support
  965  oc label ns workshop-support category=support
  966  oc describe ns workshop-support
  967  oc adm policy add-role-to-group admin workshop-support -n workshop-support
  968  oc get group
  969  oc create quota workshop-support --hard=limits.cpu=4,limits.memory=4Gi,requests.cpu=3500m,requests.memory=3Gi
  970  oc describe resourcequotas workshop
  971  oc describe resourcequotas workshop-support 
  972  ls
  973  cat limitrange.yaml 
  974  cat quota.yaml 
  975  vim limitrange.yaml 
  976  cat limitrange.yaml
  977  oc apply -f limitrange.yaml
  978  oc get limitranges 
  979  oc describe limitranges workshop-support 
  980  oc create sa project-cleaner-sa
  981  oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
  982  ls
  983  tree
  984  cd project-cleaner/
  985  ls
  986  cat cluster-role.yaml 
  987  oc apply -f cluster-role.yaml
  988  oc adm policy add-role-to-user project-cleaner -z project-cleaner-sa
  989  cat cron-job.yaml 
  990  cat example-pod.yaml 
  991  oc login -u do280-support -p redhat 
  992  cat cron-job.yaml 
  993  vim cron-job.yaml
  994  cat cron-job.yaml
  995  cat ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml 
  996  diff ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml cron-job.yaml 
  997  vim ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml
  998  vim cron-job.yaml 
  999  diff ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml cron-job.yaml 
 1000  vim cron-job.yaml 
 1001  diff ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml cron-job.yaml 
 1002  oc apply -f cron-job.yaml 
 1003  oc new-project clean-test
 1004  oc project workshop-support 
 1005  oc login -u admin -p redhatocp 
 1006  oc label ns clean-test workshop=
 1007  oc describe ns clean-test
 1008  oc get jobs,pods
 1009  oc label ns clean-test workshop=
 1010  oc label ns clean-test workshop= --overwrite=false
 1011  oc label ns clean-test workshop= --overwrite=true
 1012  oc get jobs,pods
 1013  oc label ns clean-test workshop=clean-test --overwrite=true
 1014  oc get jobs,pods
 1015  oc label ns clean-test workshop= --overwrite=true
 1016  oc get jobs,pods
 1017  oc logs pod/project-cleaner-27971630-vhvj2
 1018  vim cron-job.yaml 
 1019  oc delete -f cron-job.yaml
 1020  oc get jobs,pods
 1021  vim cron-job.yaml 
 1022  diff cron-job.yaml ~/DO280/solutions/compreview-apps/project-cleaner/cron-job.yaml 
 1023  oc apply -f cron-job.yaml 
 1024  oc project
 1025  oc get projects| grep clean 
 1026  oc delete project clean-test 
 1027  oc get projects| grep clean 
 1028  oc new-project clean-test
 1029  oc describe ns clean-test
 1030  oc project workshop-support 
 1031  oc whoami 
 1032  oc login -u admin -p redhatocp 
 1033  oc label ns clean-test workshop=
 1034  oc label ns clean-test workshop= --overwrite=true
 1035  oc get jobs,pods
 1036  oc label ns clean-test workshop=clean-test --overwrite=true
 1037  oc get jobs,pods
 1038  oc delete -f cron-job.yaml 
 1039  oc get jobs,pods
 1040  vim cron-job.yaml 
 1041  oc project
 1042  oc get projects | grep clean
 1043  oc delete project clean-test 
 1044  oc get projects | grep clean
 1045  cat cron-job.yaml 
 1046  oc apply -f cron-job.yaml
 1047  oc new-project clean-test
 1048  oc project workshop-support 
 1049  oc delete project clean-test 
 1050  oc delete -f cron-job.yaml 
 1051  oc whoami 
 1052  oc project
 1053  oc login -u do280-support -p redhat 
 1054  oc apply -f cron-job.yaml 
 1055  oc new-project clean-test
 1056  oc project workshop-support 
 1057  oc login -u admin -p redhatocp 
 1058  oc describe ns clean-test 
 1059  oc get jobs,pods
 1060  oc logs pod/project-cleaner-27971647-4rw74
 1061  oc get jobs,pods
 1062  oc describe ns clean-test 
 1063  # oc label ns clean-test workshop= --overwrite=true
 1064  ls
 1065  cat example-pod.yaml 
 1066  vim example-pod.yaml
 1067  oc whoami 
 1068  cat example-pod.yaml 
 1069  oc create -f example-pod.yaml
 1070  oc get pods 
 1071  oc get jobs,pods
 1072  watch oc get jobs,pods
 1073  oc logs pod/project-cleaner-27971654-9f22p
 1074  oc whoami 
 1075  oc project 
 1076  oc whoami 
 1077  oc logs pod/project-cleaner-27971654-9f22p
 1078  oc whoami 
 1079  oc get scc anyuid -o yaml 
 1080  oc project
 1081  oc get sa 
 1082  oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
 1083  oc get scc anyuid -o yaml 
 1084  cat cluster-role.yaml 
 1085  oc apply -f cluster-role.yaml
 1086  oc adm policy add-role-to-user project-cleaner -z project-cleaner-sa
 1087  oc login -u do280-support -p redhat 
 1088  oc get pods,jobs
 1089  oc delete -f cron-job.yaml 
 1090  oc login -u admin -p redhatocp 
 1091  o cget clusterrole | grep project 
 1092  oc get clusterrole | grep project 
 1093  oc describe clusterrole project-cleaner
 1094  oc get clusterrolebinding | grep project 
 1095  oc get clusterrolebinding 
 1096  oc get clusterrolebinding  |grep cleaner
 1097  oc get rolebinding -o wide -n project-cleaner
 1098  oc get rolebinding -n project-cleaner
 1099  oc whomai
 1100  oc whoami 
 1101  oc project
 1102  oc project project-cleaner
 1103  oc get rolebinding 
 1104  oc get rolebinding  -o wide 
 1105  oc login -u do280-support -p redhat 
 1106  oc get cj
 1107  cat cron-job.yaml 
 1108  # oc apply -f cron-job.yaml
 1109  oc get projects
 1110  oc delete project clean-test 
 1111  oc whoami 
 1112  oc get projects | grep clean 
 1113  oc apply -f cron-job.yaml 
 1114  oc new-project clean-test 
 1115  oc project workshop-support 
 1116  oc login -u admin -p redhatocp 
 1117  oc describe ns clean-test 
 1118  oc get jobs,pods
 1119  oc logs pod/project-cleaner-27971672-9dvkw
 1120  oc logs pod/project-cleaner
 1121  oc login -u admin -p redhatocp 
 1122  oc adm policy add-clusterrole-to-user project-cleaner -z project-cleaner-sa
 1123  oc adm policy add-cluster-role-to-user project-cleaner -z project-cleaner-sa
 1124  o clogin -u do280-support -p redhat 
 1125  oc login -u do280-support -p redhat 
 1126  oc get jobs,pods
 1127  oc logs pod/project-cleaner-27971677-bg9sz
 1128  oc get project clean-test 
 1129  ls
 1130  pwd
 1131   cd ..
 1132  ls
 1133  cd beeper-api/
 1134  ls
 1135  cat beeper-db.yaml 
 1136  oc apply -f beeper-db.yaml
 1137  ls
 1138  oc project 
 1139  oc whoami 
 1140  oc get project | grep workshop
 1141  oc whoami 
 1142  oc login -u admin -p redhatocp 
 1143  oc get project | grep workshop
 1144  oc get project | grep workshop-support 
 1145  oc new-project workshop-support
 1146  oc label ns workshop-support category=support
 1147  oc adm policy add-role-to-group admin workshop-support -n workshop-support
 1148  oc create quota workshop-support --hard=limits.cpu=4,limits.memory=4Gi,requests.cpu=3500m,requests.memory=3Gi
 1149  cat ../limitrange.yaml 
 1150  oc apply -f ../limitrange.yaml
 1151  oc create sa project-cleaner-sa
 1152  oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
 1153  cd ../project-cleaner/
 1154  ls
 1155  cat cluster-role.yaml 
 1156  oc apply -f cluster-role.yaml
 1157  oc adm policy add-cluster-role-to-user project-cleaner -z project-cleaner-sa
 1158  oc login -u do280-support -p redhat 
 1159  o cget jobs,pod
 1160  oc get jobs,pod
 1161  oc get cj 
 1162  oc apply -f cron-job.yaml 
 1163  oc new-project clean-test
 1164  oc login -u admin -p redhatocp 
 1165  oc get jobs,pods
 1166  oc get jobs,pods -n workshop-support
 1167  cd ..
 1168  cd beeper-api/
 1169  ls
 1170  oc project
 1171  oc whoami
 1172  oc get projects | grep workshop
 1173  cd ..
 1174  history 
[student@workstation compreview-apps]$ oc whoami 
admin
[student@workstation compreview-apps]$ oc new-project workshop-support
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation compreview-apps]$ oc label ns workshop-support category=support
namespace/workshop-support labeled
[student@workstation compreview-apps]$ oc label ns workshop-support workshop-
namespace/workshop-support unlabeled
[student@workstation compreview-apps]$ oc adm policy add-role-to-group admin workshop-support -n workshop-support
clusterrole.rbac.authorization.k8s.io/admin added: "workshop-support"
[student@workstation compreview-apps]$ oc create quota workshop-support --hard=limits.cpu=4,limits.memory=4Gi,requests.cpu=3500m,requests.memory=3Gi
resourcequota/workshop-support created
[student@workstation compreview-apps]$ ls
beeper-api  limitrange.yaml  project-cleaner  quota.yaml  subscription.yaml  support-group.yaml
[student@workstation compreview-apps]$ cat limitrange.yaml 
apiVersion: v1
kind: LimitRange
metadata:
 name: workshop-support
 namespace: workshop-support
spec:
 limits:
   - default:
       cpu: 300m
       memory: 400Mi
     defaultRequest:
       cpu: 100m
       memory: 250Mi
     type: Container
[student@workstation compreview-apps]$ oc apply -f limitrange.yaml 
limitrange/workshop-support created
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc create sa project-cleaner-sa
serviceaccount/project-cleaner-sa created
[student@workstation compreview-apps]$ 
[student@workstation compreview-apps]$ oc adm policy add-scc-to-user anyuid -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "project-cleaner-sa"
[student@workstation compreview-apps]$ ls
beeper-api  limitrange.yaml  project-cleaner  quota.yaml  subscription.yaml  support-group.yaml
[student@workstation compreview-apps]$ cd project-cleaner/
[student@workstation project-cleaner]$ ls
cluster-role.yaml  cron-job.yaml  example-pod.yaml
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cat cluster-role.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: project-cleaner
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - delete

[student@workstation project-cleaner]$ oc apply -f cluster-role.yaml 
clusterrole.rbac.authorization.k8s.io/project-cleaner unchanged
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc adm policy add-cluster-role-to-user project-cleaner -z project-cleaner-sa
clusterrole.rbac.authorization.k8s.io/project-cleaner added: "project-cleaner-sa"
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc login -u do280-support -p redhat 
oLogin successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ cat cron-job.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner
  namespace: workshop-support
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: project-cleaner
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0
              imagePullPolicy: Always
              env:
              - name: "PROJECT_TAG"
                value: "workshop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:
                limits:
                  cpu: 100m
                  memory: 200Mi
          serviceAccountName: project-cleaner-sa
          securityContext:
            runAsUser: 1001
[student@workstation project-cleaner]$ oc apply -f cron-job.yaml 
cronjob.batch/project-cleaner created
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc new-project  clean-test 
Now using project "clean-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname

[student@workstation project-cleaner]$ oc project workshop-support 
Now using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation project-cleaner]$ oc describe ns clean-test 
Name:         clean-test
Labels:       kubernetes.io/metadata.name=clean-test
              pod-security.kubernetes.io/audit=restricted
              pod-security.kubernetes.io/audit-version=v1.24
              pod-security.kubernetes.io/warn=restricted
              pod-security.kubernetes.io/warn-version=v1.24
              workshop=clean-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: do280-support
              openshift.io/sa.scc.mcs: s0:c29,c24
              openshift.io/sa.scc.supplemental-groups: 1000860000/10000
              openshift.io/sa.scc.uid-range: 1000860000/10000
Status:       Active

Resource Quotas
  Name:            workshop
  Resource         Used  Hard
  --------         ---   ---
  limits.cpu       0     2
  limits.memory    0     1Gi
  requests.cpu     0     1500m
  requests.memory  0     750Mi

Resource Limits
 Type       Resource  Min  Max    Default Request  Default Limit  Max Limit/Request Ratio
 ----       --------  ---  ---    ---------------  -------------  -----------------------
 Container  cpu       -    750m   100m             500m           -
 Container  memory    -    750Mi  250Mi            500Mi          -
[student@workstation project-cleaner]$ oc get jobs,pods
No resources found in workshop-support namespace.
[student@workstation project-cleaner]$ oc label ns clean-test workshop=
error: 'workshop' already has a value (clean-test), and --overwrite is false
[student@workstation project-cleaner]$ oc label ns clean-test workshop= --overwrite=true
namespace/clean-test labeled
[student@workstation project-cleaner]$ oc project 
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation project-cleaner]$ oc whoami
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get users.user.openshift.io ~)
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971700   0/1           40s        40s

NAME                                 READY   STATUS      RESTARTS   AGE
pod/project-cleaner-27971700-v7sgr   0/1     Completed   0          40s
[student@workstation project-cleaner]$ oc get jobs,pods
NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971700   0/1           47s        47s

NAME                                 READY   STATUS      RESTARTS   AGE
pod/project-cleaner-27971700-v7sgr   0/1     Completed   0          47s
[student@workstation project-cleaner]$ oc logs pod/project-cleaner-27971700-v7sgr
Listing namespaces with label workshop:
 - namespace: clean-test, created 61.730008 seconds ago (2023-03-08 18:59:10+00:00)
Deleting namespaces: clean-test
Namespace 'clean-test' deleted
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get project clean-test 
NAME         DISPLAY NAME   STATUS
clean-test                  Terminating
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ oc get project clean-test 
NAME         DISPLAY NAME   STATUS
clean-test                  Terminating
[student@workstation project-cleaner]$ oc get project clean-test 
NAME         DISPLAY NAME   STATUS
clean-test                  Terminating
[student@workstation project-cleaner]$ oc get project clean-test 
NAME         DISPLAY NAME   STATUS
clean-test                  Terminating
[student@workstation project-cleaner]$ oc get project clean-test 
NAME         DISPLAY NAME   STATUS
clean-test                  Terminating
[student@workstation project-cleaner]$ oc get project clean-test 
Error from server (NotFound): namespaces "clean-test" not found
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ 
[student@workstation project-cleaner]$ cd ..
[student@workstation compreview-apps]$ cd beeper-api/
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ oc project 
Using project "workshop-support" on server "https://api.ocp4.example.com:6443".
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc login -u do280-support -p redhat 
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ cat beeper-db.yaml 
apiVersion: v1
items:
- apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      template.openshift.io/expose-database_name: '{.data[''db-name'']}'
      template.openshift.io/expose-password: '{.data[''db-password'']}'
      template.openshift.io/expose-username: '{.data[''db-user'']}'
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  stringData:
    db-name: beeper
    db-password: beeper123
    db-user: beeper-api
    db-host: beeper-db
    db-port: "5432"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      template.openshift.io/expose-uri: postgres://{.spec.clusterIP}:{.spec.ports[?(.name=="postgresql")].port}
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  spec:
    ports:
    - name: postgresql
      nodePort: 0
      port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: beeper-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      template.alpha.openshift.io/wait-for-ready: "true"
    labels:
      template: postgresql-persistent-template
    name: beeper-db
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: postgresql
        app: beeper-db
    template:
      metadata:
        labels:
          name: postgresql
          app: beeper-db
      spec:
        containers:
        - capabilities: {}
          env:
          - name: POSTGRESQL_USER
            valueFrom:
              secretKeyRef:
                key: db-user
                name: beeper-db
          - name: POSTGRESQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: db-password
                name: beeper-db
          - name: POSTGRESQL_DATABASE
            valueFrom:
              secretKeyRef:
                key: db-name
                name: beeper-db
          image: registry.ocp4.example.com:8443/rhel8/postgresql-13:1-7
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/libexec/check-container
              - --live
            initialDelaySeconds: 120
            timeoutSeconds: 10
          name: postgresql
          ports:
          - containerPort: 5432
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/libexec/check-container
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              memory: 512Mi
          securityContext:
            capabilities: {}
            privileged: false
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/lib/pgsql/data
            name: postgresql-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        volumes:
        - name: postgresql-data
          persistentVolumeClaim:
            claimName: beeper-db
  status: {}
kind: List
metadata: {}
[student@workstation beeper-api]$ oc apply -f beeper-db.yaml
secret/beeper-db created
service/beeper-db created
persistentvolumeclaim/beeper-db created
deployment.apps/beeper-db created
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods -l app=beeper-db 
NAME                         READY   STATUS              RESTARTS   AGE
beeper-db-688756744f-7vt5f   0/1     ContainerCreating   0          10s
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods -l app=beeper-db 
NAME                         READY   STATUS              RESTARTS   AGE
beeper-db-688756744f-7vt5f   0/1     ContainerCreating   0          12s
[student@workstation beeper-api]$ watch oc get pods -l app=beeper-db 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods -l app=beeper-db 
NAME                         READY   STATUS    RESTARTS   AGE
beeper-db-688756744f-7vt5f   1/1     Running   0          2m30s
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc create secret tls beeper-api-cert --cert certs/beeper-api.pem --key certs/beeper-api.key
secret/beeper-api-cert created
[student@workstation beeper-api]$ ls certs/
beeper-api.csr  beeper-api.key  beeper-api.pem  ca.key  ca.pem  ca.srl  san.ext
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  selector:
    matchLabels:
      app: beeper-api
  template:
    metadata:
      labels:
        app: beeper-api
    spec:
      containers:
        - name: beeper-api
          image: registry.ocp4.example.com:8443/redhattraining/do280-beeper-api:1.0
          imagePullPolicy: Always
          resources: { }
          ports:
            - containerPort: 8080
          readinessProbe:
            httpGet:
              port: 8080
              path: /readyz
              scheme: HTTP
          livenessProbe:
            httpGet:
              port: 8080
              path: /livez
              scheme: HTTP
          startupProbe:
            httpGet:
              path: /readyz
              port: 8080
              scheme: HTTP
            failureThreshold: 30
            periodSeconds: 3
          env:
            - name: DB_HOST
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-host } }
            - name: DB_PORT
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-port } }
            - name: DB_NAME
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-name } }
            - name: DB_USER
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-user } }
            - name: DB_PASSWORD
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-password } }
            - name: TLS_ENABLED
              value: "false"
[student@workstation beeper-api]$ vim deployment.yaml
[student@workstation beeper-api]$ vim ~/.vimrc
[student@workstation beeper-api]$ vim deployment.yaml
[student@workstation beeper-api]$ vim deployment.yaml
[student@workstation beeper-api]$ cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  selector:
    matchLabels:
      app: beeper-api
  template:
    metadata:
      labels:
        app: beeper-api
    spec:
      containers:
        - name: beeper-api
          image: registry.ocp4.example.com:8443/redhattraining/do280-beeper-api:1.0
          imagePullPolicy: Always
          resources: { }
          ports:
            - containerPort: 8080
          readinessProbe:
            httpGet:
              port: 8080
              path: /readyz
              scheme: HTTPS
          livenessProbe:
            httpGet:
              port: 8080
              path: /livez
              scheme: HTTPS
          startupProbe:
            httpGet:
              path: /readyz
              port: 8080
              scheme: HTTPS
            failureThreshold: 30
            periodSeconds: 3
          env:
            - name: DB_HOST
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-host } }
            - name: DB_PORT
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-port } }
            - name: DB_NAME
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-name } }
            - name: DB_USER
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-user } }
            - name: DB_PASSWORD
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-password } }
            - name: TLS_ENABLED
              value: "true"
           volumeMounts: 
             - name: beeper-api-cert
               mountPath: /etc/pki/beeper-api/
      volumes:
        - name: beeper-api-cert
          secret:
            defaultMode: 420
            secretName: beeper-api-cert
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc apply -f deployment.yaml
error: error parsing deployment.yaml: error converting YAML to JSON: yaml: line 51: did not find expected key
[student@workstation beeper-api]$ vim +51 deployment.yaml
[student@workstation beeper-api]$ oc apply -f deployment.yaml
deployment.apps/beeper-api created
[student@workstation beeper-api]$ cat deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  selector:
    matchLabels:
      app: beeper-api
  template:
    metadata:
      labels:
        app: beeper-api
    spec:
      containers:
        - name: beeper-api
          image: registry.ocp4.example.com:8443/redhattraining/do280-beeper-api:1.0
          imagePullPolicy: Always
          resources: { }
          ports:
            - containerPort: 8080
          readinessProbe:
            httpGet:
              port: 8080
              path: /readyz
              scheme: HTTPS
          livenessProbe:
            httpGet:
              port: 8080
              path: /livez
              scheme: HTTPS
          startupProbe:
            httpGet:
              path: /readyz
              port: 8080
              scheme: HTTPS
            failureThreshold: 30
            periodSeconds: 3
          env:
            - name: DB_HOST
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-host } }
            - name: DB_PORT
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-port } }
            - name: DB_NAME
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-name } }
            - name: DB_USER
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-user } }
            - name: DB_PASSWORD
              valueFrom: { secretKeyRef: { name: beeper-db, key: db-password } }
            - name: TLS_ENABLED
              value: "true"
          volumeMounts: 
            - name: beeper-api-cert
              mountPath: /etc/pki/beeper-api/
      volumes:
        - name: beeper-api-cert
          secret:
            defaultMode: 420
            secretName: beeper-api-cert
[student@workstation beeper-api]$ cat service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  selector:
    app: CHANGE_ME
  ports:
    - port: CHANGE_ME
      targetPort: CHANGE_ME
      name: https

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ vim service.yaml
[student@workstation beeper-api]$ cat service.yaml
apiVersion: v1
kind: Service
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  selector:
    app: beeper-api
  ports:
    - port: 443
      targetPort: 8080
      name: https

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc apply -f service.yaml
service/beeper-api created
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc create route passthrough beeper-api-https --service beeper-api --hostname beeper-api.apps.ocp4.example.com 
route.route.openshift.io/beeper-api-https created
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get route 
NAME               HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/api/beeps; echo 

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/api/beeps
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/api/beeps; echo 

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ firefox https://beeper-api.apps.ocp4.example.com/swagger-ui.html & 
[1] 97901
[student@workstation beeper-api]$ Crash Annotation GraphicsCriticalError: |[0][GFX1-]: Unrecognized feature VIDEO_OVERLAY (t=2.88816) [GFX1-]: Unrecognized feature VIDEO_OVERLAY

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get route 
NAME               HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc describe route 
Name:			beeper-api-https
Namespace:		workshop-support
Created:		2 minutes ago
Labels:			<none>
Annotations:		<none>
Requested Host:		beeper-api.apps.ocp4.example.com
			   exposed on router default (host router-default.apps.ocp4.example.com) 2 minutes ago
Path:			<none>
TLS Termination:	passthrough
Insecure Policy:	<none>
Endpoint Port:		https

Service:	beeper-api
Weight:		100 (100%)
Endpoints:	<none>
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          12m
project-cleaner-27971713-z26np   0/1     Completed   0          2m19s
project-cleaner-27971714-l25tf   0/1     Completed   0          79s
project-cleaner-27971715-5tg7k   0/1     Completed   0          19s
[student@workstation beeper-api]$ oc describe svc
Name:              beeper-api
Namespace:         workshop-support
Labels:            <none>
Annotations:       <none>
Selector:          app=beeper-api
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                172.30.110.81
IPs:               172.30.110.81
Port:              https  443/TCP
TargetPort:        8080/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:              beeper-db
Namespace:         workshop-support
Labels:            template=postgresql-persistent-template
Annotations:       template.openshift.io/expose-uri: postgres://{.spec.clusterIP}:{.spec.ports[?(.name=="postgresql")].port}
Selector:          app=beeper-db
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                172.30.217.183
IPs:               172.30.217.183
Port:              postgresql  5432/TCP
TargetPort:        5432/TCP
Endpoints:         10.8.1.211:5432
Session Affinity:  None
Events:            <none>
[student@workstation beeper-api]$ cat service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  selector:
    app: beeper-api
  ports:
    - port: 443
      targetPort: 8080
      name: https

[student@workstation beeper-api]$ oc get deployment 
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
beeper-api   0/1     0            0           5m14s
beeper-db    1/1     1            1           13m
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          13m
project-cleaner-27971714-l25tf   0/1     Completed   0          2m28s
project-cleaner-27971715-5tg7k   0/1     Completed   0          88s
project-cleaner-27971716-rrdqd   0/1     Completed   0          28s
[student@workstation beeper-api]$ vim deployment.yaml 
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          14m
project-cleaner-27971715-5tg7k   0/1     Completed   0          2m22s
project-cleaner-27971716-rrdqd   0/1     Completed   0          82s
project-cleaner-27971717-fbwwr   0/1     Completed   0          22s
[student@workstation beeper-api]$ oc apply -f deployment.yaml 
deployment.apps/beeper-api unchanged
[student@workstation beeper-api]$ o cget pods 
bash: o: command not found...
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          14m
project-cleaner-27971715-5tg7k   0/1     Completed   0          2m35s
project-cleaner-27971716-rrdqd   0/1     Completed   0          95s
project-cleaner-27971717-fbwwr   0/1     Completed   0          35s
[student@workstation beeper-api]$ oc replace -f deployment.yaml 
deployment.apps/beeper-api replaced
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          14m
project-cleaner-27971715-5tg7k   0/1     Completed   0          2m44s
project-cleaner-27971716-rrdqd   0/1     Completed   0          104s
project-cleaner-27971717-fbwwr   0/1     Completed   0          44s
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          14m
project-cleaner-27971715-5tg7k   0/1     Completed   0          2m46s
project-cleaner-27971716-rrdqd   0/1     Completed   0          106s
project-cleaner-27971717-fbwwr   0/1     Completed   0          46s
[student@workstation beeper-api]$ oc get all
NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          14m
pod/project-cleaner-27971715-5tg7k   0/1     Completed   0          2m52s
pod/project-cleaner-27971716-rrdqd   0/1     Completed   0          112s
pod/project-cleaner-27971717-fbwwr   0/1     Completed   0          52s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/beeper-api   ClusterIP   172.30.110.81    <none>        443/TCP    6m6s
service/beeper-db    ClusterIP   172.30.217.183   <none>        5432/TCP   15m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     0            0           6m51s
deployment.apps/beeper-db    1/1     1            1           14m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       6m51s
replicaset.apps/beeper-db-688756744f    1         1         1       14m

NAME                            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/project-cleaner   */1 * * * *   False     0        52s             18m

NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971715   1/1           17s        2m52s
job.batch/project-cleaner-27971716   1/1           11s        112s
job.batch/project-cleaner-27971717   1/1           13s        52s

NAME                                        HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
route.route.openshift.io/beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ oc delete replicaset.apps/beeper-api-65488464b9
replicaset.apps "beeper-api-65488464b9" deleted
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       5s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          15m
project-cleaner-27971716-rrdqd   0/1     Completed   0          2m26s
project-cleaner-27971717-fbwwr   0/1     Completed   0          86s
project-cleaner-27971718-2xqv4   0/1     Completed   0          26s
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       15s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       17s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       19s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       30s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       33s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       35s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       38s
beeper-db-688756744f    1         1         1       15m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       40s
beeper-db-688756744f    1         1         1       16m
[student@workstation beeper-api]$ oc get rs
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       59s
beeper-db-688756744f    1         1         1       16m
[student@workstation beeper-api]$ oc get pods
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          16m
project-cleaner-27971717-fbwwr   0/1     Completed   0          2m17s
project-cleaner-27971718-2xqv4   0/1     Completed   0          77s
project-cleaner-27971719-qj68q   0/1     Completed   0          17s
[student@workstation beeper-api]$ oc delete -f deployment.yaml 
deployment.apps "beeper-api" deleted
[student@workstation beeper-api]$ oc get pods
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          16m
project-cleaner-27971717-fbwwr   0/1     Completed   0          2m32s
project-cleaner-27971718-2xqv4   0/1     Completed   0          92s
project-cleaner-27971719-qj68q   0/1     Completed   0          32s
[student@workstation beeper-api]$ oc get rs
NAME                   DESIRED   CURRENT   READY   AGE
beeper-db-688756744f   1         1         1       16m
[student@workstation beeper-api]$ oc get deployment 
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
beeper-db   1/1     1            1           16m
[student@workstation beeper-api]$ oc apply -f deployment
error: the path "deployment" does not exist
[student@workstation beeper-api]$ oc apply -f deployment.yaml 
deployment.apps/beeper-api created
[student@workstation beeper-api]$ oc get deployment 
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
beeper-api   0/1     0            0           9s
beeper-db    1/1     1            1           17m
[student@workstation beeper-api]$ oc get rs 
NAME                    DESIRED   CURRENT   READY   AGE
beeper-api-65488464b9   1         0         0       12s
beeper-db-688756744f    1         1         1       17m
[student@workstation beeper-api]$ oc describe svc beeper-api 
Name:              beeper-api
Namespace:         workshop-support
Labels:            <none>
Annotations:       <none>
Selector:          app=beeper-api
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                172.30.110.81
IPs:               172.30.110.81
Port:              https  443/TCP
TargetPort:        8080/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>
[student@workstation beeper-api]$ oc get deployment,pods,rs
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     0            0           35s
deployment.apps/beeper-db    1/1     1            1           17m

NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          17m
pod/project-cleaner-27971718-2xqv4   0/1     Completed   0          2m25s
pod/project-cleaner-27971719-qj68q   0/1     Completed   0          85s
pod/project-cleaner-27971720-sg8hm   0/1     Completed   0          25s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       35s
replicaset.apps/beeper-db-688756744f    1         1         1       17m
[student@workstation beeper-api]$ oc get deployment,pods,rs
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     0            0           54s
deployment.apps/beeper-db    1/1     1            1           17m

NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          17m
pod/project-cleaner-27971718-2xqv4   0/1     Completed   0          2m44s
pod/project-cleaner-27971719-qj68q   0/1     Completed   0          104s
pod/project-cleaner-27971720-sg8hm   0/1     Completed   0          44s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       54s
replicaset.apps/beeper-db-688756744f    1         1         1       17m
[student@workstation beeper-api]$ oc get deployment,pods,rs
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     0            0           60s
deployment.apps/beeper-db    1/1     1            1           17m

NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          17m
pod/project-cleaner-27971718-2xqv4   0/1     Completed   0          2m50s
pod/project-cleaner-27971719-qj68q   0/1     Completed   0          110s
pod/project-cleaner-27971720-sg8hm   0/1     Completed   0          50s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       60s
replicaset.apps/beeper-db-688756744f    1         1         1       17m
[student@workstation beeper-api]$ oc get deployment,pods,rs -l app=beeper-api 
NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       74s
[student@workstation beeper-api]$ oc get deployment,pods,rs
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     0            0           83s
deployment.apps/beeper-db    1/1     1            1           18m

NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          18m
pod/project-cleaner-27971719-qj68q   0/1     Completed   0          2m13s
pod/project-cleaner-27971720-sg8hm   0/1     Completed   0          73s
pod/project-cleaner-27971721-25xs9   0/1     Completed   0          13s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       83s
replicaset.apps/beeper-db-688756744f    1         1         1       18m
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get deployment,pods,rs
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     0            0           2m3s
deployment.apps/beeper-db    1/1     1            1           19m

NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          19m
pod/project-cleaner-27971719-qj68q   0/1     Completed   0          2m53s
pod/project-cleaner-27971720-sg8hm   0/1     Completed   0          113s
pod/project-cleaner-27971721-25xs9   0/1     Completed   0          53s

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         0         0       2m3s
replicaset.apps/beeper-db-688756744f    1         1         1       19m
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
> ^C
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Error from server (Forbidden): pods "image-debug" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Error from server (Forbidden): pods "image-debug" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
[student@workstation beeper-api]$ oc get quotas
error: the server doesn't have a resource type "quotas"
[student@workstation beeper-api]$ oc get quota
NAME               AGE   REQUEST                                                  LIMIT
workshop           27m   requests.cpu: 200m/1500m, requests.memory: 712Mi/750Mi   limits.cpu: 400m/2, limits.memory: 712Mi/1Gi
workshop-support   26m   requests.cpu: 200m/3500m, requests.memory: 712Mi/3Gi     limits.cpu: 400m/4, limits.memory: 712Mi/4Gi
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc delete quota/workshop 
Error from server (Forbidden): resourcequotas "workshop" is forbidden: User "do280-support" cannot delete resource "resourcequotas" in API group "" in the namespace "workshop-support"
[student@workstation beeper-api]$ oc login -u admin -p redhatocp 
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation beeper-api]$ oc delete quota/workshop 
resourcequota "workshop" deleted
[student@workstation beeper-api]$ oc login -u do280-support -p redhat
Login successful.

You have access to 73 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "workshop-support".
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS              RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running             0          21m
project-cleaner-27971721-25xs9   0/1     Completed           0          3m
project-cleaner-27971722-gbb42   0/1     Completed           0          2m
project-cleaner-27971723-vckwd   0/1     Completed           0          60s
project-cleaner-27971724-dt7x2   0/1     ContainerCreating   0          0s
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          21m
project-cleaner-27971721-25xs9   0/1     Completed   0          3m3s
project-cleaner-27971722-gbb42   0/1     Completed   0          2m3s
project-cleaner-27971723-vckwd   0/1     Completed   0          63s
project-cleaner-27971724-dt7x2   1/1     Running     0          3s
[student@workstation beeper-api]$ oc get deployment 
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
beeper-api   0/1     0            0           4m19s
beeper-db    1/1     1            1           21m
[student@workstation beeper-api]$ oc get rouet 
error: the server doesn't have a resource type "rouet"
[student@workstation beeper-api]$ oc get route 
NAME               HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connection timed out.

Removing debug pod ...
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connection timed out.

Removing debug pod ...
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-db-688756744f-7vt5f       1/1     Running     0          22m
project-cleaner-27971723-vckwd   0/1     Completed   0          2m11s
project-cleaner-27971724-dt7x2   0/1     Completed   0          71s
project-cleaner-27971725-8hfq6   0/1     Completed   0          11s
[student@workstation beeper-api]$ oc get deployment 
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
beeper-api   0/1     0            0           5m28s
beeper-db    1/1     1            1           22m
[student@workstation beeper-api]$ oc delete -f deployment.yaml 
deployment.apps "beeper-api" deleted
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get all
NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          22m
pod/project-cleaner-27971723-vckwd   0/1     Completed   0          2m32s
pod/project-cleaner-27971724-dt7x2   0/1     Completed   0          92s
pod/project-cleaner-27971725-8hfq6   0/1     Completed   0          32s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/beeper-api   ClusterIP   172.30.110.81    <none>        443/TCP    13m
service/beeper-db    ClusterIP   172.30.217.183   <none>        5432/TCP   22m

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-db   1/1     1            1           22m

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-db-688756744f   1         1         1       22m

NAME                            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/project-cleaner   */1 * * * *   False     0        32s             26m

NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971723   1/1           11s        2m32s
job.batch/project-cleaner-27971724   1/1           13s        92s
job.batch/project-cleaner-27971725   1/1           11s        32s

NAME                                        HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
route.route.openshift.io/beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ oc apply -f deployment.yaml 

deployment.apps/beeper-api created
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get all
NAME                                 READY   STATUS              RESTARTS   AGE
pod/beeper-api-65488464b9-mwwk5      0/1     ContainerCreating   0          6s
pod/beeper-db-688756744f-7vt5f       1/1     Running             0          22m
pod/project-cleaner-27971723-vckwd   0/1     Completed           0          2m48s
pod/project-cleaner-27971724-dt7x2   0/1     Completed           0          108s
pod/project-cleaner-27971725-8hfq6   0/1     Completed           0          48s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/beeper-api   ClusterIP   172.30.110.81    <none>        443/TCP    14m
service/beeper-db    ClusterIP   172.30.217.183   <none>        5432/TCP   22m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     1            0           6s
deployment.apps/beeper-db    1/1     1            1           22m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         1         0       6s
replicaset.apps/beeper-db-688756744f    1         1         1       22m

NAME                            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/project-cleaner   */1 * * * *   False     0        48s             26m

NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971723   1/1           11s        2m48s
job.batch/project-cleaner-27971724   1/1           13s        108s
job.batch/project-cleaner-27971725   1/1           11s        48s

NAME                                        HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
route.route.openshift.io/beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ oc get all
NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-api-65488464b9-mwwk5      0/1     Running     0          17s
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          23m
pod/project-cleaner-27971723-vckwd   0/1     Completed   0          2m59s
pod/project-cleaner-27971724-dt7x2   0/1     Completed   0          119s
pod/project-cleaner-27971725-8hfq6   0/1     Completed   0          59s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/beeper-api   ClusterIP   172.30.110.81    <none>        443/TCP    14m
service/beeper-db    ClusterIP   172.30.217.183   <none>        5432/TCP   23m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     1            0           17s
deployment.apps/beeper-db    1/1     1            1           23m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         1         0       17s
replicaset.apps/beeper-db-688756744f    1         1         1       23m

NAME                            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/project-cleaner   */1 * * * *   False     0        59s             26m

NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971723   1/1           11s        2m59s
job.batch/project-cleaner-27971724   1/1           13s        119s
job.batch/project-cleaner-27971725   1/1           11s        59s

NAME                                        HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
route.route.openshift.io/beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ oc get all
NAME                                 READY   STATUS      RESTARTS   AGE
pod/beeper-api-65488464b9-mwwk5      0/1     Running     0          20s
pod/beeper-db-688756744f-7vt5f       1/1     Running     0          23m
pod/project-cleaner-27971723-vckwd   0/1     Completed   0          3m2s
pod/project-cleaner-27971724-dt7x2   0/1     Completed   0          2m2s
pod/project-cleaner-27971725-8hfq6   0/1     Completed   0          62s
pod/project-cleaner-27971726-fqqs6   1/1     Running     0          2s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/beeper-api   ClusterIP   172.30.110.81    <none>        443/TCP    14m
service/beeper-db    ClusterIP   172.30.217.183   <none>        5432/TCP   23m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/beeper-api   0/1     1            0           20s
deployment.apps/beeper-db    1/1     1            1           23m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/beeper-api-65488464b9   1         1         0       20s
replicaset.apps/beeper-db-688756744f    1         1         1       23m

NAME                            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/project-cleaner   */1 * * * *   False     1        2s              27m

NAME                                 COMPLETIONS   DURATION   AGE
job.batch/project-cleaner-27971723   1/1           11s        3m2s
job.batch/project-cleaner-27971724   1/1           13s        2m2s
job.batch/project-cleaner-27971725   1/1           11s        62s
job.batch/project-cleaner-27971726   0/1           2s         2s

NAME                                        HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
route.route.openshift.io/beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-api-65488464b9-mwwk5      0/1     Running     0          28s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
project-cleaner-27971723-vckwd   0/1     Completed   0          3m10s
project-cleaner-27971724-dt7x2   0/1     Completed   0          2m10s
project-cleaner-27971725-8hfq6   0/1     Completed   0          70s
project-cleaner-27971726-fqqs6   0/1     Completed   0          10s
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS   AGE
beeper-api-65488464b9-mwwk5      0/1     Running     0          32s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
project-cleaner-27971724-dt7x2   0/1     Completed   0          2m14s
project-cleaner-27971725-8hfq6   0/1     Completed   0          74s
project-cleaner-27971726-fqqs6   0/1     Completed   0          14s
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     0          39s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     0          41s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     0          47s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     0          49s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     0          51s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     0          53s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Error       0          57s
beeper-db-688756744f-7vt5f       1/1     Running     0          23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     1 (4s ago)   60s
beeper-db-688756744f-7vt5f       1/1     Running     0            23m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     1 (6s ago)   62s
beeper-db-688756744f-7vt5f       1/1     Running     0            23m
[student@workstation beeper-api]$ oc logs pod/beeper-api-65488464b9-mwwk5

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.5)

2023-03-08 19:26:44.161  INFO 1 --- [           main] com.redhat.beeper.BeeperApplication      : Starting BeeperApplication v1.0.0 using Java 17.0.3 on beeper-api-65488464b9-mwwk5 with PID 1 (/app.jar started by 1000840000 in /home/jboss)
2023-03-08 19:26:44.243  INFO 1 --- [           main] com.redhat.beeper.BeeperApplication      : No active profile set, falling back to 1 default profile: "default"
2023-03-08 19:26:49.859  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2023-03-08 19:26:50.242  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 291 ms. Found 1 JPA repository interfaces.
2023-03-08 19:26:54.454  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (https)
2023-03-08 19:26:54.544  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2023-03-08 19:26:54.544  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.68]
2023-03-08 19:26:55.044  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2023-03-08 19:26:55.045  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 10585 ms
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running             1 (23s ago)   79s
beeper-db-688756744f-7vt5f       1/1     Running             0             24m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     1 (26s ago)   82s
beeper-db-688756744f-7vt5f       1/1     Running     0             24m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     1 (27s ago)   83s
beeper-db-688756744f-7vt5f       1/1     Running     0             24m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     1 (29s ago)   85s
beeper-db-688756744f-7vt5f       1/1     Running     0             24m
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get route 

NAME               HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ 

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get route 
NAME               HOST/PORT                          PATH   SERVICES     PORT    TERMINATION   WILDCARD
beeper-api-https   beeper-api.apps.ocp4.example.com          beeper-api   https   passthrough   None
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     2 (3s ago)   106s
beeper-db-688756744f-7vt5f       1/1     Running     0            24m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     2 (5s ago)   108s
beeper-db-688756744f-7vt5f       1/1     Running     0            24m
[student@workstation beeper-api]$ oc get events 
LAST SEEN   TYPE      REASON                  OBJECT                               MESSAGE
112s        Normal    Scheduled               pod/beeper-api-65488464b9-mwwk5      Successfully assigned workshop-support/beeper-api-65488464b9-mwwk5 to master01
111s        Normal    AddedInterface          pod/beeper-api-65488464b9-mwwk5      Add eth0 [10.8.1.238/23] from ovn-kubernetes
56s         Normal    Pulling                 pod/beeper-api-65488464b9-mwwk5      Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-beeper-api:1.0"
101s        Normal    Pulled                  pod/beeper-api-65488464b9-mwwk5      Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-beeper-api:1.0" in 10.505023231s
55s         Normal    Created                 pod/beeper-api-65488464b9-mwwk5      Created container beeper-api
55s         Normal    Started                 pod/beeper-api-65488464b9-mwwk5      Started container beeper-api
49s         Warning   Unhealthy               pod/beeper-api-65488464b9-mwwk5      Startup probe failed: Get "https://10.8.1.238:8080/readyz": dial tcp 10.8.1.238:8080: connect: connection refused
55s         Normal    Pulled                  pod/beeper-api-65488464b9-mwwk5      Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-beeper-api:1.0" in 315.276336ms
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-tv2t6" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-nvkcx" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-xf4n5" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-s5rfc" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-b6ws4" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-vmhm5" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-68m2r" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-pmcsg" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
16m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-r6l8r" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
12m         Warning   FailedCreate            replicaset/beeper-api-65488464b9     (combined from similar events): Error creating: pods "beeper-api-65488464b9-nzmmf" is forbidden: exceeded quota: workshop, requested: limits.memory=500Mi,requests.memory=250Mi, used: limits.memory=712Mi,requests.memory=712Mi, limited: limits.memory=1Gi,requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-fg8jb" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-4ccz6" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-hcfmg" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-fd28m" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-fsnmt" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-bh27r" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m20s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-tc6b9" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m19s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-grdph" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
9m19s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-g6nlc" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
8m39s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     (combined from similar events): Error creating: pods "beeper-api-65488464b9-wc8h7" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-d9vhr" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-q8jjz" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-fmcth" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-thdd2" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-m52l8" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-xt8t8" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m44s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-hl9tb" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m43s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-pphc6" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
7m43s       Warning   FailedCreate            replicaset/beeper-api-65488464b9     Error creating: pods "beeper-api-65488464b9-g65qr" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
5m          Warning   FailedCreate            replicaset/beeper-api-65488464b9     (combined from similar events): Error creating: pods "beeper-api-65488464b9-wnhmh" is forbidden: exceeded quota: workshop, requested: requests.memory=250Mi, used: requests.memory=512Mi, limited: requests.memory=750Mi
112s        Normal    SuccessfulCreate        replicaset/beeper-api-65488464b9     Created pod: beeper-api-65488464b9-mwwk5
9m20s       Normal    ScalingReplicaSet       deployment/beeper-api                Scaled up replica set beeper-api-65488464b9 to 1
7m44s       Normal    ScalingReplicaSet       deployment/beeper-api                Scaled up replica set beeper-api-65488464b9 to 1
112s        Normal    ScalingReplicaSet       deployment/beeper-api                Scaled up replica set beeper-api-65488464b9 to 1
24m         Warning   FailedScheduling        pod/beeper-db-688756744f-7vt5f       0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
24m         Normal    Scheduled               pod/beeper-db-688756744f-7vt5f       Successfully assigned workshop-support/beeper-db-688756744f-7vt5f to master01
24m         Normal    AddedInterface          pod/beeper-db-688756744f-7vt5f       Add eth0 [10.8.1.211/23] from ovn-kubernetes
24m         Normal    Pulling                 pod/beeper-db-688756744f-7vt5f       Pulling image "registry.ocp4.example.com:8443/rhel8/postgresql-13:1-7"
24m         Normal    Pulled                  pod/beeper-db-688756744f-7vt5f       Successfully pulled image "registry.ocp4.example.com:8443/rhel8/postgresql-13:1-7" in 26.362314167s
24m         Normal    Created                 pod/beeper-db-688756744f-7vt5f       Created container postgresql
24m         Normal    Started                 pod/beeper-db-688756744f-7vt5f       Started container postgresql
22m         Warning   Unhealthy               pod/beeper-db-688756744f-7vt5f       Readiness probe failed:
24m         Normal    SuccessfulCreate        replicaset/beeper-db-688756744f      Created pod: beeper-db-688756744f-7vt5f
24m         Normal    ExternalProvisioning    persistentvolumeclaim/beeper-db      waiting for a volume to be created, either by external provisioner "k8s-sigs.io/nfs-subdir-external-provisioner" or manually created by system administrator
24m         Normal    ScalingReplicaSet       deployment/beeper-db                 Scaled up replica set beeper-db-688756744f to 1
24m         Normal    Provisioning            persistentvolumeclaim/beeper-db      External provisioner is provisioning volume for claim "workshop-support/beeper-db"
24m         Normal    ProvisioningSucceeded   persistentvolumeclaim/beeper-db      Successfully provisioned volume pvc-c9711221-065f-4509-88b8-cda3ec6f4b7c
3m8s        Normal    Scheduled               pod/image-debug                      Successfully assigned workshop-support/image-debug to master01
3m8s        Normal    AddedInterface          pod/image-debug                      Add eth0 [10.8.1.235/23] from ovn-kubernetes
3m8s        Normal    Pulled                  pod/image-debug                      Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4edb2f3ffbd1a017d150cd1d6fca7f3cdc6a1a4e34c21a6aee0ab0366920bf0" already present on machine
3m8s        Normal    Created                 pod/image-debug                      Created container debug
3m8s        Normal    Started                 pod/image-debug                      Started container debug
2m41s       Normal    Scheduled               pod/image-debug                      Successfully assigned workshop-support/image-debug to master01
2m41s       Normal    AddedInterface          pod/image-debug                      Add eth0 [10.8.1.236/23] from ovn-kubernetes
2m41s       Normal    Pulled                  pod/image-debug                      Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4edb2f3ffbd1a017d150cd1d6fca7f3cdc6a1a4e34c21a6aee0ab0366920bf0" already present on machine
2m40s       Normal    Created                 pod/image-debug                      Created container debug
2m40s       Normal    Started                 pod/image-debug                      Started container debug
27m         Normal    Scheduled               pod/project-cleaner-27971700-v7sgr   Successfully assigned workshop-support/project-cleaner-27971700-v7sgr to master01
27m         Normal    AddedInterface          pod/project-cleaner-27971700-v7sgr   Add eth0 [10.8.1.208/23] from ovn-kubernetes
27m         Normal    Pulling                 pod/project-cleaner-27971700-v7sgr   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
27m         Normal    Pulled                  pod/project-cleaner-27971700-v7sgr   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 360.653809ms
27m         Normal    Created                 pod/project-cleaner-27971700-v7sgr   Created container project-cleaner
27m         Normal    Started                 pod/project-cleaner-27971700-v7sgr   Started container project-cleaner
27m         Normal    SuccessfulCreate        job/project-cleaner-27971700         Created pod: project-cleaner-27971700-v7sgr
25m         Normal    Completed               job/project-cleaner-27971700         Job completed
25m         Normal    Scheduled               pod/project-cleaner-27971701-b2269   Successfully assigned workshop-support/project-cleaner-27971701-b2269 to master01
25m         Normal    AddedInterface          pod/project-cleaner-27971701-b2269   Add eth0 [10.8.1.209/23] from ovn-kubernetes
25m         Normal    Pulling                 pod/project-cleaner-27971701-b2269   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
25m         Normal    Pulled                  pod/project-cleaner-27971701-b2269   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 330.400819ms
25m         Normal    Created                 pod/project-cleaner-27971701-b2269   Created container project-cleaner
25m         Normal    Started                 pod/project-cleaner-27971701-b2269   Started container project-cleaner
25m         Normal    SuccessfulCreate        job/project-cleaner-27971701         Created pod: project-cleaner-27971701-b2269
25m         Normal    Completed               job/project-cleaner-27971701         Job completed
25m         Normal    Scheduled               pod/project-cleaner-27971702-x4bf6   Successfully assigned workshop-support/project-cleaner-27971702-x4bf6 to master01
25m         Normal    AddedInterface          pod/project-cleaner-27971702-x4bf6   Add eth0 [10.8.1.210/23] from ovn-kubernetes
25m         Normal    Pulling                 pod/project-cleaner-27971702-x4bf6   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
25m         Normal    Pulled                  pod/project-cleaner-27971702-x4bf6   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 334.053073ms
25m         Normal    Created                 pod/project-cleaner-27971702-x4bf6   Created container project-cleaner
25m         Normal    Started                 pod/project-cleaner-27971702-x4bf6   Started container project-cleaner
25m         Normal    SuccessfulCreate        job/project-cleaner-27971702         Created pod: project-cleaner-27971702-x4bf6
25m         Normal    Completed               job/project-cleaner-27971702         Job completed
24m         Normal    Scheduled               pod/project-cleaner-27971703-wh78n   Successfully assigned workshop-support/project-cleaner-27971703-wh78n to master01
24m         Normal    AddedInterface          pod/project-cleaner-27971703-wh78n   Add eth0 [10.8.1.212/23] from ovn-kubernetes
24m         Normal    Pulling                 pod/project-cleaner-27971703-wh78n   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
24m         Normal    Pulled                  pod/project-cleaner-27971703-wh78n   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 361.302439ms
24m         Normal    Created                 pod/project-cleaner-27971703-wh78n   Created container project-cleaner
24m         Normal    Started                 pod/project-cleaner-27971703-wh78n   Started container project-cleaner
24m         Normal    SuccessfulCreate        job/project-cleaner-27971703         Created pod: project-cleaner-27971703-wh78n
24m         Normal    Completed               job/project-cleaner-27971703         Job completed
23m         Normal    Scheduled               pod/project-cleaner-27971704-h94sb   Successfully assigned workshop-support/project-cleaner-27971704-h94sb to master01
23m         Normal    AddedInterface          pod/project-cleaner-27971704-h94sb   Add eth0 [10.8.1.213/23] from ovn-kubernetes
23m         Normal    Pulling                 pod/project-cleaner-27971704-h94sb   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
23m         Normal    Pulled                  pod/project-cleaner-27971704-h94sb   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 315.743271ms
23m         Normal    Created                 pod/project-cleaner-27971704-h94sb   Created container project-cleaner
23m         Normal    Started                 pod/project-cleaner-27971704-h94sb   Started container project-cleaner
23m         Normal    SuccessfulCreate        job/project-cleaner-27971704         Created pod: project-cleaner-27971704-h94sb
23m         Normal    Completed               job/project-cleaner-27971704         Job completed
22m         Normal    Scheduled               pod/project-cleaner-27971705-f5h8d   Successfully assigned workshop-support/project-cleaner-27971705-f5h8d to master01
22m         Normal    AddedInterface          pod/project-cleaner-27971705-f5h8d   Add eth0 [10.8.1.214/23] from ovn-kubernetes
22m         Normal    Pulling                 pod/project-cleaner-27971705-f5h8d   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
22m         Normal    Pulled                  pod/project-cleaner-27971705-f5h8d   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 321.510648ms
22m         Normal    Created                 pod/project-cleaner-27971705-f5h8d   Created container project-cleaner
22m         Normal    Started                 pod/project-cleaner-27971705-f5h8d   Started container project-cleaner
22m         Normal    SuccessfulCreate        job/project-cleaner-27971705         Created pod: project-cleaner-27971705-f5h8d
22m         Normal    Completed               job/project-cleaner-27971705         Job completed
21m         Normal    Scheduled               pod/project-cleaner-27971706-6wc8f   Successfully assigned workshop-support/project-cleaner-27971706-6wc8f to master01
21m         Normal    AddedInterface          pod/project-cleaner-27971706-6wc8f   Add eth0 [10.8.1.215/23] from ovn-kubernetes
21m         Normal    Pulling                 pod/project-cleaner-27971706-6wc8f   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
21m         Normal    Pulled                  pod/project-cleaner-27971706-6wc8f   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 338.342054ms
21m         Normal    Created                 pod/project-cleaner-27971706-6wc8f   Created container project-cleaner
21m         Normal    Started                 pod/project-cleaner-27971706-6wc8f   Started container project-cleaner
21m         Normal    SuccessfulCreate        job/project-cleaner-27971706         Created pod: project-cleaner-27971706-6wc8f
21m         Normal    Completed               job/project-cleaner-27971706         Job completed
20m         Normal    Scheduled               pod/project-cleaner-27971707-frnxl   Successfully assigned workshop-support/project-cleaner-27971707-frnxl to master01
20m         Normal    AddedInterface          pod/project-cleaner-27971707-frnxl   Add eth0 [10.8.1.216/23] from ovn-kubernetes
20m         Normal    Pulling                 pod/project-cleaner-27971707-frnxl   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
20m         Normal    Pulled                  pod/project-cleaner-27971707-frnxl   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 324.629575ms
20m         Normal    Created                 pod/project-cleaner-27971707-frnxl   Created container project-cleaner
20m         Normal    Started                 pod/project-cleaner-27971707-frnxl   Started container project-cleaner
20m         Normal    SuccessfulCreate        job/project-cleaner-27971707         Created pod: project-cleaner-27971707-frnxl
20m         Normal    Completed               job/project-cleaner-27971707         Job completed
19m         Normal    Scheduled               pod/project-cleaner-27971708-zvjdf   Successfully assigned workshop-support/project-cleaner-27971708-zvjdf to master01
19m         Normal    AddedInterface          pod/project-cleaner-27971708-zvjdf   Add eth0 [10.8.1.217/23] from ovn-kubernetes
19m         Normal    Pulling                 pod/project-cleaner-27971708-zvjdf   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
19m         Normal    Pulled                  pod/project-cleaner-27971708-zvjdf   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 319.502746ms
19m         Normal    Created                 pod/project-cleaner-27971708-zvjdf   Created container project-cleaner
19m         Normal    Started                 pod/project-cleaner-27971708-zvjdf   Started container project-cleaner
19m         Normal    SuccessfulCreate        job/project-cleaner-27971708         Created pod: project-cleaner-27971708-zvjdf
19m         Normal    Completed               job/project-cleaner-27971708         Job completed
18m         Normal    Scheduled               pod/project-cleaner-27971709-z98q5   Successfully assigned workshop-support/project-cleaner-27971709-z98q5 to master01
18m         Normal    AddedInterface          pod/project-cleaner-27971709-z98q5   Add eth0 [10.8.1.218/23] from ovn-kubernetes
18m         Normal    Pulling                 pod/project-cleaner-27971709-z98q5   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
18m         Normal    Pulled                  pod/project-cleaner-27971709-z98q5   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 312.750687ms
18m         Normal    Created                 pod/project-cleaner-27971709-z98q5   Created container project-cleaner
18m         Normal    Started                 pod/project-cleaner-27971709-z98q5   Started container project-cleaner
18m         Normal    SuccessfulCreate        job/project-cleaner-27971709         Created pod: project-cleaner-27971709-z98q5
18m         Normal    Completed               job/project-cleaner-27971709         Job completed
17m         Normal    Scheduled               pod/project-cleaner-27971710-q7qwz   Successfully assigned workshop-support/project-cleaner-27971710-q7qwz to master01
17m         Normal    AddedInterface          pod/project-cleaner-27971710-q7qwz   Add eth0 [10.8.1.219/23] from ovn-kubernetes
17m         Normal    Pulling                 pod/project-cleaner-27971710-q7qwz   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
17m         Normal    Pulled                  pod/project-cleaner-27971710-q7qwz   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 315.072126ms
17m         Normal    Created                 pod/project-cleaner-27971710-q7qwz   Created container project-cleaner
17m         Normal    Started                 pod/project-cleaner-27971710-q7qwz   Started container project-cleaner
17m         Normal    SuccessfulCreate        job/project-cleaner-27971710         Created pod: project-cleaner-27971710-q7qwz
17m         Normal    Completed               job/project-cleaner-27971710         Job completed
16m         Normal    Scheduled               pod/project-cleaner-27971711-cgpsk   Successfully assigned workshop-support/project-cleaner-27971711-cgpsk to master01
16m         Normal    AddedInterface          pod/project-cleaner-27971711-cgpsk   Add eth0 [10.8.1.220/23] from ovn-kubernetes
16m         Normal    Pulling                 pod/project-cleaner-27971711-cgpsk   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
16m         Normal    Pulled                  pod/project-cleaner-27971711-cgpsk   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 318.435925ms
16m         Normal    Created                 pod/project-cleaner-27971711-cgpsk   Created container project-cleaner
16m         Normal    Started                 pod/project-cleaner-27971711-cgpsk   Started container project-cleaner
16m         Normal    SuccessfulCreate        job/project-cleaner-27971711         Created pod: project-cleaner-27971711-cgpsk
16m         Normal    Completed               job/project-cleaner-27971711         Job completed
15m         Normal    Scheduled               pod/project-cleaner-27971712-qkzxp   Successfully assigned workshop-support/project-cleaner-27971712-qkzxp to master01
15m         Normal    AddedInterface          pod/project-cleaner-27971712-qkzxp   Add eth0 [10.8.1.221/23] from ovn-kubernetes
15m         Normal    Pulling                 pod/project-cleaner-27971712-qkzxp   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
15m         Normal    Pulled                  pod/project-cleaner-27971712-qkzxp   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 314.696327ms
15m         Normal    Created                 pod/project-cleaner-27971712-qkzxp   Created container project-cleaner
15m         Normal    Started                 pod/project-cleaner-27971712-qkzxp   Started container project-cleaner
15m         Normal    SuccessfulCreate        job/project-cleaner-27971712         Created pod: project-cleaner-27971712-qkzxp
15m         Normal    Completed               job/project-cleaner-27971712         Job completed
14m         Normal    Scheduled               pod/project-cleaner-27971713-z26np   Successfully assigned workshop-support/project-cleaner-27971713-z26np to master01
14m         Normal    AddedInterface          pod/project-cleaner-27971713-z26np   Add eth0 [10.8.1.222/23] from ovn-kubernetes
14m         Normal    Pulling                 pod/project-cleaner-27971713-z26np   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
14m         Normal    Pulled                  pod/project-cleaner-27971713-z26np   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 344.501893ms
14m         Normal    Created                 pod/project-cleaner-27971713-z26np   Created container project-cleaner
14m         Normal    Started                 pod/project-cleaner-27971713-z26np   Started container project-cleaner
14m         Normal    SuccessfulCreate        job/project-cleaner-27971713         Created pod: project-cleaner-27971713-z26np
14m         Normal    Completed               job/project-cleaner-27971713         Job completed
13m         Normal    Scheduled               pod/project-cleaner-27971714-l25tf   Successfully assigned workshop-support/project-cleaner-27971714-l25tf to master01
13m         Normal    AddedInterface          pod/project-cleaner-27971714-l25tf   Add eth0 [10.8.1.223/23] from ovn-kubernetes
13m         Normal    Pulling                 pod/project-cleaner-27971714-l25tf   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
13m         Normal    Pulled                  pod/project-cleaner-27971714-l25tf   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 311.334054ms
13m         Normal    Created                 pod/project-cleaner-27971714-l25tf   Created container project-cleaner
13m         Normal    Started                 pod/project-cleaner-27971714-l25tf   Started container project-cleaner
13m         Normal    SuccessfulCreate        job/project-cleaner-27971714         Created pod: project-cleaner-27971714-l25tf
13m         Normal    Completed               job/project-cleaner-27971714         Job completed
12m         Normal    Scheduled               pod/project-cleaner-27971715-5tg7k   Successfully assigned workshop-support/project-cleaner-27971715-5tg7k to master01
12m         Normal    AddedInterface          pod/project-cleaner-27971715-5tg7k   Add eth0 [10.8.1.225/23] from ovn-kubernetes
12m         Normal    Pulling                 pod/project-cleaner-27971715-5tg7k   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
12m         Normal    Pulled                  pod/project-cleaner-27971715-5tg7k   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 315.976344ms
12m         Normal    Created                 pod/project-cleaner-27971715-5tg7k   Created container project-cleaner
12m         Normal    Started                 pod/project-cleaner-27971715-5tg7k   Started container project-cleaner
12m         Normal    SuccessfulCreate        job/project-cleaner-27971715         Created pod: project-cleaner-27971715-5tg7k
12m         Normal    Completed               job/project-cleaner-27971715         Job completed
11m         Normal    Scheduled               pod/project-cleaner-27971716-rrdqd   Successfully assigned workshop-support/project-cleaner-27971716-rrdqd to master01
11m         Normal    AddedInterface          pod/project-cleaner-27971716-rrdqd   Add eth0 [10.8.1.226/23] from ovn-kubernetes
11m         Normal    Pulling                 pod/project-cleaner-27971716-rrdqd   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
11m         Normal    Pulled                  pod/project-cleaner-27971716-rrdqd   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 311.021177ms
11m         Normal    Created                 pod/project-cleaner-27971716-rrdqd   Created container project-cleaner
11m         Normal    Started                 pod/project-cleaner-27971716-rrdqd   Started container project-cleaner
11m         Normal    SuccessfulCreate        job/project-cleaner-27971716         Created pod: project-cleaner-27971716-rrdqd
11m         Normal    Completed               job/project-cleaner-27971716         Job completed
10m         Normal    Scheduled               pod/project-cleaner-27971717-fbwwr   Successfully assigned workshop-support/project-cleaner-27971717-fbwwr to master01
10m         Normal    AddedInterface          pod/project-cleaner-27971717-fbwwr   Add eth0 [10.8.1.227/23] from ovn-kubernetes
10m         Normal    Pulling                 pod/project-cleaner-27971717-fbwwr   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
10m         Normal    Pulled                  pod/project-cleaner-27971717-fbwwr   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 330.386277ms
10m         Normal    Created                 pod/project-cleaner-27971717-fbwwr   Created container project-cleaner
10m         Normal    Started                 pod/project-cleaner-27971717-fbwwr   Started container project-cleaner
10m         Normal    SuccessfulCreate        job/project-cleaner-27971717         Created pod: project-cleaner-27971717-fbwwr
10m         Normal    Completed               job/project-cleaner-27971717         Job completed
9m34s       Normal    Scheduled               pod/project-cleaner-27971718-2xqv4   Successfully assigned workshop-support/project-cleaner-27971718-2xqv4 to master01
9m33s       Normal    AddedInterface          pod/project-cleaner-27971718-2xqv4   Add eth0 [10.8.1.228/23] from ovn-kubernetes
9m33s       Normal    Pulling                 pod/project-cleaner-27971718-2xqv4   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
9m33s       Normal    Pulled                  pod/project-cleaner-27971718-2xqv4   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 330.438755ms
9m32s       Normal    Created                 pod/project-cleaner-27971718-2xqv4   Created container project-cleaner
9m32s       Normal    Started                 pod/project-cleaner-27971718-2xqv4   Started container project-cleaner
9m34s       Normal    SuccessfulCreate        job/project-cleaner-27971718         Created pod: project-cleaner-27971718-2xqv4
9m23s       Normal    Completed               job/project-cleaner-27971718         Job completed
8m33s       Normal    Scheduled               pod/project-cleaner-27971719-qj68q   Successfully assigned workshop-support/project-cleaner-27971719-qj68q to master01
8m33s       Normal    AddedInterface          pod/project-cleaner-27971719-qj68q   Add eth0 [10.8.1.229/23] from ovn-kubernetes
8m33s       Normal    Pulling                 pod/project-cleaner-27971719-qj68q   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
8m32s       Normal    Pulled                  pod/project-cleaner-27971719-qj68q   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 320.044618ms
8m31s       Normal    Created                 pod/project-cleaner-27971719-qj68q   Created container project-cleaner
8m31s       Normal    Started                 pod/project-cleaner-27971719-qj68q   Started container project-cleaner
8m34s       Normal    SuccessfulCreate        job/project-cleaner-27971719         Created pod: project-cleaner-27971719-qj68q
8m21s       Normal    Completed               job/project-cleaner-27971719         Job completed
7m34s       Normal    Scheduled               pod/project-cleaner-27971720-sg8hm   Successfully assigned workshop-support/project-cleaner-27971720-sg8hm to master01
7m33s       Normal    AddedInterface          pod/project-cleaner-27971720-sg8hm   Add eth0 [10.8.1.230/23] from ovn-kubernetes
7m33s       Normal    Pulling                 pod/project-cleaner-27971720-sg8hm   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
7m33s       Normal    Pulled                  pod/project-cleaner-27971720-sg8hm   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 322.376811ms
7m32s       Normal    Created                 pod/project-cleaner-27971720-sg8hm   Created container project-cleaner
7m32s       Normal    Started                 pod/project-cleaner-27971720-sg8hm   Started container project-cleaner
7m34s       Normal    SuccessfulCreate        job/project-cleaner-27971720         Created pod: project-cleaner-27971720-sg8hm
7m22s       Normal    Completed               job/project-cleaner-27971720         Job completed
6m34s       Normal    Scheduled               pod/project-cleaner-27971721-25xs9   Successfully assigned workshop-support/project-cleaner-27971721-25xs9 to master01
6m34s       Normal    AddedInterface          pod/project-cleaner-27971721-25xs9   Add eth0 [10.8.1.231/23] from ovn-kubernetes
6m34s       Normal    Pulling                 pod/project-cleaner-27971721-25xs9   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
6m33s       Normal    Pulled                  pod/project-cleaner-27971721-25xs9   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 302.966481ms
6m32s       Normal    Created                 pod/project-cleaner-27971721-25xs9   Created container project-cleaner
6m32s       Normal    Started                 pod/project-cleaner-27971721-25xs9   Started container project-cleaner
6m34s       Normal    SuccessfulCreate        job/project-cleaner-27971721         Created pod: project-cleaner-27971721-25xs9
6m22s       Normal    Completed               job/project-cleaner-27971721         Job completed
5m34s       Normal    Scheduled               pod/project-cleaner-27971722-gbb42   Successfully assigned workshop-support/project-cleaner-27971722-gbb42 to master01
5m34s       Normal    AddedInterface          pod/project-cleaner-27971722-gbb42   Add eth0 [10.8.1.232/23] from ovn-kubernetes
5m34s       Normal    Pulling                 pod/project-cleaner-27971722-gbb42   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
5m33s       Normal    Pulled                  pod/project-cleaner-27971722-gbb42   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 311.517065ms
5m32s       Normal    Created                 pod/project-cleaner-27971722-gbb42   Created container project-cleaner
5m32s       Normal    Started                 pod/project-cleaner-27971722-gbb42   Started container project-cleaner
5m34s       Normal    SuccessfulCreate        job/project-cleaner-27971722         Created pod: project-cleaner-27971722-gbb42
5m23s       Normal    Completed               job/project-cleaner-27971722         Job completed
4m34s       Normal    Scheduled               pod/project-cleaner-27971723-vckwd   Successfully assigned workshop-support/project-cleaner-27971723-vckwd to master01
4m34s       Normal    AddedInterface          pod/project-cleaner-27971723-vckwd   Add eth0 [10.8.1.233/23] from ovn-kubernetes
4m34s       Normal    Pulling                 pod/project-cleaner-27971723-vckwd   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
4m33s       Normal    Pulled                  pod/project-cleaner-27971723-vckwd   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 309.224628ms
4m33s       Normal    Created                 pod/project-cleaner-27971723-vckwd   Created container project-cleaner
4m32s       Normal    Started                 pod/project-cleaner-27971723-vckwd   Started container project-cleaner
4m34s       Normal    SuccessfulCreate        job/project-cleaner-27971723         Created pod: project-cleaner-27971723-vckwd
4m23s       Normal    Completed               job/project-cleaner-27971723         Job completed
3m34s       Normal    Scheduled               pod/project-cleaner-27971724-dt7x2   Successfully assigned workshop-support/project-cleaner-27971724-dt7x2 to master01
3m33s       Normal    AddedInterface          pod/project-cleaner-27971724-dt7x2   Add eth0 [10.8.1.234/23] from ovn-kubernetes
3m33s       Normal    Pulling                 pod/project-cleaner-27971724-dt7x2   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
3m33s       Normal    Pulled                  pod/project-cleaner-27971724-dt7x2   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 313.320265ms
3m32s       Normal    Created                 pod/project-cleaner-27971724-dt7x2   Created container project-cleaner
3m32s       Normal    Started                 pod/project-cleaner-27971724-dt7x2   Started container project-cleaner
3m34s       Normal    SuccessfulCreate        job/project-cleaner-27971724         Created pod: project-cleaner-27971724-dt7x2
3m21s       Normal    Completed               job/project-cleaner-27971724         Job completed
2m34s       Normal    Scheduled               pod/project-cleaner-27971725-8hfq6   Successfully assigned workshop-support/project-cleaner-27971725-8hfq6 to master01
2m34s       Normal    AddedInterface          pod/project-cleaner-27971725-8hfq6   Add eth0 [10.8.1.237/23] from ovn-kubernetes
2m34s       Normal    Pulling                 pod/project-cleaner-27971725-8hfq6   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
2m33s       Normal    Pulled                  pod/project-cleaner-27971725-8hfq6   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 314.402811ms
2m33s       Normal    Created                 pod/project-cleaner-27971725-8hfq6   Created container project-cleaner
2m33s       Normal    Started                 pod/project-cleaner-27971725-8hfq6   Started container project-cleaner
2m34s       Normal    SuccessfulCreate        job/project-cleaner-27971725         Created pod: project-cleaner-27971725-8hfq6
2m23s       Normal    Completed               job/project-cleaner-27971725         Job completed
94s         Normal    Scheduled               pod/project-cleaner-27971726-fqqs6   Successfully assigned workshop-support/project-cleaner-27971726-fqqs6 to master01
94s         Normal    AddedInterface          pod/project-cleaner-27971726-fqqs6   Add eth0 [10.8.1.239/23] from ovn-kubernetes
94s         Normal    Pulling                 pod/project-cleaner-27971726-fqqs6   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
93s         Normal    Pulled                  pod/project-cleaner-27971726-fqqs6   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 302.65825ms
93s         Normal    Created                 pod/project-cleaner-27971726-fqqs6   Created container project-cleaner
92s         Normal    Started                 pod/project-cleaner-27971726-fqqs6   Started container project-cleaner
94s         Normal    SuccessfulCreate        job/project-cleaner-27971726         Created pod: project-cleaner-27971726-fqqs6
83s         Normal    Completed               job/project-cleaner-27971726         Job completed
34s         Normal    Scheduled               pod/project-cleaner-27971727-m67bf   Successfully assigned workshop-support/project-cleaner-27971727-m67bf to master01
34s         Normal    AddedInterface          pod/project-cleaner-27971727-m67bf   Add eth0 [10.8.1.240/23] from ovn-kubernetes
34s         Normal    Pulling                 pod/project-cleaner-27971727-m67bf   Pulling image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0"
33s         Normal    Pulled                  pod/project-cleaner-27971727-m67bf   Successfully pulled image "registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.0" in 308.646773ms
33s         Normal    Created                 pod/project-cleaner-27971727-m67bf   Created container project-cleaner
32s         Normal    Started                 pod/project-cleaner-27971727-m67bf   Started container project-cleaner
34s         Normal    SuccessfulCreate        job/project-cleaner-27971727         Created pod: project-cleaner-27971727-m67bf
23s         Normal    Completed               job/project-cleaner-27971727         Job completed
27m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971700
25m         Normal    JobAlreadyActive        cronjob/project-cleaner              Not starting job because prior execution is running and concurrency policy is Forbid
25m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971700, status: Complete
25m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971701
25m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971701, status: Complete
25m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971702
25m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971702, status: Complete
24m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971703
24m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971703, status: Complete
24m         Normal    SuccessfulDelete        cronjob/project-cleaner              Deleted job project-cleaner-27971700
23m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971704
23m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971704, status: Complete
23m         Normal    SuccessfulDelete        cronjob/project-cleaner              Deleted job project-cleaner-27971701
22m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971705
22m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971705, status: Complete
22m         Normal    SuccessfulDelete        cronjob/project-cleaner              Deleted job project-cleaner-27971702
21m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971706
21m         Normal    SawCompletedJob         cronjob/project-cleaner              Saw completed job: project-cleaner-27971706, status: Complete
21m         Normal    SuccessfulDelete        cronjob/project-cleaner              Deleted job project-cleaner-27971703
20m         Normal    SuccessfulCreate        cronjob/project-cleaner              Created job project-cleaner-27971707
34s         Normal    SuccessfulCreate        cronjob/project-cleaner              (combined from similar events): Created job project-cleaner-27971727
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     4 (20s ago)   4m5s
beeper-db-688756744f-7vt5f       1/1     Running     0             26m
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     4 (22s ago)   4m7s
beeper-db-688756744f-7vt5f       1/1     Running     0             26m
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     Running     4 (23s ago)   4m8s
beeper-db-688756744f-7vt5f       1/1     Running     0             26m
[student@workstation beeper-api]$ oc logs pod/beeper-api-65488464b9-mwwk5

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.5)

2023-03-08 19:29:32.749  INFO 1 --- [           main] com.redhat.beeper.BeeperApplication      : Starting BeeperApplication v1.0.0 using Java 17.0.3 on beeper-api-65488464b9-mwwk5 with PID 1 (/app.jar started by 1000840000 in /home/jboss)
2023-03-08 19:29:32.752  INFO 1 --- [           main] com.redhat.beeper.BeeperApplication      : No active profile set, falling back to 1 default profile: "default"
2023-03-08 19:29:38.963  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2023-03-08 19:29:39.441  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 310 ms. Found 1 JPA repository interfaces.
2023-03-08 19:29:44.050  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (https)
2023-03-08 19:29:44.062  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2023-03-08 19:29:44.063  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.68]
2023-03-08 19:29:44.446  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2023-03-08 19:29:44.446  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 11396 ms
2023-03-08 19:29:47.451  INFO 1 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2023-03-08 19:29:47.665  INFO 1 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 5.6.12.Final
2023-03-08 19:29:48.543  INFO 1 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.1.2.Final}
2023-03-08 19:29:48.958  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2023-03-08 19:30:00.186 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.

org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.makeConnection(Driver.java:402) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.3.7.jar!/:42.3.7]
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:364) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:206) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:476) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:561) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-4.0.3.jar!/:na]
	at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:181) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:175) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:173) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:127) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1460) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1494) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:409) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:396) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1154) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:908) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:734) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at com.redhat.beeper.BeeperApplication.main(BeeperApplication.java:15) ~[classes!/:1.0.0]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:108) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:65) ~[app.jar:1.0.0]
Caused by: java.net.SocketTimeoutException: Connect timed out
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546) ~[na:na]
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) ~[na:na]
	at java.base/java.net.Socket.connect(Socket.java:633) ~[na:na]
	at org.postgresql.core.PGStream.createSocket(PGStream.java:241) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235) ~[postgresql-42.3.7.jar!/:42.3.7]
	... 56 common frames omitted

2023-03-08 19:30:00.188  WARN 1 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata

org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.makeConnection(Driver.java:402) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.3.7.jar!/:42.3.7]
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:364) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:206) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:476) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:561) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-4.0.3.jar!/:na]
	at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:181) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:175) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:173) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:127) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1460) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1494) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:409) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:396) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1154) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:908) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:734) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at com.redhat.beeper.BeeperApplication.main(BeeperApplication.java:15) ~[classes!/:1.0.0]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:108) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:65) ~[app.jar:1.0.0]
Caused by: java.net.SocketTimeoutException: Connect timed out
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546) ~[na:na]
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) ~[na:na]
	at java.base/java.net.Socket.connect(Socket.java:633) ~[na:na]
	at org.postgresql.core.PGStream.createSocket(PGStream.java:241) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235) ~[postgresql-42.3.7.jar!/:42.3.7]
	... 56 common frames omitted

2023-03-08 19:30:00.212  INFO 1 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQLDialect
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
> ^C
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connection timed out.

Removing debug pod ...
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connection timed out.

Removing debug pod ...
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     CrashLoopBackOff   5 (2m2s ago)   8m52s
beeper-db-688756744f-7vt5f       1/1     Running            0              31m
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc logs pod/beeper-api-65488464b9-mwwk5

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.5)

2023-03-08 19:31:51.941  INFO 1 --- [           main] com.redhat.beeper.BeeperApplication      : Starting BeeperApplication v1.0.0 using Java 17.0.3 on beeper-api-65488464b9-mwwk5 with PID 1 (/app.jar started by 1000840000 in /home/jboss)
2023-03-08 19:31:51.945  INFO 1 --- [           main] com.redhat.beeper.BeeperApplication      : No active profile set, falling back to 1 default profile: "default"
2023-03-08 19:31:57.958  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2023-03-08 19:31:58.443  INFO 1 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 395 ms. Found 1 JPA repository interfaces.
2023-03-08 19:32:03.161  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (https)
2023-03-08 19:32:03.250  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2023-03-08 19:32:03.250  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.68]
2023-03-08 19:32:03.557  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2023-03-08 19:32:03.558  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 11402 ms
2023-03-08 19:32:06.366  INFO 1 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2023-03-08 19:32:06.649  INFO 1 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 5.6.12.Final
2023-03-08 19:32:07.261  INFO 1 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.1.2.Final}
2023-03-08 19:32:07.760  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2023-03-08 19:32:18.986 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.

org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.makeConnection(Driver.java:402) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.3.7.jar!/:42.3.7]
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:364) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:206) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:476) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:561) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-4.0.3.jar!/:na]
	at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:181) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:175) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:173) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:127) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1460) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1494) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:409) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:396) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1154) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:908) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:734) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at com.redhat.beeper.BeeperApplication.main(BeeperApplication.java:15) ~[classes!/:1.0.0]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:108) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:65) ~[app.jar:1.0.0]
Caused by: java.net.SocketTimeoutException: Connect timed out
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546) ~[na:na]
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) ~[na:na]
	at java.base/java.net.Socket.connect(Socket.java:633) ~[na:na]
	at org.postgresql.core.PGStream.createSocket(PGStream.java:241) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235) ~[postgresql-42.3.7.jar!/:42.3.7]
	... 56 common frames omitted

2023-03-08 19:32:18.987  WARN 1 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata

org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.makeConnection(Driver.java:402) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.3.7.jar!/:42.3.7]
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:364) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:206) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:476) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:561) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-4.0.3.jar!/:na]
	at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:181) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:101) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:175) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:173) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:127) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1460) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1494) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:409) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:396) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1154) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:908) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:734) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at com.redhat.beeper.BeeperApplication.main(BeeperApplication.java:15) ~[classes!/:1.0.0]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:108) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:65) ~[app.jar:1.0.0]
Caused by: java.net.SocketTimeoutException: Connect timed out
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546) ~[na:na]
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) ~[na:na]
	at java.base/java.net.Socket.connect(Socket.java:633) ~[na:na]
	at org.postgresql.core.PGStream.createSocket(PGStream.java:241) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235) ~[postgresql-42.3.7.jar!/:42.3.7]
	... 56 common frames omitted

2023-03-08 19:32:19.000  INFO 1 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQLDialect
2023-03-08 19:32:21.685  INFO 1 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2023-03-08 19:32:32.697 ERROR 1 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.

org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:331) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.makeConnection(Driver.java:402) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.Driver.connect(Driver.java:261) ~[postgresql-42.3.7.jar!/:42.3.7]
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:138) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:364) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:206) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:476) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:561) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:115) ~[HikariCP-4.0.3.jar!/:na]
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) ~[HikariCP-4.0.3.jar!/:na]
	at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:181) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:44) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.internal.exec.ImprovedExtractionContextImpl.getJdbcConnection(ImprovedExtractionContextImpl.java:63) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.extract.spi.ExtractionContext.getQueryResults(ExtractionContext.java:43) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.extract.internal.SequenceInformationExtractorLegacyImpl.extractMetadata(SequenceInformationExtractorLegacyImpl.java:39) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.initializeSequences(DatabaseInformationImpl.java:66) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.<init>(DatabaseInformationImpl.java:60) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.internal.Helper.buildDatabaseInformation(Helper.java:183) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:104) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:196) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:85) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.internal.SessionFactoryImpl.<init>(SessionFactoryImpl.java:335) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:471) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1498) ~[hibernate-core-5.6.12.Final.jar!/:5.6.12.Final]
	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:409) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:396) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[spring-beans-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1154) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:908) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[spring-context-5.3.23.jar!/:5.3.23]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:734) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:308) ~[spring-boot-2.7.5.jar!/:2.7.5]
	at com.redhat.beeper.BeeperApplication.main(BeeperApplication.java:15) ~[classes!/:1.0.0]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:108) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) ~[app.jar:1.0.0]
	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:65) ~[app.jar:1.0.0]
Caused by: java.net.SocketTimeoutException: Connect timed out
	at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546) ~[na:na]
	at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[na:na]
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) ~[na:na]
	at java.base/java.net.Socket.connect(Socket.java:633) ~[na:na]
	at org.postgresql.core.PGStream.createSocket(PGStream.java:241) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.PGStream.<init>(PGStream.java:98) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109) ~[postgresql-42.3.7.jar!/:42.3.7]
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235) ~[postgresql-42.3.7.jar!/:42.3.7]
	... 55 common frames omitted

2023-03-08 19:32:32.698  WARN 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 08001
2023-03-08 19:32:32.698 ERROR 1 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : The connection attempt failed.
2023-03-08 19:32:32.705 ERROR 1 --- [           main] j.LocalContainerEntityManagerFactoryBean : Failed to initialize JPA EntityManagerFactory: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
2023-03-08 19:32:32.705  WARN 1 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: default] Unable to build Hibernate SessionFactory; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to open JDBC Connection for DDL execution
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     CrashLoopBackOff   6 (22s ago)   10m
beeper-db-688756744f-7vt5f       1/1     Running            0             33m
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods | grep beeper 
beeper-api-65488464b9-mwwk5      0/1     CrashLoopBackOff   6 (43s ago)   11m
beeper-db-688756744f-7vt5f       1/1     Running            0             33m
[student@workstation beeper-api]$ oc 
OpenShift Client

This client helps you develop, build, deploy, and run your applications on any
OpenShift or Kubernetes cluster. It also includes the administrative
commands for managing a cluster under the 'adm' subcommand.

Basic Commands:
  login             Log in to a server
  new-project       Request a new project
  new-app           Create a new application
  status            Show an overview of the current project
  project           Switch to another project
  projects          Display existing projects
  explain           Get documentation for a resource

Build and Deploy Commands:
  rollout           Manage a Kubernetes deployment or OpenShift deployment config
  rollback          Revert part of an application back to a previous deployment
  new-build         Create a new build configuration
  start-build       Start a new build
  cancel-build      Cancel running, pending, or new builds
  import-image      Import images from a container image registry
  tag               Tag existing images into image streams

Application Management Commands:
  create            Create a resource from a file or from stdin
  apply             Apply a configuration to a resource by file name or stdin
  get               Display one or many resources
  describe          Show details of a specific resource or group of resources
  edit              Edit a resource on the server
  set               Commands that help set specific features on objects
  label             Update the labels on a resource
  annotate          Update the annotations on a resource
  expose            Expose a replicated application as a service or route
  delete            Delete resources by file names, stdin, resources and names, or by resources and label selector
  scale             Set a new size for a deployment, replica set, or replication controller
  autoscale         Autoscale a deployment config, deployment, replica set, stateful set, or replication controller
  secrets           Manage secrets

Troubleshooting and Debugging Commands:
  logs              Print the logs for a container in a pod
  rsh               Start a shell session in a container
  rsync             Copy files between a local file system and a pod
  port-forward      Forward one or more local ports to a pod
  debug             Launch a new instance of a pod for debugging
  exec              Execute a command in a container
  proxy             Run a proxy to the Kubernetes API server
  attach            Attach to a running container
  run               Run a particular image on the cluster
  cp                Copy files and directories to and from containers
  wait              Experimental: Wait for a specific condition on one or many resources

Advanced Commands:
  adm               Tools for managing a cluster
  replace           Replace a resource by file name or stdin
  patch             Update fields of a resource
  process           Process a template into list of resources
  extract           Extract secrets or config maps to disk
  observe           Observe changes to resources and react to them (experimental)
  policy            Manage authorization policy
  auth              Inspect authorization
  image             Useful commands for managing images
  registry          Commands for working with the registry
  idle              Idle scalable resources
  api-versions      Print the supported API versions on the server, in the form of "group/version"
  api-resources     Print the supported API resources on the server
  cluster-info      Display cluster information
  diff              Diff the live version against a would-be applied version
  kustomize         Build a kustomization target from a directory or URL.

Settings Commands:
  logout            End the current server session
  config            Modify kubeconfig files
  whoami            Return information about the current session
  completion        Output shell completion code for the specified shell (bash, zsh, fish, or powershell)

Other Commands:
  plugin            Provides utilities for interacting with plugins
  version           Print the client and server version information

Usage:
  oc [flags] [options]

Use "oc <command> --help" for more information about a given command.
Use "oc options" for a list of global command-line options (applies to all commands).
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/api/beeps; echo 

[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get netpol 
NAME       POD-SELECTOR   AGE
workshop   <none>         45m
[student@workstation beeper-api]$ oc describe netpol workshop 
Name:         workshop
Namespace:    workshop-support
Created on:   2023-03-08 13:56:04 -0500 EST
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: <any> (traffic allowed to all ports)
    From:
      NamespaceSelector: workshop=template-test
    From:
      NamespaceSelector: network.openshift.io/policy-group=ingress
  Not affecting egress traffic
  Policy Types: Ingress
[student@workstation beeper-api]$ oc delete netpol workshop
networkpolicy.networking.k8s.io "workshop" deleted
[student@workstation beeper-api]$ oc get netpol 
No resources found in workshop-support namespace.
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/api/beeps; echo 

[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connected to 172.30.217.183:5432.
Ncat: 0 bytes sent, 0 bytes received in 0.02 seconds.

Removing debug pod ...
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc get pods 
NAME                             READY   STATUS      RESTARTS      AGE
beeper-api-65488464b9-mwwk5      1/1     Running     8 (77s ago)   17m
beeper-db-688756744f-7vt5f       1/1     Running     0             40m
project-cleaner-27971741-6k22b   0/1     Completed   0             2m16s
project-cleaner-27971742-w46x8   0/1     Completed   0             76s
project-cleaner-27971743-2bcs7   0/1     Completed   0             16s
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem -X 'POST' 'https://beeper-api.apps.ocp4.example.com/api/beep' -H 'Content-Type: application/json' -d '{"username": "user1", "content": "first message"}'
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ ls
beeper-api-ingresspolicy.yaml  beeper-db.yaml  certs  db-networkpolicy.yaml  deployment.yaml  service.yaml
[student@workstation beeper-api]$ cat db-networkpolicy.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-policy
  namespace: workshop-support
spec:
  podSelector:
    matchLabels:
      CHANGE_ME
  ingress:
    - from:
       CHANGE_ME
      ports:
        CHANGE_ME
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ vim db-networkpolicy.yaml
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ cat db-networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-policy
  namespace: workshop-support
spec:
  podSelector:
    matchLabels:
      app: beeper-db
  ingress:
    - from:
       - namespaceSelector:
           matchLabels:
             category: support
         podSelector:
           matchLabels:
             app: beeper-api
      ports:
        - protocol: TCP
          port: 5432
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc apply -f db-networkpolicy.yaml
networkpolicy.networking.k8s.io/database-policy created
[student@workstation beeper-api]$ oc get netpol
NAME              POD-SELECTOR    AGE
database-policy   app=beeper-db   8s
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc describe netpol 
Name:         database-policy
Namespace:    workshop-support
Created on:   2023-03-08 14:48:33 -0500 EST
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     app=beeper-db
  Allowing ingress traffic:
    To Port: 5432/TCP
    From:
      NamespaceSelector: category=support
      PodSelector: app=beeper-api
  Not affecting egress traffic
  Policy Types: Ingress
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-db.workshop-support.svc.cluster.local 5432
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connection timed out.

Removing debug pod ...
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/api/beeps; echo 
[{"id":1,"username":"user1","content":"first message","votes":0}]
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-api.workshop-support.svc.cluster.local 443
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connected to 172.30.110.81:443.
Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.

Removing debug pod ...
[student@workstation beeper-api]$ cat beeper-api-ingresspolicy.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  podSelector: CHANGE_ME
  ingress:
    - from:
        CHANGE_ME
      ports:
        CHANGE_ME
[student@workstation beeper-api]$ vim beeper-api-ingresspolicy.yaml
[student@workstation beeper-api]$ cat beeper-api-ingresspolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: beeper-api
  namespace: workshop-support
spec:
  podSelector: {}
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              network.openshift.io/policy-group: ingress
      ports:
        - protocol: TCP
          port: 8080
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc apply -f beeper-api-ingresspolicy.yaml
networkpolicy.networking.k8s.io/beeper-api created
[student@workstation beeper-api]$ oc get netpol 
NAME              POD-SELECTOR    AGE
beeper-api        <none>          6s
database-policy   app=beeper-db   3m28s
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc describe netpol beeper-api 
Name:         beeper-api
Namespace:    workshop-support
Created on:   2023-03-08 14:51:55 -0500 EST
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      NamespaceSelector: network.openshift.io/policy-group=ingress
  Not affecting egress traffic
  Policy Types: Ingress
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ oc debug --to-namespace="workshop-support" -- nc -z -v beeper-api.workshop-support.svc.cluster.local 443
Starting pod/image-debug ...
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connection timed out.

Removing debug pod ...
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ curl -s --cacert certs/ca.pem https://beeper-api.apps.ocp4.example.com/livez; echo 
{"status":"UP"}
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ 
[student@workstation beeper-api]$ cd
[student@workstation ~]$ lab grade compreview-apps
SUCCESS The workshop-support project exists and has the following labels: {'category': 'support'}
SUCCESS Validating the quota in the workshop-support project
SUCCESS Validating the limit range in the workshop-support project
SUCCESS Verifying project-cleaner-sa serviceaccount exists in workshop-support
SUCCESS The project-cleaner-sa service account has the system:openshift:scc:anyuid cluster role
FAIL    The project-cleaner-sa service account has the project-cleaner cluster role in the workshop-support namespace
        - The step has failed
SUCCESS Verifying project-cleaner cronjob configuration
SUCCESS Checking the beeper-api deployment is ready
SUCCESS Checking tls from outside of the cluster using the /home/student/DO280/labs/compreview-apps/beeper-api/certs/ca.pem CA certificate
SUCCESS Verify that non beeper-api pods cannot connect to the database pod
[student@workstation ~]$ oc get rolebinding -o wide -n workshop-support 
NAME                    ROLE                               AGE   USERS   GROUPS                                    SERVICEACCOUNTS
admin                   ClusterRole/admin                  58m   admin                                             
admin-0                 ClusterRole/admin                  57m           workshop-support                          
system:deployers        ClusterRole/system:deployer        58m                                                     workshop-support/deployer
system:image-builders   ClusterRole/system:image-builder   58m                                                     workshop-support/builder
system:image-pullers    ClusterRole/system:image-puller    58m           system:serviceaccounts:workshop-support   
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ lab finish compreview-apps
SUCCESS Remove workshop-support project
SUCCESS Deleting do280-support in IdM
SUCCESS Deleting group workshop-support
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ 

